{"volume":"research-methods-in-psychology-demo","page":"4-science-and-common-sense-1","summary":"The necessity of a scientific approach in psychology is often questioned, with many relying on common sense or intuition\u2014known as folk psychology\u2014for understanding human behavior. However, scientific research frequently contradicts these intuitive beliefs, revealing inaccuracies. For instance, the belief that expressing anger can alleviate it has been debunked, as has the notion that false confessions are rare. Common myths, such as using only 10% of our brain or the effectiveness of calorie-reducing diets, persist due to heuristics and confirmation bias. Psychologists emphasize skepticism and the pursuit of empirical evidence to challenge these misconceptions. Additionally, they embrace uncertainty, welcoming unanswered questions as opportunities for scientific exploration.","markdown":"<i-callout variant=\"info\" title=\"Learning Objectives\">\n\n1\\. Explain the limitations of common sense when it comes to achieving a detailed and accurate understanding of human behavior.\n\n2\\. Give several examples of common sense or folk psychology that are incorrect.\n\n3\\. Define skepticism and its role in scientific psychology.\n\n<\/i-callout>\n\nSome people wonder whether the scientific approach to psychology is necessary. Can we not reach the same conclusions based on common sense or intuition? Certainly we all have intuitive beliefs about people\u2019s behavior, thoughts, and feelings\u2014and these beliefs are collectively referred to as **folk\u00a0psychology**. Although much of our folk psychology is probably reasonably accurate, it is clear that much of it is not. For example, most people believe that anger can be relieved by \u201cletting it out\u201d\u2014perhaps by punching something or screaming loudly. Scientific research, however, has shown that this approach tends to leave people feeling more angry, not less (Bushman, 2002)\\[1\\]. Likewise, most people believe that no one would confess to a crime that they had not committed unless perhaps that person was being physically tortured. But again, extensive empirical research has shown that false confessions are surprisingly common and occur for a variety of reasons (Kassin & Gudjonsson, 2004). \u00a0\n\n<i-callout>\n\n_**Some Great Myths**_\n\nIn _50 Great Myths of Popular Psychology_, psychologist Scott Lilienfeld and colleagues discuss several widely held commonsense beliefs about human behavior that scientific research has shown to be incorrect (Lilienfeld, Lynn, Ruscio, & Beyerstein, 2010)[\\[3\\]](https:\/\/kpu.pressbooks.pub\/psychmethods4e\/chapter\/science-and-common-sense\/#footnote-27-3). Here is a short list:\n\n* \u201cPeople use only 10% of their brain power.\u201d\n* \u201cMost people experience a midlife crisis in their 40\u2019s or 50\u2019s.\u201d\n* \u201cStudents learn best when teaching styles are matched to their learning styles.\u201d\n* \u201cLow self-esteem is a major cause of psychological problems.\u201d\n* \u201cPsychiatric admissions and crimes increase during full moons.\u201d\n\n<\/i-callout>\n\nHow can so many of our intuitive beliefs about human behavior be so wrong? Notice that this is an empirical question, and it just so happens that psychologists have conducted scientific research on it and identified many contributing factors (Gilovich, 1991)\\[4\\]. One is that forming detailed and accurate beliefs requires powers of observation, memory, and analysis to an extent that we do not naturally possess. It would be nearly impossible to count the number of words spoken by the women and men we happen to encounter, estimate the number of words they spoke per day, average these numbers for both groups, and compare them\u2014all in our heads. This is why we tend to rely on mental shortcuts (what psychologists refer to as **heuristics**) in forming and maintaining our beliefs. For example, if a belief is widely shared\u2014especially if it is endorsed by \u201cexperts\u201d\u2014and it makes intuitive sense, we tend to assume it is true. This is compounded by the fact that we then tend to focus on cases that confirm our intuitive beliefs and not on cases that dis-confirm them. This is \u00a0called\u00a0**confirmation\u00a0bias**. For example, once we begin to believe that women are more talkative than men, we tend to notice and remember talkative women and silent men but ignore or forget silent women and talkative men. We also hold incorrect beliefs in part because it would be nice if they\u00a0_were_\u00a0true. For example, many people believe that calorie-reducing diets are an effective long-term treatment for obesity, yet a thorough review of the scientific evidence has shown that they are not (Mann et al., 2007)\\[5\\]. People may continue to believe in the effectiveness of dieting in part because it gives them hope for losing weight if they are obese or makes them feel good about their own \u201cself-control\u201d if they are not.\u00a0\n\nScientists\u2014especially psychologists\u2014understand that they are just as susceptible as anyone else to intuitive but incorrect beliefs. This is why they cultivate an attitude of\u00a0**skepticism**. Being skeptical does not mean being cynical or distrustful, nor does it mean questioning every belief or claim one comes across (which would be impossible anyway). Instead, it means pausing to consider alternatives and to search for evidence\u2014especially systematically collected empirical evidence\u2014when there is enough at stake to justify doing so. For example, imagine that you read a magazine article that claims that giving children a weekly allowance is a good way to help them develop financial responsibility. This is an interesting and potentially important claim (especially if you have children of your own). Taking an attitude of skepticism, however, would mean pausing to ask whether it might be instead that receiving an allowance merely teaches children to spend money\u2014perhaps even to be more materialistic. Taking an attitude of skepticism would also mean asking what evidence supports the original claim. Is the author a scientific researcher? Is any scientific evidence cited? If the issue was important enough, it might also mean turning to the research literature to see if anyone else had studied it.\n\nBecause there is often not enough evidence to fully evaluate a belief or claim, scientists also cultivate\u00a0a **tolerance\u00a0for\u00a0uncertainty**. They accept that there are many things that they simply do not know. For example, it turns out that there is no scientific evidence that receiving an allowance causes children to be more financially responsible, nor is there any scientific evidence that it causes them to be materialistic. Although this kind of uncertainty can be problematic from a practical perspective\u2014for example, making it difficult to decide what to do when our children ask for an allowance\u2014it is exciting from a scientific perspective. If we do not know the answer to an interesting and empirically testable question, science, and perhaps even you as a researcher, may be able to provide the answer.\n\n1.  Bushman, B. J. (2002). Does venting anger feed or extinguish the flame? Catharsis, rumination, distraction, anger, and aggressive responding. _Personality and Social Psychology Bulletin, 28_, 724\u2013731.\n2.  Kassin, S. M., & Gudjonsson, G. H. (2004). The psychology of confession evidence: A review of the literature and issues. _Psychological Science in the Public Interest, 5_, 33\u201367.\n3.  Lilienfeld, S. O., Lynn, S. J., Ruscio, J., & Beyerstein, B. L. (2010). _50 great myths of popular psychology_. Malden, MA: Wiley-Blackwell.\n4.  Gilovich, T. (1991). _How we know what isn\u2019t so: The fallibility of human reason in everyday life_. New York, NY: Free Press.\n5.  Mann, T., Tomiyama, A. J., Westling, E., Lew, A., Samuels, B., & Chatman, J. (2007). Medicare\u2019s search for effective obesity treatments: Diets are not the answer. _American Psychologist, 62_, 220\u2013233.","text":"Learning Objectives\n\n1. Explain the limitations of common sense when it comes to achieving a detailed and accurate understanding of human behavior.\n2. Give several examples of common sense or folk psychology that are incorrect.\n3. Define skepticism and its role in scientific psychology.\u00a0\n\nSome people wonder whether the scientific approach to psychology is necessary. Can we not reach the same conclusions based on common sense or intuition? Certainly we all have intuitive beliefs about people\u2019s behavior, thoughts, and feelings\u2014and these beliefs are collectively referred to as folk\u00a0psychology. Although much of our folk psychology is probably reasonably accurate, it is clear that much of it is not. For example, most people believe that anger can be relieved by \u201cletting it out\u201d\u2014perhaps by punching something or screaming loudly. Scientific research, however, has shown that this approach tends to leave people feeling more angry, not less (Bushman, 2002)[1]. Likewise, most people believe that no one would confess to a crime that they had not committed unless perhaps that person was being physically tortured. But again, extensive empirical research has shown that false confessions are surprisingly common and occur for a variety of reasons (Kassin & Gudjonsson, 2004). \u00a0\n\nSome Great Myths\n\nIn 50 Great Myths of Popular Psychology, psychologist Scott Lilienfeld and colleagues discuss several widely held commonsense beliefs about human behavior that scientific research has shown to be incorrect (Lilienfeld, Lynn, Ruscio, & Beyerstein, 2010)[3]. Here is a short list:\n\n\u201cPeople use only 10% of their brain power.\u201d\u201cMost people experience a midlife crisis in their 40\u2019s or 50\u2019s.\u201d\u201cStudents learn best when teaching styles are matched to their learning styles.\u201d\u201cLow self-esteem is a major cause of psychological problems.\u201d\u201cPsychiatric admissions and crimes increase during full moons.\u201d\n\nHow can so many of our intuitive beliefs about human behavior be so wrong? Notice that this is an empirical question, and it just so happens that psychologists have conducted scientific research on it and identified many contributing factors (Gilovich, 1991)[4]. One is that forming detailed and accurate beliefs requires powers of observation, memory, and analysis to an extent that we do not naturally possess. It would be nearly impossible to count the number of words spoken by the women and men we happen to encounter, estimate the number of words they spoke per day, average these numbers for both groups, and compare them\u2014all in our heads. This is why we tend to rely on mental shortcuts (what psychologists refer to as heuristics) in forming and maintaining our beliefs. For example, if a belief is widely shared\u2014especially if it is endorsed by \u201cexperts\u201d\u2014and it makes intuitive sense, we tend to assume it is true. This is compounded by the fact that we then tend to focus on cases that confirm our intuitive beliefs and not on cases that dis-confirm them. This is \u00a0called\u00a0confirmation\u00a0bias. For example, once we begin to believe that women are more talkative than men, we tend to notice and remember talkative women and silent men but ignore or forget silent women and talkative men. We also hold incorrect beliefs in part because it would be nice if they\u00a0were\u00a0true. For example, many people believe that calorie-reducing diets are an effective long-term treatment for obesity, yet a thorough review of the scientific evidence has shown that they are not (Mann et al., 2007)[5]. People may continue to believe in the effectiveness of dieting in part because it gives them hope for losing weight if they are obese or makes them feel good about their own \u201cself-control\u201d if they are not.\u00a0\n\nScientists\u2014especially psychologists\u2014understand that they are just as susceptible as anyone else to intuitive but incorrect beliefs. This is why they cultivate an attitude of\u00a0skepticism. Being skeptical does not mean being cynical or distrustful, nor does it mean questioning every belief or claim one comes across (which would be impossible anyway). Instead, it means pausing to consider alternatives and to search for evidence\u2014especially systematically collected empirical evidence\u2014when there is enough at stake to justify doing so. For example, imagine that you read a magazine article that claims that giving children a weekly allowance is a good way to help them develop financial responsibility. This is an interesting and potentially important claim (especially if you have children of your own). Taking an attitude of skepticism, however, would mean pausing to ask whether it might be instead that receiving an allowance merely teaches children to spend money\u2014perhaps even to be more materialistic. Taking an attitude of skepticism would also mean asking what evidence supports the original claim. Is the author a scientific researcher? Is any scientific evidence cited? If the issue was important enough, it might also mean turning to the research literature to see if anyone else had studied it.\n\nBecause there is often not enough evidence to fully evaluate a belief or claim, scientists also cultivate\u00a0a tolerance\u00a0for\u00a0uncertainty. They accept that there are many things that they simply do not know. For example, it turns out that there is no scientific evidence that receiving an allowance causes children to be more financially responsible, nor is there any scientific evidence that it causes them to be materialistic. Although this kind of uncertainty can be problematic from a practical perspective\u2014for example, making it difficult to decide what to do when our children ask for an allowance\u2014it is exciting from a scientific perspective. If we do not know the answer to an interesting and empirically testable question, science, and perhaps even you as a researcher, may be able to provide the answer.\n\nBushman, B. J. (2002). Does venting anger feed or extinguish the flame? Catharsis, rumination, distraction, anger, and aggressive responding. Personality and Social Psychology Bulletin, 28, 724\u2013731.Kassin, S. M., & Gudjonsson, G. H. (2004). The psychology of confession evidence: A review of the literature and issues. Psychological Science in the Public Interest, 5, 33\u201367.Lilienfeld, S. O., Lynn, S. J., Ruscio, J., & Beyerstein, B. L. (2010). 50 great myths of popular psychology. Malden, MA: Wiley-Blackwell.Gilovich, T. (1991). How we know what isn\u2019t so: The fallibility of human reason in everyday life. New York, NY: Free Press.Mann, T., Tomiyama, A. J., Westling, E., Lew, A., Samuels, B., & Chatman, J. (2007). Medicare\u2019s search for effective obesity treatments: Diets are not the answer. American Psychologist, 62, 220\u2013233. \u00a0","contextuality":{"text":"The necessity of a scientific approach in psychology is often questioned, with many relying on common sense or intuition\u2014known as folk psychology\u2014for understanding human behavior. However, scientific research frequently contradicts these intuitive _______, revealing inaccuracies. For instance, the ______ that expressing anger can alleviate it has been ________, as has the notion that false ___________ are rare. Common myths, such as using only 10% of our brain or the effectiveness of _______-reducing diets, persist due to heuristics and confirmation bias. Psychologists emphasize __________ and the pursuit of empirical evidence to _________ these misconceptions. Additionally, they embrace uncertainty, welcoming __________ questions as opportunities for scientific exploration.","gaps":[["beliefs",248,7],["belief",299,6],["debunked",354,8],["confessions",393,11],["calorie",489,7],["skepticism",586,10],["challenge",638,9],["unanswered",720,10]]},"contextuality_plus":{"text":"The necessity of a scientific approach in psychology is often questioned, with many relying on common sense or intuition\u2014known as folk psychology\u2014for understanding human behavior. However, scientific research frequently contradicts these _________ beliefs, revealing inaccuracies. For instance, the belief that expressing anger can _________ it has been debunked, as has the notion that false ___________ are rare. Common myths, such as using only 10% of our brain or the effectiveness of _______-reducing diets, persist due to heuristics and confirmation bias. Psychologists emphasize __________ and the pursuit of empirical evidence to _________ these misconceptions. Additionally, they embrace ___________, welcoming unanswered questions as opportunities for scientific ___________.","gaps":[["intuitive",238,9],["alleviate",332,9],["confessions",393,11],["calorie",489,7],["skepticism",586,10],["challenge",638,9],["uncertainty",697,11],["exploration",773,11]]},"keyword":{"text":"The necessity of a scientific approach in psychology is often questioned, with many relying on common sense or intuition\u2014known as folk psychology\u2014for understanding human behavior. However, __________ research frequently contradicts these intuitive beliefs, revealing inaccuracies. For instance, the belief that expressing anger can alleviate it has been debunked, as has the notion that false confessions are rare. Common myths, such as using only 10% of our brain or the effectiveness of calorie-reducing diets, persist due to heuristics and confirmation bias. Psychologists emphasize skepticism and the pursuit of empirical evidence to challenge these misconceptions. Additionally, they embrace uncertainty, welcoming unanswered questions as opportunities for __________ exploration.","gaps":[["scientific",189,10],["scientific",762,10]]}}
{"volume":"formal-operator-training-guidelines","page":"7-other-training","summary":"The outlined training requirements for operations personnel in a Refinery Business Unit emphasize the necessary qualifications for working in a new plant. The Capital Project Team is tasked with developing and delivering training materials, with Learning & Development (L&D) personnel providing subject matter expertise as needed. Additionally, training related to plant changes and procedure updates is managed through the Management of Change (MOC) system and the Document Management System. Subject matter experts develop and deliver this training, ensuring operators are informed of changes impacting their roles. Completion of training is documented using various electronic tools.","markdown":"### 7.1 New Plant Training\n\nThe following table outlines the required training for operations personnel working in a Refinery Business Unit when starting up a new plant. Operations personnel completing these requirements will be qualified to work jobs associated within the new plants. Development and delivery of any new plant training material is the responsibility of the Capital Project Team assigned to the new plant. L&D personnel may be available during the development phase to serve as SMEs but are not responsible for development of material or delivery to the affected personnel.\n\n| Phase | Required Training | Personnel |\n| --- | --- | --- |\n| Pre Startup | Comprehensive Unit Schools covering Volumes 1 \u2013 4 of the new plant EOMs.\u00a0 \u00a0 \u00a0 \u00a0(KR-998s daily)<br><br>80% Minimum Test Score (Daily & Comprehensive)<br><br>STL Solo Assessments \/ Console STL Checklist completed | Head Operators<br><br>Control Room Operators<br><br>Fully Qualified Operators<br><br>Trainees |\n| Post Startup | All CROs, FQOs and trainees must continue their New Plant training and complete all other normal qualification requirements outlined previously in this section. | Control Room Operators<br><br>Fully Qualified Operators<br><br>Operator Trainees |\n| Transfers to New Plant after Startup | All Head Operators, Fully Qualified Operators, and Operator Trainees transferring into the New Plant are subject to normal qualification requirements as outlined in the Formal Training Guidelines. | All Head Operators<br><br>All Fully Qualified Operators<br><br>All Trainees |\n| A KR-981 or KR-981A must be completed for all HOs, CROs, FQOs, and trainees. |     |     |\n\nTable\u00a09: New Plant Training Requirements\n\n### 7.2 Management of Change (MOC) Training\n\nTraining for plant changes, procedure updates and any other modifications in the business unit that impacts the performance of the operator role are managed within the MOC system of record and the Document Management System. Training is developed and delivered by subject matter experts with knowledge of the approved changes. Completed acknowledgments and training sessions are recorded in a variety of electronic tools.","text":"7.1 New Plant Training\n\nThe following table outlines the required training for operations personnel working in a Refinery Business Unit when starting up a new plant. Operations personnel completing these requirements will be qualified to work jobs associated within the new plants. Development and delivery of any new plant training material is the responsibility of the Capital Project Team assigned to the new plant. L&D personnel may be available during the development phase to serve as SMEs but are not responsible for development of material or delivery to the affected personnel.\n\nTable\u00a09: New Plant Training Requirements\n7.2 Management of Change (MOC) Training\n\nTraining for plant changes, procedure updates and any other modifications in the business unit that impacts the performance of the operator role are managed within the MOC system of record and the Document Management System. Training is developed and delivered by subject matter experts with knowledge of the approved changes. Completed acknowledgments and training sessions are recorded in a variety of electronic tools.","contextuality":{"text":"The outlined training requirements for operations personnel in a Refinery Business Unit emphasize the necessary qualifications for working in a new plant. The Capital Project Team is tasked with __________ and delivering training materials, with Learning & Development (L&D) personnel providing subject matter _________ as needed. Additionally, training related to plant changes and procedure _______ is managed through the Management of Change (MOC) ______ and the Document Management System. Subject matter _______ develop and deliver this training, ensuring operators are ________ of changes impacting their roles. Completion of training is documented using various electronic _____.","gaps":[["developing",195,10],["expertise",310,9],["updates",393,7],["system",451,6],["experts",509,7],["informed",575,8],["tools",680,5]]},"contextuality_plus":{"text":"The outlined training requirements for operations personnel in a Refinery Business Unit emphasize the necessary qualifications for working in a new plant. The Capital Project Team is tasked with developing and __________ training materials, with Learning & Development (L&D) personnel providing subject matter _________ as needed. Additionally, training related to plant changes and procedure _______ is managed through the Management of Change (MOC) ______ and the Document Management System. Subject matter experts develop and _______ this training, ensuring operators are informed of changes impacting their _____. Completion of training is documented using various electronic _____.","gaps":[["delivering",210,10],["expertise",310,9],["updates",393,7],["system",451,6],["deliver",529,7],["roles",611,5],["tools",680,5]]},"keyword":{"text":"The outlined training requirements for operations personnel in a Refinery Business Unit emphasize the necessary qualifications for working in a new plant. The Capital Project Team is tasked with developing and delivering training materials, with ________ & Development (L&D) personnel providing subject matter expertise as needed. Additionally, ________ related to plant changes and procedure updates is managed through the Management of Change (MOC) ______ and the Document Management System. Subject matter experts develop and deliver this ________, ensuring operators are informed of changes impacting their roles. Completion of ________ is documented using various electronic tools.","gaps":[["system",451,6],["Learning",246,8],["training",345,8],["training",542,8],["training",632,8]]}}
{"volume":"research-methods-in-psychology-demo","page":"9-generating-good-research-questions-1","summary":"When developing a research idea, transforming it into empirically testable questions is crucial. This can be achieved by reviewing the discussion sections of recent articles, where researchers often propose future research directions. Alternatively, one can generate questions by conceptualizing a behavior or characteristic as a variable and exploring its frequency or relationship with other variables. Evaluating research questions involves assessing their interestingness and feasibility. A question is interesting if its answer is uncertain, fills a literature gap, and has practical implications. Feasibility depends on resources, time, and accessibility. Employing tried and tested methods can enhance feasibility and ensure continuity with existing research.","markdown":"<i-callout variant=\"info\" title=\"Learning Objectives\">\n\n1\\. Describe some techniques for turning research ideas into empirical research questions and use those techniques to generate questions.\n\n2\\. Explain what makes a research question interesting and evaluate research questions in terms of their interestingness.\n\n<\/i-callout>\n\nOnce you have a research idea, you need to use it to generate one or more empirically testable research questions, that is, questions expressed in terms of a single variable or relationship between variables. One way to do this is to look closely at the discussion section in a recent research article on the topic. This is the last major section of the article, in which the researchers summarize their results, interpret them in the context of past research, and suggest directions for future research. These suggestions often take the form of specific research questions, which you can then try to answer with additional research. This can be a good strategy because it is likely that the suggested questions have already been identified as interesting and important by experienced researchers.\u00a0\n\nBut you may also want to generate your own research questions. How can you do this? First, if you have a particular behavior or psychological characteristic in mind, you can simply conceptualize it as a variable and ask how frequent or intense it is. How many words on average do people speak per day? How accurate are our memories of traumatic events? What percentage of people have sought professional help for depression? If the question has never been studied scientifically\u2014which is something that you will learn when you conduct your literature review\u2014then it might be interesting and worth pursuing.\u00a0\n\nIf scientific research has already answered the question of how frequent or intense the behavior or characteristic is, then you should consider turning it into a question about a relationship between that behavior or characteristic and some other variable. One way to do this is to ask yourself the following series of more general questions and write down all the answers you can think of.\n\n* What are some possible causes of the behavior or characteristic?\n* What are some possible effects of the behavior or characteristic?\n* What types of people might exhibit more or less of the behavior or characteristic?\n* What types of situations might elicit more or less of the behavior or characteristic?\n\nIn general, each answer you write down can be conceptualized as a second variable, suggesting a question about a relationship. If you were interested in talkativeness, for example, it might occur to you that a possible cause of this psychological characteristic is family size. Is there a relationship between family size and talkativeness? Or it might occur to you that people seem to be more talkative in same-sex groups than mixed-sex groups. Is there a difference in the average level of talkativeness of people in same-sex groups and people in mixed-sex groups? This approach should allow you to generate many different empirically testable questions about almost any behavior or psychological characteristic.\n\nIf through this process you generate a question that has never been studied scientifically\u2014which again is something that you will learn in your literature review\u2014then it might be interesting and worth pursuing. But what if you find that it\u00a0has\u00a0been studied scientifically? Although novice researchers often want to give up and move on to a new question at this point, this is not necessarily a good strategy. For one thing, the fact that the question has been studied scientifically and the research published suggests that it is of interest to the scientific community. For another, the question can almost certainly be refined so that its answer will still contribute something new to the research literature. Again, asking yourself a series of more general questions about the relationship is a good strategy.\n\n* Are there other ways to define and measure the variables?\n* Are there types of people for whom the relationship might be stronger or weaker?\n* Are there situations in which the relationship might be stronger or weaker\u2014including situations with practical importance?\n\nFor example, research has shown that women and men speak about the same number of words per day\u2014but this was when talkativeness was measured in terms of the number of words spoken per day among university students in the United States and Mexico. We can still ask whether other ways of measuring talkativeness\u2014perhaps the number of different people spoken to each day\u2014produce the same result. Or we can ask whether studying elderly people or people from other cultures produces the same result. Again, this approach should help you generate many different research questions about almost any relationship.\n\n### Evaluating Research Questions\n\nResearchers usually generate many more research questions than they ever attempt to answer. This means they must have some way of evaluating the research questions they generate so that they can choose which ones to pursue. In this section, we consider two criteria for evaluating research questions: the interestingness of the question and the feasibility of answering it.\n\nHow often do people tie their shoes? Do people feel pain when you punch them in the jaw? Are women more likely to wear makeup than men? Do people prefer vanilla or chocolate ice cream? Although it would be a fairly simple matter to design a study and collect data to answer these questions, you probably would not want to because they are not interesting. We are not talking here about whether a research question is interesting to us personally but whether it is interesting to people more generally and, especially, to the scientific community. But what makes a research question interesting in this sense? Here we look at three factors that affect the\u00a0**interestingness**\u00a0of a research question: the answer is in doubt, the answer fills a gap in the research literature, and the answer has important practical implications.\u00a0\n\nFirst, a research question is interesting to the extent that its answer is in doubt. Obviously, questions that have been answered by scientific research are no longer interesting as the subject of new empirical research. But the fact that a question has not been answered by scientific research does not necessarily make it interesting. There has to be some reasonable chance that the answer to the question will be something that we did not already know. But how can you assess this before actually collecting data? One approach is to try to think of reasons to expect different answers to the question\u2014especially ones that seem to conflict with common sense. If you can think of reasons to expect at least two different answers, then the question might be interesting. If you can think of reasons to expect only one answer, then it probably is not. The question of whether women are more talkative than men is interesting because there are reasons to expect both answers. The existence of the stereotype itself suggests the answer could be yes, but the fact that women\u2019s and men\u2019s verbal abilities are fairly similar suggests the answer could be no. The question of whether people feel pain when you punch them in the jaw is not interesting because there is absolutely no reason to think that the answer could be anything other than a resounding yes.\n\nA second important factor to consider when deciding if a research question is interesting is whether answering it will fill a gap in the research literature. Again, this means in part that the question has not already been answered by scientific research. But it also means that the question is in some sense a natural one for people who are familiar with the research literature. For example, the question of whether taking lecture notes by hand can help improve students\u2019 exam performance would be likely to occur to anyone who was familiar with research on note taking\u00a0and the ineffectiveness of shallow processing on learning.\n\nA final factor to consider when deciding whether a research question is interesting is whether its answer has important practical implications. Again, the question of whether taking notes by hand improves learning has important implications for education, including classroom policies concerning technology use. The question of whether cell phone use impairs driving is interesting because it is relevant to the personal safety of everyone who travels by car and to the debate over whether cell phone use should be restricted by law.\n\nA second important criterion for evaluating research questions is the feasibility\u00a0of successfully answering them. There are many factors that affect feasibility, including time, money, equipment and materials, technical knowledge and skill, and access to research participants. Clearly, researchers need to take these factors into account so that they do not waste time and effort pursuing research that they cannot complete successfully. Looking through a sample of professional journals in psychology will reveal many studies that are complicated and difficult to carry out. These include longitudinal designs in which participants are tracked over many years, neuroimaging studies in which participants\u2019 brain activity is measured while they carry out various mental tasks, and complex non-experimental studies involving several variables and complicated statistical analyses. Keep in mind, though, that such research tends to be carried out by teams of highly trained researchers whose work is often supported in part by government and private grants. Also, keep in mind that research does not have to be complicated or difficult to produce interesting and important results. Looking through a sample of professional journals will also reveal studies that are relatively simple and easy to carry out\u2014perhaps involving a convenience sample of university students and a paper-and-pencil task. A final point here is that it is generally good practice to use methods that have already been used successfully by other researchers. For example, if you want to manipulate people\u2019s moods to make some of them happy, it would be a good idea to use one of the many approaches that have been used successfully by other researchers (e.g., paying them a compliment). This is good not only for the sake of feasibility\u2014the approach is \u201ctried and true\u201d\u2014but also because it provides greater continuity with previous research. This makes it easier to compare your results with those of other researchers and to understand the implications of their research for yours, and vice versa.","text":"Learning Objectives\n\n1. Describe some techniques for turning research ideas into empirical research questions and use those techniques to generate questions.\n2. Explain what makes a research question interesting and evaluate research questions in terms of their interestingness.\u00a0\n\nOnce you have a research idea, you need to use it to generate one or more empirically testable research questions, that is, questions expressed in terms of a single variable or relationship between variables. One way to do this is to look closely at the discussion section in a recent research article on the topic. This is the last major section of the article, in which the researchers summarize their results, interpret them in the context of past research, and suggest directions for future research. These suggestions often take the form of specific research questions, which you can then try to answer with additional research. This can be a good strategy because it is likely that the suggested questions have already been identified as interesting and important by experienced researchers.\u00a0\n\nBut you may also want to generate your own research questions. How can you do this? First, if you have a particular behavior or psychological characteristic in mind, you can simply conceptualize it as a variable and ask how frequent or intense it is. How many words on average do people speak per day? How accurate are our memories of traumatic events? What percentage of people have sought professional help for depression? If the question has never been studied scientifically\u2014which is something that you will learn when you conduct your literature review\u2014then it might be interesting and worth pursuing.\u00a0\n\nIf scientific research has already answered the question of how frequent or intense the behavior or characteristic is, then you should consider turning it into a question about a relationship between that behavior or characteristic and some other variable. One way to do this is to ask yourself the following series of more general questions and write down all the answers you can think of.\n\nWhat are some possible causes of the behavior or characteristic?What are some possible effects of the behavior or characteristic?What types of people might exhibit more or less of the behavior or characteristic?What types of situations might elicit more or less of the behavior or characteristic?\n\nIn general, each answer you write down can be conceptualized as a second variable, suggesting a question about a relationship. If you were interested in talkativeness, for example, it might occur to you that a possible cause of this psychological characteristic is family size. Is there a relationship between family size and talkativeness? Or it might occur to you that people seem to be more talkative in same-sex groups than mixed-sex groups. Is there a difference in the average level of talkativeness of people in same-sex groups and people in mixed-sex groups? This approach should allow you to generate many different empirically testable questions about almost any behavior or psychological characteristic.\n\nIf through this process you generate a question that has never been studied scientifically\u2014which again is something that you will learn in your literature review\u2014then it might be interesting and worth pursuing. But what if you find that it\u00a0has\u00a0been studied scientifically? Although novice researchers often want to give up and move on to a new question at this point, this is not necessarily a good strategy. For one thing, the fact that the question has been studied scientifically and the research published suggests that it is of interest to the scientific community. For another, the question can almost certainly be refined so that its answer will still contribute something new to the research literature. Again, asking yourself a series of more general questions about the relationship is a good strategy.\n\nAre there other ways to define and measure the variables?Are there types of people for whom the relationship might be stronger or weaker?Are there situations in which the relationship might be stronger or weaker\u2014including situations with practical importance?\n\nFor example, research has shown that women and men speak about the same number of words per day\u2014but this was when talkativeness was measured in terms of the number of words spoken per day among university students in the United States and Mexico. We can still ask whether other ways of measuring talkativeness\u2014perhaps the number of different people spoken to each day\u2014produce the same result. Or we can ask whether studying elderly people or people from other cultures produces the same result. Again, this approach should help you generate many different research questions about almost any relationship.\n\n\u00a0\n\nEvaluating Research Questions\n\nResearchers usually generate many more research questions than they ever attempt to answer. This means they must have some way of evaluating the research questions they generate so that they can choose which ones to pursue. In this section, we consider two criteria for evaluating research questions: the interestingness of the question and the feasibility of answering it.\n\nHow often do people tie their shoes? Do people feel pain when you punch them in the jaw? Are women more likely to wear makeup than men? Do people prefer vanilla or chocolate ice cream? Although it would be a fairly simple matter to design a study and collect data to answer these questions, you probably would not want to because they are not interesting. We are not talking here about whether a research question is interesting to us personally but whether it is interesting to people more generally and, especially, to the scientific community. But what makes a research question interesting in this sense? Here we look at three factors that affect the\u00a0interestingness\u00a0of a research question: the answer is in doubt, the answer fills a gap in the research literature, and the answer has important practical implications.\u00a0\n\nFirst, a research question is interesting to the extent that its answer is in doubt. Obviously, questions that have been answered by scientific research are no longer interesting as the subject of new empirical research. But the fact that a question has not been answered by scientific research does not necessarily make it interesting. There has to be some reasonable chance that the answer to the question will be something that we did not already know. But how can you assess this before actually collecting data? One approach is to try to think of reasons to expect different answers to the question\u2014especially ones that seem to conflict with common sense. If you can think of reasons to expect at least two different answers, then the question might be interesting. If you can think of reasons to expect only one answer, then it probably is not. The question of whether women are more talkative than men is interesting because there are reasons to expect both answers. The existence of the stereotype itself suggests the answer could be yes, but the fact that women\u2019s and men\u2019s verbal abilities are fairly similar suggests the answer could be no. The question of whether people feel pain when you punch them in the jaw is not interesting because there is absolutely no reason to think that the answer could be anything other than a resounding yes.\n\nA second important factor to consider when deciding if a research question is interesting is whether answering it will fill a gap in the research literature. Again, this means in part that the question has not already been answered by scientific research. But it also means that the question is in some sense a natural one for people who are familiar with the research literature. For example, the question of whether taking lecture notes by hand can help improve students\u2019 exam performance would be likely to occur to anyone who was familiar with research on note taking\u00a0and the ineffectiveness of shallow processing on learning.\n\nA final factor to consider when deciding whether a research question is interesting is whether its answer has important practical implications. Again, the question of whether taking notes by hand improves learning has important implications for education, including classroom policies concerning technology use. The question of whether cell phone use impairs driving is interesting because it is relevant to the personal safety of everyone who travels by car and to the debate over whether cell phone use should be restricted by law.\n\nA second important criterion for evaluating research questions is the feasibility\u00a0of successfully answering them. There are many factors that affect feasibility, including time, money, equipment and materials, technical knowledge and skill, and access to research participants. Clearly, researchers need to take these factors into account so that they do not waste time and effort pursuing research that they cannot complete successfully. Looking through a sample of professional journals in psychology will reveal many studies that are complicated and difficult to carry out. These include longitudinal designs in which participants are tracked over many years, neuroimaging studies in which participants\u2019 brain activity is measured while they carry out various mental tasks, and complex non-experimental studies involving several variables and complicated statistical analyses. Keep in mind, though, that such research tends to be carried out by teams of highly trained researchers whose work is often supported in part by government and private grants. Also, keep in mind that research does not have to be complicated or difficult to produce interesting and important results. Looking through a sample of professional journals will also reveal studies that are relatively simple and easy to carry out\u2014perhaps involving a convenience sample of university students and a paper-and-pencil task. A final point here is that it is generally good practice to use methods that have already been used successfully by other researchers. For example, if you want to manipulate people\u2019s moods to make some of them happy, it would be a good idea to use one of the many approaches that have been used successfully by other researchers (e.g., paying them a compliment). This is good not only for the sake of feasibility\u2014the approach is \u201ctried and true\u201d\u2014but also because it provides greater continuity with previous research. This makes it easier to compare your results with those of other researchers and to understand the implications of their research for yours, and vice versa.\u00a0","contextuality":{"text":"When developing a research idea, transforming it into empirically testable questions is crucial. This can be achieved by _________ the discussion sections of recent articles, where ___________ often propose future research directions. Alternatively, one can generate _________ by conceptualizing a behavior or characteristic as a ________ and exploring its frequency or relationship with other _________. Evaluating research questions involves assessing their interestingness and feasibility. A ________ is interesting if its answer is uncertain, fills a __________ gap, and has practical implications. Feasibility _______ on resources, time, and accessibility. Employing tried and tested _______ can enhance feasibility and ensure continuity with ________ research.","gaps":[["reviewing",121,9],["researchers",181,11],["questions",267,9],["variable",330,8],["variables",394,9],["question",495,8],["literature",555,10],["depends",615,7],["methods",689,7],["existing",748,8]]},"contextuality_plus":{"text":"When developing a research idea, transforming it into empirically testable questions is crucial. This can be ________ by reviewing the discussion sections of recent articles, where ___________ often propose future research directions. Alternatively, one can generate questions by _______________ a behavior or characteristic as a variable and _________ its frequency or relationship with other variables. Evaluating research questions ________ assessing their interestingness and feasibility. A question is interesting if its answer is _________, fills a literature gap, and has practical ____________. Feasibility depends on resources, time, and accessibility. Employing _____ and tested methods can enhance feasibility and ensure __________ with existing research.","gaps":[["achieved",109,8],["researchers",181,11],["conceptualizing",280,15],["exploring",343,9],["involves",435,8],["uncertain",536,9],["implications",589,12],["tried",672,5],["continuity",732,10]]},"keyword":{"text":"When developing a research idea, transforming it into empirically testable questions is crucial. This can be achieved by reviewing the discussion sections of recent ________, where researchers often propose future ________ directions. Alternatively, one can generate _________ by conceptualizing a behavior or characteristic as a variable and exploring its frequency or relationship with other variables. Evaluating research _________ involves assessing their interestingness and ___________. A question is interesting if its answer is uncertain, fills a literature gap, and has practical implications. Feasibility depends on resources, time, and accessibility. Employing tried and tested methods can enhance ___________ and ensure continuity with existing ________.","gaps":[["articles",165,8],["feasibility",480,11],["feasibility",709,11],["questions",267,9],["questions",425,9],["research",214,8],["research",757,8]]}}
{"volume":"research-methods-in-psychology","page":"2-understanding-science","summary":"Psychology, often surprising to many, is a science similar to astronomy, biology, and chemistry in its methodological approach to understanding the natural world, particularly human behavior. This scientific approach is defined by three core principles: systematic empiricism, empirical questions, and public knowledge. Systematic empiricism involves carefully planned and recorded observations, as demonstrated by studies that challenge stereotypes through data rather than assumptions. Empirical questions, such as gender differences in communication, are those that can be answered through observation, unlike value-based questions. Public knowledge is achieved through publishing research, enabling scientific progress through collaboration and self-correction, as seen in efforts like the Many Labs Replication Project. In contrast, pseudoscience lacks these principles, often failing to provide empirical evidence or public scrutiny, and can lead to harmful beliefs and practices. Understanding these distinctions highlights the importance of scientific rigor and helps differentiate legitimate scientific inquiry from pseudoscientific claims.","markdown":"<i-callout variant=\"info\" title=\"Learning Objectives\">\n\n1\\. Define science.\n\n2\\. Describe the three fundamental features of science.\n\n3\\. Explain why psychology is a science.\u00a0\n\n4\\. Define pseudoscience and give some examples.\n\n<\/i-callout>\n\nSome people are surprised to learn that psychology is a science. They generally agree that astronomy, biology, and chemistry are sciences but wonder what psychology has in common with these other fields. Before answering this question, however, it is worth reflecting on what astronomy, biology, and chemistry have in common with each other. It is clearly not their subject matter. Astronomers study celestial bodies, biologists study living organisms, and chemists study matter and its properties. It is also not the equipment and techniques that they use. Few biologists would know what to do with a radio telescope, for example, and few chemists would know how to track a moose population in the wild. For these and other reasons, philosophers and scientists who have thought deeply about this question have concluded that what the sciences have in common is a general approach to understanding the natural world. Psychology is a science because it takes this same general approach to understanding one aspect of the natural world: human behavior.\n\nThe general scientific approach has three fundamental features (Stanovich, 2010)\\[1\\]. The first is systematic empiricism. Empiricism refers to learning based on observation, and scientists learn about the natural world systematically, by carefully planning, making, recording, and analyzing observations of it. As we will see, logical reasoning and even creativity play important roles in science too, but scientists are unique in their insistence on checking their ideas about the way the world is against their systematic observations. Notice, for example, that Mehl and his colleagues did not trust other people\u2019s stereotypes or even their own informal observations. Instead, they systematically recorded, counted, and compared the number of words spoken by a large sample of women and men. Furthermore, when their systematic observations turned out to conflict with people\u2019s stereotypes, they trusted their systematic observations.\u00a0\n\nThe second feature of the scientific approach\u2014which follows in a straightforward way from the first\u2014is that it is concerned with empirical questions. These are questions about the way the world actually is and, therefore, can be answered by systematically observing it. The question of whether women talk more than men is empirical in this way. Either women really do talk more than men or they do not, and this can be determined by systematically observing how much women and men actually talk. Having said this, there are many interesting and important questions that are not empirically testable and that science is not in a position to answer. Among these are questions about values\u2014whether things are good or bad, just or unjust, or beautiful or ugly, and how the world ought to be. So although the question of whether a stereotype is accurate or inaccurate is an empirically testable one that science can answer, the question\u2014or, rather, the value judgment\u2014of whether it is wrong for people to hold inaccurate stereotypes is not. Similarly, the question of whether criminal behavior has a genetic basis is an empirical question, but the question of what actions ought to be considered illegal is not. It is especially important for researchers in psychology to be mindful of this distinction.\n\nThe third feature of science is that it creates public knowledge. After asking their empirical questions, making their systematic observations, and drawing their conclusions, scientists publish their work. This usually means writing an article for publication in a professional journal, in which they put their research question in the context of previous research, describe in detail the methods they used to answer their question, and clearly present their results and conclusions. Increasingly, scientists are opting to publish their work in open access journals, in which the articles are freely available to all \u2013 scientists and nonscientists alike. This important choice allows publicly-funded research to create knowledge that is truly public.\n\nPublication is an essential feature of science for two reasons. One is that science is a social process\u2014a large-scale collaboration among many researchers distributed across both time and space. Our current scientific knowledge of most topics is based on many different studies conducted by many different researchers who have shared their work publicly over many years. The second is that publication allows science to be self-correcting. Individual scientists understand that, despite their best efforts, their methods can be flawed and their conclusions incorrect. Publication allows others in the scientific community to detect and correct these errors so that, over time, scientific knowledge increasingly reflects the way the world actually is.\u00a0\n\nA good example of the self-correcting nature of science is the \u201cMany Labs Replication Project\u201d \u2013 a large and coordinated effort by prominent psychological scientists around the world to attempt to replicate findings from 13 classic and contemporary studies (Klein et al., 2013)\\[2\\]. One of the findings selected by these researchers for replication was the fascinating effect, first reported by Simone Schnall and her colleagues at the University of Plymouth, that washing one\u2019s hands leads people to view moral transgressions\u2014ranging from keeping money inside a found wallet to using a kitten for sexual arousal\u2014as less wrong (Schnall, Benton, & Harvey, 2008)\\[3\\]. If reliable, this effect might help explain why so many religious traditions associate physical cleanliness with moral purity. However, despite using the same materials and nearly identical procedures with a much larger sample, the \u201cMany Labs\u201d researchers were unable to replicate the original finding (Johnson, Cheung, & Donnellan, 2013)\\[4\\], suggesting that the original finding may have stemmed from the relatively small sample size (which can lead to unreliable results) used in the original study. To be clear, at this stage we are still unable to definitively conclude that the handwashing effect does not exist; however, the effort that has gone into testing its reliability certainly demonstrates the collaborative and cautious nature of scientific progress.\n\nFor more on the replication crisis in psychology see: http:\/\/nobaproject.com\/modules\/the-replication-crisis-in-psychology\n\nPseudoscience refers to activities and beliefs that are claimed to be scientific by their proponents\u2014and may appear to be scientific at first glance\u2014but are not. Consider the theory of biorhythms (not to be confused with sleep cycles or circadian rhythms that do have a scientific basis). The idea is that people\u2019s physical, intellectual, and emotional abilities run in cycles that begin when they are born and continue until they die. Allegedly, the physical cycle has a period of 23 days, the intellectual cycle a period of 33 days, and the emotional cycle a period of 28 days. So, for example, if you had the option of when to schedule an exam, you would want to schedule it for a time when your intellectual cycle will be at a high point. The theory of biorhythms has been around for more than 100 years, and you can find numerous popular books and websites about biorhythms, often containing impressive and scientific-sounding terms like sinusoidal wave and bioelectricity. The problem with biorhythms, however, is that scientific evidence indicates they do not exist (Hines, 1998)\\[5\\].\n\nA set of beliefs or activities can be said to be pseudoscientific if (a) its adherents claim or imply that it is scientific but (b) it lacks one or more of the three features of science. For instance, it might lack systematic empiricism. Either there is no relevant scientific research or, as in the case of biorhythms, there is relevant scientific research but it is ignored. It might also lack public knowledge. People who promote the beliefs or activities might claim to have conducted scientific research but never publish that research in a way that allows others to evaluate it.\u00a0\n\nA set of beliefs and activities might also be pseudoscientific because it does not address empirical questions. The philosopher Karl Popper was especially concerned with this idea (Popper, 2002)\\[6\\]. He argued more specifically that any scientific claim must be expressed in such a way that there are observations that would\u2014if they were made\u2014count as evidence against the claim. In other words, scientific claims must be falsifiable. The claim that women talk more than men is falsifiable because systematic observations could reveal either that they do talk more than men or that they do not. As an example of an unfalsifiable claim, consider that many people who believe in extrasensory perception (ESP) and other psychic powers claim that such powers can disappear when they are observed too closely. This makes it so that no possible observation would count as evidence against ESP. If a careful test of a self-proclaimed psychic showed that she predicted the future at better-than-chance levels, this would be consistent with the claim that she had psychic powers. But if she failed to predict the future at better-than-chance levels, this would also be consistent with the claim because her powers can supposedly disappear when they are observed too closely.\n\nWhy should we concern ourselves with pseudoscience? There are at least three reasons. One is that learning about pseudoscience helps bring the fundamental features of science\u2014and their importance\u2014into sharper focus. A second is that biorhythms, psychic powers, astrology, and many other pseudoscientific beliefs are widely held and are promoted on the Internet, on television, and in books and magazines. Far from being harmless, the promotion of these beliefs often results in great personal toll as, for example, believers in pseudoscience opt for \u201ctreatments\u201d such as homeopathy for serious medical conditions instead of empirically-supported treatments. Learning what makes them pseudoscientific can help us to identify and evaluate such beliefs and practices when we encounter them. A third reason is that many pseudosciences purport to explain some aspect of human behavior and mental processes, including biorhythms, astrology, graphology (handwriting analysis), and magnet therapy for pain control. It is important for students of psychology to distinguish their own field clearly from this \u201cpseudo psychology.\u201d\n\n<i-callout>\n\n_**The Skeptic\u2019s Dictionary**_\n\nAn excellent source for information on pseudoscience is _The Skeptic\u2019s Dictionary_ ([http:\/\/www.skepdic.com](http:\/\/www.skepdic.com\/)). Among the pseudoscientific beliefs and practices you can learn about are the following:\n\n* **Cryptozoology.**\u00a0The study of \u201chidden\u201d creatures like Bigfoot, the Loch Ness monster, and the chupacabra.\n* **Pseudoscientific psychotherapies.**\u00a0Past-life regression, rebirthing therapy, and bioscream therapy, among others.\n* **Homeopathy.**\u00a0The treatment of medical conditions using natural substances that have been diluted sometimes to the point of no longer being present.\n* **Pyramidology.**\u00a0Odd theories about the origin and function of the Egyptian pyramids (e.g., that they were built by extraterrestrials) and the idea that pyramids, in general, have healing and other special powers.\n\nAnother excellent online resource is _Neurobonkers_ ([http:\/\/neurobonkers.com](http:\/\/neurobonkers.com\/)), which regularly posts articles that investigate claims that pertain specifically to psychological science.\n\n<\/i-callout>\n\n1.  Stanovich, K. E. (2010). _How to think straight about psychology_ (9th ed.). Boston, MA: Allyn & Bacon.\n2.  Klein, R. A., Ratliff, K. A., Vianello, M., Adams, R. B., Bahn\u00edk, S., Bernstein, M. J., . . . Nosek, B. A. (2013). Investigating variation in replicability: A \u201cmany labs\u201d replication project. _Social Psychology, 45_(3), 142-152. doi: 10.1027\/1864-9335\/a000178\n3.  Schnall, S., Benton, J., & Harvey, S. (2008). With a clean conscience: Cleanliness reduces the severity of moral judgments. _Psychological Science, 19_(12), 1219-1222. doi: 10.1111\/j.1467-9280.2008.02227.x\n4.  Johnson, D. J., Cheung, F., & Donnellan, M. B. (2013). Does cleanliness influence moral judgments? A direct replication of Schnall, Benton, and Harvey (2008). _Social Psychology, 45_(3), 209-215. doi: 10.1027\/1864-9335\/a000186\n5.  Hines, T. M. (1998). Comprehensive review of biorhythm theory. _Psychological Reports, 83_, 19\u201364.\n6.  Popper, K. R. (2002). _Conjectures and refutations: The growth of scientific knowledge_. New York, NY: Routledge.","text":"Learning Objectives\n\n1. Define science.\n2. Describe the three fundamental features of science.\n3. Explain why psychology is a science.\u00a0\n4. Define pseudoscience and give some examples.\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nSome people are surprised to learn that psychology is a science. They generally agree that astronomy, biology, and chemistry are sciences but wonder what psychology has in common with these other fields. Before answering this question, however, it is worth reflecting on what astronomy, biology, and chemistry have in common with each other. It is clearly not their subject matter. Astronomers study celestial bodies, biologists study living organisms, and chemists study matter and its properties. It is also not the equipment and techniques that they use. Few biologists would know what to do with a radio telescope, for example, and few chemists would know how to track a moose population in the wild. For these and other reasons, philosophers and scientists who have thought deeply about this question have concluded that what the sciences have in common is a general approach to understanding the natural world. Psychology is a science because it takes this same general approach to understanding one aspect of the natural world: human behavior.\u00a0\n\nThe general scientific approach has three fundamental features (Stanovich, 2010)[1]. The first is systematic empiricism. Empiricism refers to learning based on observation, and scientists learn about the natural world systematically, by carefully planning, making, recording, and analyzing observations of it. As we will see, logical reasoning and even creativity play important roles in science too, but scientists are unique in their insistence on checking their ideas about the way the world is against their systematic observations. Notice, for example, that Mehl and his colleagues did not trust other people\u2019s stereotypes or even their own informal observations. Instead, they systematically recorded, counted, and compared the number of words spoken by a large sample of women and men. Furthermore, when their systematic observations turned out to conflict with people\u2019s stereotypes, they trusted their systematic observations.\u00a0\nThe second feature of the scientific approach\u2014which follows in a straightforward way from the first\u2014is that it is concerned with empirical questions. These are questions about the way the world actually is and, therefore, can be answered by systematically observing it. The question of whether women talk more than men is empirical in this way. Either women really do talk more than men or they do not, and this can be determined by systematically observing how much women and men actually talk. Having said this, there are many interesting and important questions that are not empirically testable and that science is not in a position to answer. Among these are questions about values\u2014whether things are good or bad, just or unjust, or beautiful or ugly, and how the world ought to be. So although the question of whether a stereotype is accurate or inaccurate is an empirically testable one that science can answer, the question\u2014or, rather, the value judgment\u2014of whether it is wrong for people to hold inaccurate stereotypes is not. Similarly, the question of whether criminal behavior has a genetic basis is an empirical question, but the question of what actions ought to be considered illegal is not. It is especially important for researchers in psychology to be mindful of this distinction.\nThe third feature of science is that it creates public knowledge. After asking their empirical questions, making their systematic observations, and drawing their conclusions, scientists publish their work. This usually means writing an article for publication in a professional journal, in which they put their research question in the context of previous research, describe in detail the methods they used to answer their question, and clearly present their results and conclusions. Increasingly, scientists are opting to publish their work in open access journals, in which the articles are freely available to all \u2013 scientists and nonscientists alike. This important choice allows publicly-funded research to create knowledge that is truly public.\n\nPublication is an essential feature of science for two reasons. One is that science is a social process\u2014a large-scale collaboration among many researchers distributed across both time and space. Our current scientific knowledge of most topics is based on many different studies conducted by many different researchers who have shared their work publicly over many years. The second is that publication allows science to be self-correcting. Individual scientists understand that, despite their best efforts, their methods can be flawed and their conclusions incorrect. Publication allows others in the scientific community to detect and correct these errors so that, over time, scientific knowledge increasingly reflects the way the world actually is.\u00a0\n\n\nA good example of the self-correcting nature of science is the \u201cMany Labs Replication Project\u201d \u2013 a large and coordinated effort by prominent psychological scientists around the world to attempt to replicate findings from 13 classic and contemporary studies (Klein et al., 2013)[2]. One of the findings selected by these researchers for replication was the fascinating effect, first reported by Simone Schnall and her colleagues at the University of Plymouth, that washing one\u2019s hands leads people to view moral transgressions\u2014ranging from keeping money inside a found wallet to using a kitten for sexual arousal\u2014as less wrong (Schnall, Benton, & Harvey, 2008)[3]. If reliable, this effect might help explain why so many religious traditions associate physical cleanliness with moral purity. However, despite using the same materials and nearly identical procedures with a much larger sample, the \u201cMany Labs\u201d researchers were unable to replicate the original finding (Johnson, Cheung, & Donnellan, 2013)[4], suggesting that the original finding may have stemmed from the relatively small sample size (which can lead to unreliable results) used in the original study. To be clear, at this stage we are still unable to definitively conclude that the handwashing effect does not exist; however, the effort that has gone into testing its reliability certainly demonstrates the collaborative and cautious nature of scientific progress.\n\n\nFor more on the replication crisis in psychology see: http:\/\/nobaproject.com\/modules\/the-replication-crisis-in-psychology\n\nPseudoscience refers to activities and beliefs that are claimed to be scientific by their proponents\u2014and may appear to be scientific at first glance\u2014but are not. Consider the theory of biorhythms (not to be confused with sleep cycles or circadian rhythms that do have a scientific basis). The idea is that people\u2019s physical, intellectual, and emotional abilities run in cycles that begin when they are born and continue until they die. Allegedly, the physical cycle has a period of 23 days, the intellectual cycle a period of 33 days, and the emotional cycle a period of 28 days. So, for example, if you had the option of when to schedule an exam, you would want to schedule it for a time when your intellectual cycle will be at a high point. The theory of biorhythms has been around for more than 100 years, and you can find numerous popular books and websites about biorhythms, often containing impressive and scientific-sounding terms like sinusoidal wave and bioelectricity. The problem with biorhythms, however, is that scientific evidence indicates they do not exist (Hines, 1998)[5].\nA set of beliefs or activities can be said to be pseudoscientific if (a) its adherents claim or imply that it is scientific but (b) it lacks one or more of the three features of science. For instance, it might lack systematic empiricism. Either there is no relevant scientific research or, as in the case of biorhythms, there is relevant scientific research but it is ignored. It might also lack public knowledge. People who promote the beliefs or activities might claim to have conducted scientific research but never publish that research in a way that allows others to evaluate it.\u00a0\nA set of beliefs and activities might also be pseudoscientific because it does not address empirical questions. The philosopher Karl Popper was especially concerned with this idea (Popper, 2002)[6]. He argued more specifically that any scientific claim must be expressed in such a way that there are observations that would\u2014if they were made\u2014count as evidence against the claim. In other words, scientific claims must be falsifiable. The claim that women talk more than men is falsifiable because systematic observations could reveal either that they do talk more than men or that they do not. As an example of an unfalsifiable claim, consider that many people who believe in extrasensory perception (ESP) and other psychic powers claim that such powers can disappear when they are observed too closely. This makes it so that no possible observation would count as evidence against ESP. If a careful test of a self-proclaimed psychic showed that she predicted the future at better-than-chance levels, this would be consistent with the claim that she had psychic powers. But if she failed to predict the future at better-than-chance levels, this would also be consistent with the claim because her powers can supposedly disappear when they are observed too closely.\nWhy should we concern ourselves with pseudoscience? There are at least three reasons. One is that learning about pseudoscience helps bring the fundamental features of science\u2014and their importance\u2014into sharper focus. A second is that biorhythms, psychic powers, astrology, and many other pseudoscientific beliefs are widely held and are promoted on the Internet, on television, and in books and magazines. Far from being harmless, the promotion of these beliefs often results in great personal toll as, for example, believers in pseudoscience opt for \u201ctreatments\u201d such as homeopathy for serious medical conditions instead of empirically-supported treatments. Learning what makes them pseudoscientific can help us to identify and evaluate such beliefs and practices when we encounter them. A third reason is that many pseudosciences purport to explain some aspect of human behavior and mental processes, including biorhythms, astrology, graphology (handwriting analysis), and magnet therapy for pain control. It is important for students of psychology to distinguish their own field clearly from this \u201cpseudo psychology.\u201d\n\nThe Skeptic\u2019s Dictionary\n\nAn excellent source for information on pseudoscience is The Skeptic\u2019s Dictionary (http:\/\/www.skepdic.com). Among the pseudoscientific beliefs and practices you can learn about are the following:\n\nCryptozoology.\u00a0The study of \u201chidden\u201d creatures like Bigfoot, the Loch Ness monster, and the chupacabra.Pseudoscientific psychotherapies.\u00a0Past-life regression, rebirthing therapy, and bioscream therapy, among others.Homeopathy.\u00a0The treatment of medical conditions using natural substances that have been diluted sometimes to the point of no longer being present.Pyramidology.\u00a0Odd theories about the origin and function of the Egyptian pyramids (e.g., that they were built by extraterrestrials) and the idea that pyramids, in general, have healing and other special powers.\n\nAnother excellent online resource is Neurobonkers (http:\/\/neurobonkers.com), which regularly posts articles that investigate claims that pertain specifically to psychological science.\u00a0\n\nStanovich, K. E. (2010). How to think straight about psychology (9th ed.). Boston, MA: Allyn & Bacon.Klein, R. A., Ratliff, K. A., Vianello, M., Adams, R. B., Bahn\u00edk, S., Bernstein, M. J., . . . Nosek, B. A. (2013). Investigating variation in replicability: A \u201cmany labs\u201d replication project. Social Psychology, 45(3), 142-152. doi: 10.1027\/1864-9335\/a000178Schnall, S., Benton, J., & Harvey, S. (2008). With a clean conscience: Cleanliness reduces the severity of moral judgments. Psychological Science, 19(12), 1219-1222. doi: 10.1111\/j.1467-9280.2008.02227.xJohnson, D. J., Cheung, F., & Donnellan, M. B. (2013). Does cleanliness influence moral judgments? A direct replication of Schnall, Benton, and Harvey (2008). Social Psychology, 45(3), 209-215. doi: 10.1027\/1864-9335\/a000186Hines, T. M. (1998). Comprehensive review of biorhythm theory. Psychological Reports, 83, 19\u201364.Popper, K. R. (2002). Conjectures and refutations: The growth of scientific knowledge. New York, NY: Routledge. \u00a0","contextuality":{"text":"Psychology, often surprising to many, is a science similar to astronomy, biology, and chemistry in its methodological approach to understanding the natural world, particularly human behavior. This scientific approach is defined by three core __________: systematic empiricism, empirical questions, and public knowledge. Systematic empiricism ________ carefully planned and recorded observations, as demonstrated by studies that challenge ___________ through data rather than assumptions. Empirical _________, such as gender differences in communication, are those that can be ________ through observation, unlike value-based questions. Public knowledge is achieved through publishing research, enabling scientific ________ through collaboration and self-correction, as seen in efforts ____ the Many Labs Replication Project. In contrast, pseudoscience lacks these principles, often failing to provide empirical evidence or public scrutiny, and can ____ to harmful beliefs and practices. Understanding these distinctions __________ the importance of scientific rigor and helps differentiate legitimate scientific _______ from pseudoscientific claims.","gaps":[["principles",242,10],["involves",342,8],["stereotypes",438,11],["questions",498,9],["answered",576,8],["progress",714,8],["like",785,4],["lead",948,4],["highlights",1020,10],["inquiry",1112,7]]},"contextuality_plus":{"text":"Psychology, often surprising to many, is a science similar to astronomy, biology, and chemistry in its methodological approach to understanding the natural world, particularly human behavior. This scientific approach is defined by three core __________: systematic empiricism, empirical questions, and public knowledge. Systematic empiricism ________ carefully planned and recorded observations, as demonstrated by studies that challenge ___________ through data rather than assumptions. Empirical questions, such as gender ___________ in communication, are those that can be answered through observation, ______ value-based questions. Public knowledge is ________ through publishing research, enabling scientific progress through _____________ and self-correction, as seen in efforts like the Many Labs Replication Project. In contrast, pseudoscience lacks these principles, often failing to provide empirical evidence or ______ scrutiny, and can lead to harmful beliefs and practices. Understanding these ____________ highlights the importance of scientific rigor and helps differentiate legitimate scientific _______ from pseudoscientific claims.","gaps":[["principles",242,10],["involves",342,8],["stereotypes",438,11],["differences",524,11],["unlike",606,6],["achieved",656,8],["collaboration",731,13],["public",923,6],["distinctions",1007,12],["inquiry",1112,7]]},"keyword":{"text":"Psychology, often surprising to many, is a science similar to astronomy, biology, and chemistry in its methodological approach to understanding the natural world, particularly human behavior. This scientific ________ is defined by three core principles: systematic empiricism, _________ questions, and public knowledge. Systematic empiricism involves carefully planned and recorded observations, as demonstrated by studies that challenge stereotypes through data rather than assumptions. Empirical _________, such as gender differences in communication, are those that can be answered through observation, unlike value-based _________. Public knowledge is achieved through publishing research, enabling __________ progress through collaboration and self-correction, as seen in efforts like the Many Labs Replication Project. In contrast, pseudoscience lacks these principles, often failing to provide _________ evidence or public scrutiny, and can lead to harmful beliefs and practices. Understanding these distinctions highlights the importance of __________ rigor and helps differentiate legitimate __________ inquiry from pseudo__________ claims.","gaps":[["empirical",277,9],["empirical",901,9],["approach",208,8],["scientific",703,10],["scientific",1049,10],["scientific",1101,10],["scientific",1131,10],["questions",498,9],["questions",625,9]]}}
{"volume":"research-methods-in-psychology","page":"1-methods-of-knowing","summary":"The text discusses various methods of acquiring knowledge, including intuition, authority, rationalism, empiricism, and the scientific method. Intuition relies on gut feelings and instincts, which can be flawed due to cognitive biases. Authority involves accepting ideas from figures like parents or the media, though history shows such sources can be misleading. Rationalism uses logic to draw conclusions, but incorrect premises can lead to false results. Empiricism is based on observation and experience, yet can be deceptive due to sensory limitations. The scientific method, combining rationalism and structured observations, is the most reliable for producing valid knowledge, though it has limitations such as resource intensity and applicability only to empirical questions.","markdown":"<i-callout variant=\"info\" title=\"Learning Objectives\">\n\n1\\. Describe the 5 methods of acquiring knowledge.\u00a0\n\n2\\. Understand the benefits and problems with each.\n\n<\/i-callout>\n\nTake a minute to ponder some of what you know and how you acquired that knowledge. Perhaps you know that you should make your bed in the morning because your mother or father told you this is what you should do, perhaps you know that swans are white because all of the swans you have seen are white, or perhaps you know that your friend is lying to you because she is acting strange and won\u2019t look you in the eye. But should we trust knowledge from these sources? The methods of acquiring knowledge can be broken down into five categories each with its own strengths and weaknesses.\n\n### Intuition\n\nThe first method of knowing is intuition. When we use our intuition, we are relying on our guts, our emotions, and\/or our instincts to guide us. Rather than examining facts or using rational thought, intuition involves believing what feels true. The problem with relying on intuition is that our intuitions can be wrong because they are driven by cognitive and motivational biases rather than logical reasoning or scientific evidence. While the strange behavior of your friend may lead you to think s\/he is lying to you it may just be that s\/he is holding in a bit of gas or is preoccupied with some other issue that is irrelevant to you. However, weighing alternatives and thinking of all the different possibilities can be paralyzing for some people and sometimes decisions based on intuition are actually superior to those based on analysis (people interested in this idea should read Malcolm Gladwell\u2019s book Blink). \u00a0\n\n### Authority\n\nPerhaps one of the most common methods of acquiring knowledge is through authority. This method involves accepting new ideas because some authority figure states that they are true. These authorities include parents, the media, doctors, Priests and other religious authorities, the government, and professors. While in an ideal world we should be able to trust authority figures, history has taught us otherwise and many instances of atrocities against humanity are a consequence of people unquestioningly following authority (e.g., Salem Witch Trials, Nazi War Crimes). On a more benign level, while your parents may have told you that you should make your bed in the morning, making your bed provides the warm damp environment in which mites thrive. Keeping the sheets open provides a less hospitable environment for mites. These examples illustrate that the problem with using authority to obtain knowledge is that they may be wrong, they may just be using their intuition to arrive at their conclusions, and they may have their own reasons to mislead you. Nevertheless, much of the information we acquire is through authority because we don\u2019t have time to question and independently research every piece of knowledge we learn through authority. But we can learn to evaluate the credentials of authority figures, to evaluate the methods they used to arrive at their conclusions, and evaluate whether they have any reasons to mislead us.\n\n### Rationalism\n\nRationalism involves using logic and reasoning to acquire new knowledge. Using this method premises are stated and logical rules are followed to arrive at sound conclusions. For instance, if I am given the premise that all swans are white and the premise that this is a swan then I can come to the rational conclusion that this swan is white without actually seeing the swan. The problem with this method is that if the premises are wrong or there is an error in logic then the conclusion will not be valid. For instance, the premise that all swans are white is incorrect; there are black swans in Australia. Also, unless formally trained in the rules of logic it is easy to make an error. Nevertheless, if the premises are correct and logical rules are followed appropriately then this is sound means of acquiring knowledge.\u00a0\n\n### Empiricism\n\nEmpiricism involves acquiring knowledge through observation and experience. Once again many of you may have believed that all swans are white because you have only ever seen white swans. For centuries people believed the world is flat because it appears to be flat. These examples and the many visual illusions that trick our senses illustrate the problems with relying on empiricism alone to derive knowledge. We are limited in what we can experience and observe and our senses can deceive us. Moreover, our prior experiences can alter the way we perceive events. Nevertheless, empiricism is at the heart of the scientific method. Science relies on observations. But not just any observations, science relies on structured observations which is known as systematic empiricism.\n\n### The Scientific Method\n\nThe scientific method is a process of systematically collecting and evaluating evidence to test ideas and answer questions. While scientists may use intuition, authority, rationalism, and empiricism to generate new ideas they don\u2019t stop there. Scientists go a step further by using systematic empiricism to make careful observations under various controlled conditions in order to test their ideas and they use rationalism to arrive at valid conclusions. While the scientific method is the most likely of all of the methods to produce valid knowledge, like all methods of acquiring knowledge it also has its drawbacks. One major problem is that it is not always feasible to use the scientific method; this method can require considerable time and resources. Another problem with the scientific method is that it cannot be used to answer all questions. As described in the following section, the scientific method can only be used to address empirical questions. This book and your research methods course are designed to provide you with an in-depth examination of how psychologists use the scientific method to advance our understanding of human behavior and the mind.\n\n1.  Gladwell, M. E. (2005). _Blink: The power of thinking without thinking_. (9th ed.). New York: Little, Brown & Co.","text":"Learning Objectives\n\n1. Describe the 5 methods of acquiring knowledge.\u00a0\n2. Understand the benefits and problems with each.\u00a0\n\nTake a minute to ponder some of what you know and how you acquired that knowledge. Perhaps you know that you should make your bed in the morning because your mother or father told you this is what you should do, perhaps you know that swans are white because all of the swans you have seen are white, or perhaps you know that your friend is lying to you because she is acting strange and won\u2019t look you in the eye. But should we trust knowledge from these sources? The methods of acquiring knowledge can be broken down into five categories each with its own strengths and weaknesses.\u00a0\n\nIntuition\n\nThe first method of knowing is intuition. When we use our intuition, we are relying on our guts, our emotions, and\/or our instincts to guide us. Rather than examining facts or using rational thought, intuition involves believing what feels true. The problem with relying on intuition is that our intuitions can be wrong because they are driven by cognitive and motivational biases rather than logical reasoning or scientific evidence. While the strange behavior of your friend may lead you to think s\/he is lying to you it may just be that s\/he is holding in a bit of gas or is preoccupied with some other issue that is irrelevant to you. However, weighing alternatives and thinking of all the different possibilities can be paralyzing for some people and sometimes decisions based on intuition are actually superior to those based on analysis (people interested in this idea should read Malcolm Gladwell\u2019s book Blink). \u00a0\n\nAuthority\n\nPerhaps one of the most common methods of acquiring knowledge is through authority. This method involves accepting new ideas because some authority figure states that they are true. These authorities include parents, the media, doctors, Priests and other religious authorities, the government, and professors. While in an ideal world we should be able to trust authority figures, history has taught us otherwise and many instances of atrocities against humanity are a consequence of people unquestioningly following authority (e.g., Salem Witch Trials, Nazi War Crimes). On a more benign level, while your parents may have told you that you should make your bed in the morning, making your bed provides the warm damp environment in which mites thrive. Keeping the sheets open provides a less hospitable environment for mites. These examples illustrate that the problem with using authority to obtain knowledge is that they may be wrong, they may just be using their intuition to arrive at their conclusions, and they may have their own reasons to mislead you. Nevertheless, much of the information we acquire is through authority because we don\u2019t have time to question and independently research every piece of knowledge we learn through authority. But we can learn to evaluate the credentials of authority figures, to evaluate the methods they used to arrive at their conclusions, and evaluate whether they have any reasons to mislead us.\u00a0\n\nRationalism\n\nRationalism involves using logic and reasoning to acquire new knowledge. Using this method premises are stated and logical rules are followed to arrive at sound conclusions. For instance, if I am given the premise that all swans are white and the premise that this is a swan then I can come to the rational conclusion that this swan is white without actually seeing the swan. The problem with this method is that if the premises are wrong or there is an error in logic then the conclusion will not be valid. For instance, the premise that all swans are white is incorrect; there are black swans in Australia. Also, unless formally trained in the rules of logic it is easy to make an error. Nevertheless, if the premises are correct and logical rules are followed appropriately then this is sound means of acquiring knowledge.\u00a0\n\n\u00a0\n\nEmpiricism\n\nEmpiricism involves acquiring knowledge through observation and experience. Once again many of you may have believed that all swans are white because you have only ever seen white swans. For centuries people believed the world is flat because it appears to be flat. These examples and the many visual illusions that trick our senses illustrate the problems with relying on empiricism alone to derive knowledge. We are limited in what we can experience and observe and our senses can deceive us. Moreover, our prior experiences can alter the way we perceive events. Nevertheless, empiricism is at the heart of the scientific method. Science relies on observations. But not just any observations, science relies on structured observations which is known as systematic empiricism.\n\n\u00a0\n\nThe Scientific Method\n\nThe scientific method is a process of systematically collecting and evaluating evidence to test ideas and answer questions. While scientists may use intuition, authority, rationalism, and empiricism to generate new ideas they don\u2019t stop there. Scientists go a step further by using systematic empiricism to make careful observations under various controlled conditions in order to test their ideas and they use rationalism to arrive at valid conclusions. While the scientific method is the most likely of all of the methods to produce valid knowledge, like all methods of acquiring knowledge it also has its drawbacks. One major problem is that it is not always feasible to use the scientific method; this method can require considerable time and resources. Another problem with the scientific method is that it cannot be used to answer all questions. As described in the following section, the scientific method can only be used to address empirical questions. This book and your research methods course are designed to provide you with an in-depth examination of how psychologists use the scientific method to advance our understanding of human behavior and the mind.\n\nGladwell, M. E. (2005). Blink: The power of thinking without thinking. (9th ed.). New York: Little, Brown & Co. \u00a0","contextuality":{"text":"The text discusses various methods of acquiring knowledge, including intuition, authority, rationalism, empiricism, and the scientific method. Intuition relies on gut feelings and _________, which can be flawed due to cognitive biases. Authority ________ accepting ideas from figures like parents or the media, though history _____ such sources can be misleading. Rationalism uses logic to draw conclusions, but _________ premises can lead to false results. Empiricism is _____ on observation and experience, yet can be deceptive due to sensory ___________. The scientific method, combining rationalism and structured ____________, is the most reliable for producing valid knowledge, though it has ___________ such as resource intensity and applicability only to empirical _________.","gaps":[["instincts",180,9],["involves",246,8],["shows",326,5],["incorrect",412,9],["based",472,5],["limitations",545,11],["observations",618,12],["limitations",698,11],["questions",773,9]]},"contextuality_plus":{"text":"The text discusses various methods of acquiring knowledge, including intuition, authority, rationalism, empiricism, and the scientific method. Intuition relies on gut feelings and _________, which can be flawed due to cognitive biases. Authority ________ accepting ideas from figures like parents or the media, though history _____ such sources can be misleading. Rationalism uses logic to draw conclusions, but _________ premises can lead to false results. Empiricism is _____ on observation and experience, yet can be deceptive due to sensory ___________. The scientific method, combining rationalism and structured ____________, is the most reliable for producing valid knowledge, though it has ___________ such as resource intensity and applicability only to _________ questions.","gaps":[["instincts",180,9],["involves",246,8],["shows",326,5],["incorrect",412,9],["based",472,5],["limitations",545,11],["observations",618,12],["limitations",698,11],["empirical",763,9]]},"keyword":{"text":"The text discusses various methods of acquiring knowledge, including intuition, authority, rationalism, empiricism, and the scientific method. Intuition relies on gut feelings and instincts, which can be flawed due to cognitive biases. Authority involves accepting ideas from figures like parents or the media, though history shows such sources can be misleading. Rationalism uses logic to draw conclusions, but incorrect premises can lead to false results. Empiricism is based on observation and experience, yet can be deceptive due to sensory limitations. The __________ method, combining rationalism and structured observations, is the most reliable for producing valid _________, though it has limitations such as resource intensity and applicability only to empirical questions.","gaps":[["knowledge",673,9],["scientific",562,10]]}}
{"volume":"research-methods-in-psychology-demo","page":"11-designing-a-research-study-1","summary":"In psychological research, generating a hypothesis involves identifying and operationally defining variables to enable measurement. Variables, which can be quantitative (like height) or categorical (like chosen major), vary across individuals or situations. Operational definitions transform abstract constructs, such as depression, into measurable entities, often using established methods in the literature. Researchers must also determine the population of interest, using representative samples to generalize findings. Sampling methods include simple random sampling and convenience sampling, though the latter may not be representative. Research can be experimental, manipulating independent variables to determine causal relationships, or non-experimental, observing variables as they naturally occur. Laboratory studies offer high internal validity by controlling extraneous variables, while field studies provide higher external validity by reflecting real-world settings. Despite challenges, careful design and control in experiments can enhance both internal and external validity, allowing for meaningful conclusions about variable relationships.","markdown":"<i-callout variant=\"info\" title=\"Learning Objectives\">\n\n1\\. Define the concept of a variable, distinguish quantitative from categorical variables, and give examples of variables that might be of interest to psychologists.\n\n2\\. Explain the difference between a population and a sample.\n\n3\\. Distinguish between experimental and non-experimental research.\u00a0\n\n4\\. Distinguish between lab studies, field studies, and field experiments.\n\n<\/i-callout>\n\n### Variables and Operational Definitions \u00a0\n\nPart of generating a hypothesis involves identifying the variables that you want to study and operationally defining those variables so that they can be measured. Research questions in psychology are about variables. A\u00a0**variable**\u00a0is a quantity or quality that varies across people or situations. For example, the height of the students enrolled in a university course is a variable because it varies from student to student. The chosen major of the students is also a variable as long as not everyone in the class has declared the same major. Almost everything in our world varies and as such thinking of examples of constants (things that don\u2019t vary) is far more difficult. A rare example of a constant is the speed of light. Variables can be either quantitative or categorical. A\u00a0**quantitative\u00a0variable**\u00a0is a quantity, such as height, that is typically measured by assigning a number to each individual. Other examples of quantitative variables include people\u2019s level of talkativeness, how depressed they are, and the number of siblings they have. A **categorical\u00a0variable**\u00a0is a quality, such as chosen major, and is typically measured by assigning a category label to each individual (e.g., Psychology, English, Nursing, etc.). Other examples include people\u2019s nationality, their occupation, and whether they are receiving psychotherapy.\n\nAfter the researcher generates their hypothesis and selects the variables they want to manipulate and measure, the researcher needs to find ways to actually measure the variables of interest. This requires an\u00a0**operational\u00a0definition**\u2014a definition of the variable in terms of precisely how it is to be measured. Most variables that researchers are interested in studying cannot be directly observed or measured and this poses a problem because empiricism (observation) is at the heart of the scientific method. Operationally defining a variable involves taking an abstract construct like depression that cannot be directly observed and transforming it into something that can be directly observed and measured. Most variables can be operationally defined in many different ways. For example, depression can be operationally defined as people\u2019s scores on a paper-and-pencil depression scale such as the Beck Depression Inventory, the number of depressive symptoms they are experiencing, or whether they have been diagnosed with major depressive disorder. Researchers are wise to choose an operational definition that has been used extensively in the research literature.\n\nIn addition to identifying which variables to manipulate and measure, and operationally defining those variables, researchers need to identify the population of interest. Researchers in psychology are usually interested in drawing conclusions about some very large group of people. This is called the\u00a0**population**. It could be all American teenagers, children with autism, professional athletes, or even just human beings\u2014depending on the interests and goals of the researcher. But they usually study only a small subset or\u00a0**sample**\u00a0of the population. For example, a researcher might measure the talkativeness of a few hundred university students with the intention of drawing conclusions about the talkativeness of men and women in general. It is important, therefore, for researchers to use a representative sample\u2014one that is similar to the population in important respects.\u00a0\n\nOne method of obtaining a sample is **simple random sampling**, in which every member of the population has an equal chance of being selected for the sample. For example, a pollster could start with a list of all the registered voters in a city (the population), randomly select 100 of them from the list (the sample), and ask those 100 whom they intend to vote for. Unfortunately, random sampling is difficult or impossible in most psychological research because the populations are less clearly defined than the registered voters in a city. How could a researcher give all American teenagers or all children with autism an equal chance of being selected for a sample? The most common alternative to random sampling is **convenience sampling**, in which the sample consists of individuals who happen to be nearby and willing to participate (such as introductory psychology students). Of course, the obvious problem with convenience sampling is that the sample might not be representative of the population and therefore it may be less appropriate to generalize the results from the sample to that population.\n\nThe next step a researcher must take is to decide which type of approach they will use to collect the data. As you will learn in your research methods course there are many different approaches to research that can be divided in many different ways. One of the most fundamental distinctions is between experimental and non-experimental research.\n\nResearchers who want to test hypotheses about causal relationships between variables (i.e., their goal is to explain) need to use an experimental method. This is because the experimental method is the only method that allows us to determine causal relationships. Using the experimental approach, researchers first manipulate one or more variables while attempting to control extraneous variables, and then they measure how the manipulated variables affect participants\u2019 responses.\u00a0\n\nThe terms independent variable and dependent variable are used in the context of experimental research. The **independent variable** is the variable the experimenter manipulates (it is the presumed cause) and the **dependent variable** is the variable the experimenter measures (it is the presumed effect).\n\n**Extraneous variables**\u00a0are any variable other than the dependent variable. **Confounds** are a specific type of extraneous variable that systematically varies along with the variables under investigation and therefore provides an alternative explanation for the results. When researchers design an experiment they need to ensure that they control for confounds; they need to ensure that extraneous variables don\u2019t become confounding variables because in order to make a causal conclusion they need to make sure alternative explanations for the results have been ruled out.\n\nAs an example, if we manipulate the lighting in the room and examine the effects of that manipulation on workers\u2019 productivity,\u00a0then the lighting conditions (bright lights vs. dim lights) would be considered the independent variable and the workers\u2019 productivity would be considered the dependent variable. If the bright lights are noisy then that noise would be a confound since the noise would be present whenever the lights are bright and the noise would be absent when the lights are dim. If noise is varying systematically with light then we wouldn\u2019t know if a difference in worker productivity across the two lighting conditions is due to noise or light. So confounds are bad, they disrupt our ability to make causal conclusions about the nature of the relationship between variables. However, if there is noise in the room both when the lights are on and when the lights are off then noise is merely an extraneous variable (it is a variable other than the independent or dependent variable) and we don\u2019t worry much about extraneous variables. This is because unless a variable varies systematically with the manipulated independent variable it cannot be a competing explanation for the results.\n\nResearchers who are simply interested in describing characteristics of people, describing relationships between variables, and using those relationships to make predictions can use non-experimental research. Using the non-experimental approach, the researcher simply measures variables as they naturally occur, but they do not manipulate them. For instance, if I just measured the number of traffic fatalities in America last year that involved the use of a cell phone but I did not actually manipulate cell phone use then this would be categorized as non-experimental research. Alternatively, if I stood at a busy intersection and recorded drivers\u2019 genders and whether or not they were using a cell phone when they passed through the intersection to see whether men or women are more likely to use a cell phone when driving, then this would be non-experimental research. It is important to point out that non-experimental does not mean nonscientific. Non-experimental research is scientific in nature. It can be used to fulfill two of the three goals of science (to describe and to predict). However, unlike with experimental research, we cannot make causal conclusions using this method; we cannot say that one variable causes another variable using this method.\n\nThe next major distinction between research methods is between laboratory and field studies. A **laboratory study** is a study that is conducted in the laboratory environment. In contrast, a **field study** is a study that is conducted in the real-world, in a natural environment.\n\nLaboratory experiments typically have high\u00a0**internal validity**. Internal validity refers to the degree to which we can confidently infer a causal relationship between variables. When we conduct an experimental study in a laboratory environment we have very high internal validity because we manipulate one variable while controlling all other outside extraneous variables. When we manipulate an independent variable and observe an effect on a dependent variable and we control for everything else so that the only difference between our experimental groups or conditions is the one manipulated variable then we can be quite confident that it is the independent variable that is causing the change in the dependent variable. In contrast, because field studies are conducted in the real-world, the experimenter typically has less control over the environment and potential extraneous variables, and this decreases internal validity, making it less appropriate to arrive at causal conclusions.\u00a0\n\nBut there is typically a trade-off between internal and external validity. **External validity** simply refers to the degree to which we can generalize the findings to other circumstances or settings, like the real-world environment. When internal validity is high, external validity tends to be low; and when internal validity is low, external validity tends to be high. So laboratory studies are typically low in external validity, while field studies are typically high in external validity. Since field studies are conducted in the real-world environment it is far more appropriate to generalize the findings to that real-world environment than when the research is conducted in the more artificial sterile laboratory.\n\nFinally, there are field studies which are non-experimental in nature because nothing is manipulated. But there are also **field experiments** where an independent variable is manipulated in a natural setting and extraneous variables are controlled. Depending on their overall quality and the level of control of extraneous variables, such field experiments can have high external and high internal validity.","text":"Learning Objectives\n\n1. Define the concept of a variable, distinguish quantitative from categorical variables, and give examples of variables that might be of interest to psychologists.\n2. Explain the difference between a population and a sample.\n3. Distinguish between experimental and non-experimental research.\u00a0\n4. Distinguish between lab studies, field studies, and field experiments.\n\n\u00a0\n\nVariables and Operational Definitions \u00a0\n\nPart of generating a hypothesis involves identifying the variables that you want to study and operationally defining those variables so that they can be measured. Research questions in psychology are about variables. A\u00a0variable\u00a0is a quantity or quality that varies across people or situations. For example, the height of the students enrolled in a university course is a variable because it varies from student to student. The chosen major of the students is also a variable as long as not everyone in the class has declared the same major. Almost everything in our world varies and as such thinking of examples of constants (things that don\u2019t vary) is far more difficult. A rare example of a constant is the speed of light. Variables can be either quantitative or categorical. A\u00a0quantitative\u00a0variable\u00a0is a quantity, such as height, that is typically measured by assigning a number to each individual. Other examples of quantitative variables include people\u2019s level of talkativeness, how depressed they are, and the number of siblings they have. A categorical\u00a0variable\u00a0is a quality, such as chosen major, and is typically measured by assigning a category label to each individual (e.g., Psychology, English, Nursing, etc.). Other examples include people\u2019s nationality, their occupation, and whether they are receiving psychotherapy.\n\nAfter the researcher generates their hypothesis and selects the variables they want to manipulate and measure, the researcher needs to find ways to actually measure the variables of interest. This requires an\u00a0operational\u00a0definition\u2014a definition of the variable in terms of precisely how it is to be measured. Most variables that researchers are interested in studying cannot be directly observed or measured and this poses a problem because empiricism (observation) is at the heart of the scientific method. Operationally defining a variable involves taking an abstract construct like depression that cannot be directly observed and transforming it into something that can be directly observed and measured. Most variables can be operationally defined in many different ways. For example, depression can be operationally defined as people\u2019s scores on a paper-and-pencil depression scale such as the Beck Depression Inventory, the number of depressive symptoms they are experiencing, or whether they have been diagnosed with major depressive disorder. Researchers are wise to choose an operational definition that has been used extensively in the research literature.\n\nIn addition to identifying which variables to manipulate and measure, and operationally defining those variables, researchers need to identify the population of interest. Researchers in psychology are usually interested in drawing conclusions about some very large group of people. This is called the\u00a0population. It could be all American teenagers, children with autism, professional athletes, or even just human beings\u2014depending on the interests and goals of the researcher. But they usually study only a small subset or\u00a0sample\u00a0of the population. For example, a researcher might measure the talkativeness of a few hundred university students with the intention of drawing conclusions about the talkativeness of men and women in general. It is important, therefore, for researchers to use a representative sample\u2014one that is similar to the population in important respects.\u00a0\n\nOne method of obtaining a sample is simple random sampling, in which every member of the population has an equal chance of being selected for the sample. For example, a pollster could start with a list of all the registered voters in a city (the population), randomly select 100 of them from the list (the sample), and ask those 100 whom they intend to vote for. Unfortunately, random sampling is difficult or impossible in most psychological research because the populations are less clearly defined than the registered voters in a city. How could a researcher give all American teenagers or all children with autism an equal chance of being selected for a sample? The most common alternative to random sampling is convenience sampling, in which the sample consists of individuals who happen to be nearby and willing to participate (such as introductory psychology students). Of course, the obvious problem with convenience sampling is that the sample might not be representative of the population and therefore it may be less appropriate to generalize the results from the sample to that population.\n\nThe next step a researcher must take is to decide which type of approach they will use to collect the data. As you will learn in your research methods course there are many different approaches to research that can be divided in many different ways. One of the most fundamental distinctions is between experimental and non-experimental research.\u00a0\n\nResearchers who want to test hypotheses about causal relationships between variables (i.e., their goal is to explain) need to use an experimental method. This is because the experimental method is the only method that allows us to determine causal relationships. Using the experimental approach, researchers first manipulate one or more variables while attempting to control extraneous variables, and then they measure how the manipulated variables affect participants\u2019 responses.\u00a0\n\nThe terms independent variable and dependent variable are used in the context of experimental research. The independent variable is the variable the experimenter manipulates (it is the presumed cause) and the dependent variable is the variable the experimenter measures (it is the presumed effect).\n\nExtraneous variables\u00a0are any variable other than the dependent variable. Confounds are a specific type of extraneous variable that systematically varies along with the variables under investigation and therefore provides an alternative explanation for the results. When researchers design an experiment they need to ensure that they control for confounds; they need to ensure that extraneous variables don\u2019t become confounding variables because in order to make a causal conclusion they need to make sure alternative explanations for the results have been ruled out.\n\nAs an example, if we manipulate the lighting in the room and examine the effects of that manipulation on workers\u2019 productivity,\u00a0then the lighting conditions (bright lights vs. dim lights) would be considered the independent variable and the workers\u2019 productivity would be considered the dependent variable. If the bright lights are noisy then that noise would be a confound since the noise would be present whenever the lights are bright and the noise would be absent when the lights are dim. If noise is varying systematically with light then we wouldn\u2019t know if a difference in worker productivity across the two lighting conditions is due to noise or light. So confounds are bad, they disrupt our ability to make causal conclusions about the nature of the relationship between variables. However, if there is noise in the room both when the lights are on and when the lights are off then noise is merely an extraneous variable (it is a variable other than the independent or dependent variable) and we don\u2019t worry much about extraneous variables. This is because unless a variable varies systematically with the manipulated independent variable it cannot be a competing explanation for the results.\n\nResearchers who are simply interested in describing characteristics of people, describing relationships between variables, and using those relationships to make predictions can use non-experimental research. Using the non-experimental approach, the researcher simply measures variables as they naturally occur, but they do not manipulate them. For instance, if I just measured the number of traffic fatalities in America last year that involved the use of a cell phone but I did not actually manipulate cell phone use then this would be categorized as non-experimental research. Alternatively, if I stood at a busy intersection and recorded drivers\u2019 genders and whether or not they were using a cell phone when they passed through the intersection to see whether men or women are more likely to use a cell phone when driving, then this would be non-experimental research. It is important to point out that non-experimental does not mean nonscientific. Non-experimental research is scientific in nature. It can be used to fulfill two of the three goals of science (to describe and to predict). However, unlike with experimental research, we cannot make causal conclusions using this method; we cannot say that one variable causes another variable using this method.\u00a0\n\nThe next major distinction between research methods is between laboratory and field studies. A laboratory study is a study that is conducted in the laboratory environment. In contrast, a field study is a study that is conducted in the real-world, in a natural environment.\n\nLaboratory experiments typically have high\u00a0internal validity. Internal validity refers to the degree to which we can confidently infer a causal relationship between variables. When we conduct an experimental study in a laboratory environment we have very high internal validity because we manipulate one variable while controlling all other outside extraneous variables. When we manipulate an independent variable and observe an effect on a dependent variable and we control for everything else so that the only difference between our experimental groups or conditions is the one manipulated variable then we can be quite confident that it is the independent variable that is causing the change in the dependent variable. In contrast, because field studies are conducted in the real-world, the experimenter typically has less control over the environment and potential extraneous variables, and this decreases internal validity, making it less appropriate to arrive at causal conclusions.\u00a0\n\nBut there is typically a trade-off between internal and external validity. External validity simply refers to the degree to which we can generalize the findings to other circumstances or settings, like the real-world environment. When internal validity is high, external validity tends to be low; and when internal validity is low, external validity tends to be high. So laboratory studies are typically low in external validity, while field studies are typically high in external validity. Since field studies are conducted in the real-world environment it is far more appropriate to generalize the findings to that real-world environment than when the research is conducted in the more artificial sterile laboratory.\n\nFinally, there are field studies which are non-experimental in nature because nothing is manipulated. But there are also field experiments where an independent variable is manipulated in a natural setting and extraneous variables are controlled. Depending on their overall quality and the level of control of extraneous variables, such field experiments can have high external and high internal validity.","contextuality":{"text":"In psychological research, generating a hypothesis involves identifying and operationally defining variables to enable measurement. Variables, which can be ____________ (like height) or categorical (like chosen major), vary across individuals or situations. Operational definitions transform abstract constructs, such as depression, into measurable entities, often using established _______ in the literature. Researchers must also _________ the population of interest, using representative samples to generalize findings. Sampling methods _______ simple random sampling and convenience sampling, though the latter may not be representative. Research can be experimental, manipulating independent _________ to determine causal relationships, or non-experimental, observing _________ as they naturally occur. Laboratory studies _____ high internal validity by controlling extraneous variables, while field studies _______ higher external validity by reflecting real-world settings. Despite challenges, careful design and control in experiments can enhance both ________ and external validity, allowing for meaningful ___________ about variable relationships.","gaps":[["quantitative",156,12],["methods",383,7],["determine",432,9],["include",540,7],["variables",697,9],["variables",773,9],["offer",827,5],["provide",913,7],["internal",1060,8],["conclusions",1116,11]]},"contextuality_plus":{"text":"In psychological research, generating a hypothesis involves identifying and operationally defining variables to enable measurement. Variables, which can be ____________ (like height) or categorical (like ______ major), vary across individuals or situations. Operational definitions transform ________ constructs, such as depression, into measurable ________, often using established methods in the literature. Researchers must also _________ the population of interest, using representative samples to generalize findings. Sampling methods include simple random sampling and ___________ sampling, though the latter may not be representative. Research can be experimental, manipulating independent variables to _________ causal relationships, or non-experimental, observing variables as they naturally occur. Laboratory _______ offer high internal validity by controlling extraneous variables, while field studies provide higher external validity by reflecting real-world settings. Despite challenges, careful design and control in ___________ can enhance both internal and external validity, allowing for meaningful ___________ about variable relationships.","gaps":[["quantitative",156,12],["chosen",204,6],["abstract",292,8],["entities",349,8],["determine",432,9],["convenience",575,11],["determine",710,9],["studies",819,7],["experiments",1031,11],["conclusions",1116,11]]},"keyword":{"text":"In psychological research, generating a hypothesis involves identifying and operationally defining variables to enable measurement. Variables, which can be quantitative (like height) or categorical (like chosen major), vary across individuals or situations. Operational definitions transform abstract constructs, such as depression, into measurable entities, often using established methods in the literature. Researchers must also determine the population of interest, using representative samples to generalize findings. ________ methods include simple random sampling and convenience sampling, though the latter may not be representative. Research can be experimental, manipulating independent _________ to determine causal relationships, or non-experimental, observing _________ as they naturally occur. Laboratory studies offer high internal ________ by controlling extraneous _________, while field studies provide higher external ________ by reflecting real-world settings. Despite challenges, careful design and control in experiments can enhance both internal and external ________, allowing for meaningful conclusions about variable relationships.","gaps":[["Sampling",523,8],["validity",847,8],["validity",937,8],["validity",1082,8],["variables",697,9],["variables",773,9],["variables",882,9]]}}
{"volume":"research-methods-in-psychology-demo","page":"13-drawing-conclusions-and-reporting-the-results-1","summary":"Scientific theories are continually evaluated and refined based on research findings, as statistics are inherently probabilistic and subject to errors. Confirming a hypothesis can support but not prove a theory, as results may reflect errors, and multiple theories might predict the same outcomes. This is due to the philosophical \"problem of induction,\" where no amount of confirming cases can definitively prove a theory, as future disconfirming evidence may arise. Disconfirming a hypothesis doesn't necessarily disprove a theory, as it may result from errors or unmet assumptions. Researchers must adapt or abandon theories if they consistently fail. Findings are shared through peer-reviewed journals, book chapters, and conference presentations, with the former being the most prestigious.","markdown":"<i-callout variant=\"info\" title=\"Learning Objectives\">\n\n1\\. Identify the conclusions researchers can make based on the outcome of their studies.\n\n2\\. Describe why scientists avoid the term \u201cscientific proof.\u201d\n\n3\\. Explain the different ways that scientists share their findings.\n\n<\/i-callout>\n\nSince statistics are probabilistic in nature and findings can reflect type I or type II errors, we cannot use the results of a single study to conclude with certainty that a theory is true. Rather theories are supported, refuted, or modified based on the results of research. \u00a0\n\nIf the results are statistically significant and consistent with the hypothesis and the theory that was used to generate the hypothesis, then researchers can conclude that the theory is supported. Not only did the theory make an accurate prediction, but there is now a new phenomenon that the theory accounts for. If a hypothesis is disconfirmed in a systematic empirical study, then the theory has been weakened. It made an inaccurate prediction, and there is now a new phenomenon that it does not account for.\n\nAlthough this seems straightforward, there are some complications. First, confirming a hypothesis can strengthen a theory but it can never prove a theory. In fact, scientists tend to avoid the word \u201cprove\u201d when talking and writing about theories. One reason for this avoidance is that the result may reflect a type I error. \u00a0Another reason for this\u00a0avoidance\u00a0is that there may be other plausible theories that imply the same hypothesis, which means that confirming the hypothesis strengthens all those theories equally. A third reason is that it is always possible that another test of the hypothesis or a test of a new hypothesis derived from the theory will be disconfirmed. This\u00a0difficulty\u00a0is a version of the famous philosophical \u201cproblem of induction.\u201d One cannot definitively prove a general principle (e.g., \u201cAll swans are white.\u201d) just by observing confirming cases (e.g., white swans)\u2014no matter how many. It is always possible that a disconfirming case (e.g., a black swan) will eventually come along. For these reasons, scientists tend to think of theories\u2014even highly successful ones\u2014as subject to revision based on new and unexpected observations.\n\nA second complication has to do with what it means when a hypothesis is disconfirmed. According to the strictest version of the hypothetico-deductive method, disconfirming a hypothesis disproves the theory it was derived from. In formal logic, the premises \u201cif\u00a0_A_\u00a0then\u00a0_B_\u201d and \u201cnot\u00a0_B_\u201d necessarily lead to the conclusion \u201cnot\u00a0_A_.\u201d If\u00a0_A_\u00a0is the theory and\u00a0_B_\u00a0is the hypothesis (\u201cif\u00a0_A_\u00a0then\u00a0_B_\u201d), then disconfirming the hypothesis (\u201cnot\u00a0_B_\u201d) must mean that the theory is incorrect (\u201cnot\u00a0_A_\u201d). In practice, however, scientists do not give up on their theories so easily. One reason is that one disconfirmed hypothesis could be a missed opportunity (the result of a type II error) or it could be the result of a faulty research design. Perhaps the researcher did not successfully manipulate the independent variable or measure the dependent variable.\n\nA disconfirmed hypothesis could also mean that some unstated but relatively minor assumption of the theory was not met. For example, if Zajonc had failed to find social facilitation in cockroaches, he could have concluded that drive theory is still correct but it applies only to animals with sufficiently complex nervous systems. That is, the evidence from a study can be used to modify a theory.\u00a0This practice does not mean that researchers are free to ignore disconfirmations of their theories. If they cannot improve their research designs or modify their theories to account for repeated disconfirmations, then they eventually must abandon their theories and replace them with ones that are more successful.\n\nThe bottom line here is that because statistics are probabilistic in nature and because all research studies have flaws there is no such thing as scientific proof, there is only scientific evidence.\n\nThe final step in the research process involves reporting the results. As described in the section on Reviewing the Research Literature in this chapter, results are typically reported in peer-reviewed journal articles and at conferences.\u00a0\n\nThe most prestigious way to report one\u2019s findings is by writing a manuscript and having it published in a peer-reviewed scientific journal. Manuscripts published in psychology journals typically must adhere to the writing style of the American Psychological Association (APA style). You will likely be learning the major elements of this writing style in this course.\n\nAnother way to report findings is by writing a book chapter that is published in an edited book. Preferably the editor of the book puts the chapter through peer review but this is not always the case and some scientists are invited by editors to write book chapters.\n\nA fun way to disseminate findings is to give a presentation at a conference. This can either be done as an oral presentation or a poster presentation. Oral presentations involve getting up in front of an audience of fellow scientists and giving a talk that might last anywhere from 10 minutes to 1 hour (depending on the conference) and then fielding questions from the audience. Alternatively, poster presentations involve summarizing the study on a large poster that provides a brief overview of the purpose, methods, results, and discussion. The presenter stands by their poster for an hour or two and discusses it with people who pass by. Presenting one\u2019s work at a conference is a great way to get feedback from one\u2019s peers before attempting to undergo the more rigorous peer-review process involved in publishing a journal article.","text":"Learning Objectives\n\n1. Identify the conclusions researchers can make based on the outcome of their studies.\n2. Describe why scientists avoid the term \u201cscientific proof.\u201d\n3. Explain the different ways that scientists share their findings.\u00a0\n\nSince statistics are probabilistic in nature and findings can reflect type I or type II errors, we cannot use the results of a single study to conclude with certainty that a theory is true. Rather theories are supported, refuted, or modified based on the results of research. \u00a0\n\nIf the results are statistically significant and consistent with the hypothesis and the theory that was used to generate the hypothesis, then researchers can conclude that the theory is supported. Not only did the theory make an accurate prediction, but there is now a new phenomenon that the theory accounts for. If a hypothesis is disconfirmed in a systematic empirical study, then the theory has been weakened. It made an inaccurate prediction, and there is now a new phenomenon that it does not account for.\n\nAlthough this seems straightforward, there are some complications. First, confirming a hypothesis can strengthen a theory but it can never prove a theory. In fact, scientists tend to avoid the word \u201cprove\u201d when talking and writing about theories. One reason for this avoidance is that the result may reflect a type I error. \u00a0Another reason for this\u00a0avoidance\u00a0is that there may be other plausible theories that imply the same hypothesis, which means that confirming the hypothesis strengthens all those theories equally. A third reason is that it is always possible that another test of the hypothesis or a test of a new hypothesis derived from the theory will be disconfirmed. This\u00a0difficulty\u00a0is a version of the famous philosophical \u201cproblem of induction.\u201d One cannot definitively prove a general principle (e.g., \u201cAll swans are white.\u201d) just by observing confirming cases (e.g., white swans)\u2014no matter how many. It is always possible that a disconfirming case (e.g., a black swan) will eventually come along. For these reasons, scientists tend to think of theories\u2014even highly successful ones\u2014as subject to revision based on new and unexpected observations.\n\nA second complication has to do with what it means when a hypothesis is disconfirmed. According to the strictest version of the hypothetico-deductive method, disconfirming a hypothesis disproves the theory it was derived from. In formal logic, the premises \u201cif\u00a0A\u00a0then\u00a0B\u201d and \u201cnot\u00a0B\u201d necessarily lead to the conclusion \u201cnot\u00a0A.\u201d If\u00a0A\u00a0is the theory and\u00a0B\u00a0is the hypothesis (\u201cif\u00a0A\u00a0then\u00a0B\u201d), then disconfirming the hypothesis (\u201cnot\u00a0B\u201d) must mean that the theory is incorrect (\u201cnot\u00a0A\u201d). In practice, however, scientists do not give up on their theories so easily. One reason is that one disconfirmed hypothesis could be a missed opportunity (the result of a type II error) or it could be the result of a faulty research design. Perhaps the researcher did not successfully manipulate the independent variable or measure the dependent variable.\n\nA disconfirmed hypothesis could also mean that some unstated but relatively minor assumption of the theory was not met. For example, if Zajonc had failed to find social facilitation in cockroaches, he could have concluded that drive theory is still correct but it applies only to animals with sufficiently complex nervous systems. That is, the evidence from a study can be used to modify a theory.\u00a0This practice does not mean that researchers are free to ignore disconfirmations of their theories. If they cannot improve their research designs or modify their theories to account for repeated disconfirmations, then they eventually must abandon their theories and replace them with ones that are more successful.\n\nThe bottom line here is that because statistics are probabilistic in nature and because all research studies have flaws there is no such thing as scientific proof, there is only scientific evidence.\n\nThe final step in the research process involves reporting the results. As described in the section on Reviewing the Research Literature in this chapter, results are typically reported in peer-reviewed journal articles and at conferences.\u00a0\n\nThe most prestigious way to report one\u2019s findings is by writing a manuscript and having it published in a peer-reviewed scientific journal. Manuscripts published in psychology journals typically must adhere to the writing style of the American Psychological Association (APA style). You will likely be learning the major elements of this writing style in this course.\n\nAnother way to report findings is by writing a book chapter that is published in an edited book. Preferably the editor of the book puts the chapter through peer review but this is not always the case and some scientists are invited by editors to write book chapters.\n\nA fun way to disseminate findings is to give a presentation at a conference. This can either be done as an oral presentation or a poster presentation. Oral presentations involve getting up in front of an audience of fellow scientists and giving a talk that might last anywhere from 10 minutes to 1 hour (depending on the conference) and then fielding questions from the audience. Alternatively, poster presentations involve summarizing the study on a large poster that provides a brief overview of the purpose, methods, results, and discussion. The presenter stands by their poster for an hour or two and discusses it with people who pass by. Presenting one\u2019s work at a conference is a great way to get feedback from one\u2019s peers before attempting to undergo the more rigorous peer-review process involved in publishing a journal article.","contextuality":{"text":"Scientific theories are continually evaluated and refined based on research findings, as statistics are inherently probabilistic and subject to errors. Confirming a __________ can support but not prove a theory, as results may reflect errors, and multiple ________ might predict the same outcomes. This is due to the _____________ \"problem of induction,\" where no amount of confirming _____ can definitively prove a theory, as future disconfirming evidence may arise. Disconfirming a __________ doesn't necessarily disprove a theory, as it may ______ from errors or unmet assumptions. Researchers must adapt or _______ theories if they consistently fail. Findings are shared through ____-reviewed journals, book chapters, and conference _____________, with the former being the most ___________.","gaps":[["hypothesis",165,10],["theories",256,8],["philosophical",317,13],["cases",385,5],["hypothesis",484,10],["result",544,6],["abandon",611,7],["peer",683,4],["presentations",737,13],["prestigious",783,11]]},"contextuality_plus":{"text":"Scientific theories are continually evaluated and refined based on research findings, as statistics are inherently probabilistic and subject to errors. __________ a hypothesis can support but not prove a ______, as results may reflect errors, and multiple ________ might predict the same outcomes. This is due to the _____________ \"problem of induction,\" where no amount of __________ cases can definitively prove a theory, as future disconfirming evidence may arise. Disconfirming a __________ doesn't necessarily disprove a theory, as it may ______ from errors or unmet assumptions. Researchers must adapt or _______ theories if they consistently fail. Findings are shared through peer-reviewed journals, book ________, and conference presentations, with the former being the most ___________.","gaps":[["Confirming",152,10],["theory",204,6],["theories",256,8],["philosophical",317,13],["confirming",374,10],["hypothesis",484,10],["result",544,6],["abandon",611,7],["chapters",712,8],["prestigious",783,11]]},"keyword":{"text":"Scientific theories are continually evaluated and refined based on research findings, as statistics are inherently probabilistic and subject to errors. Confirming a hypothesis can support but not prove a ______, as results may reflect errors, and multiple ________ might predict the same outcomes. This is due to the philosophical \"problem of induction,\" where no amount of confirming cases can definitively prove a ______, as future disconfirming evidence may arise. Disconfirming a hypothesis doesn't necessarily disprove a ______, as it may result from ______ or unmet assumptions. Researchers must adapt or abandon ________ if they consistently fail. Findings are shared through peer-reviewed journals, book chapters, and conference presentations, with the former being the most prestigious.","gaps":[["theories",256,8],["theories",619,8],["errors",556,6],["theory",204,6],["theory",416,6],["theory",526,6]]}}
{"volume":"research-methods-in-psychology","page":"4-science-and-common-sense","summary":"The necessity of a scientific approach in psychology is often questioned, with many relying on common sense or intuition\u2014known as folk psychology\u2014for understanding human behavior. However, scientific research frequently contradicts these intuitive beliefs, revealing inaccuracies. For instance, the belief that expressing anger can alleviate it has been debunked, as has the notion that false confessions are rare. Common myths, such as using only 10% of our brain or the effectiveness of calorie-reducing diets, persist due to heuristics and confirmation bias. Psychologists emphasize skepticism and the pursuit of empirical evidence to challenge these misconceptions. Additionally, they embrace uncertainty, welcoming unanswered questions as opportunities for scientific exploration.","markdown":"<i-callout variant=\"info\" title=\"Learning Objectives\">\n\n1\\. Explain the limitations of common sense when it comes to achieving a detailed and accurate understanding of human behavior.\n\n2\\. Give several examples of common sense or folk psychology that are incorrect.\n\n3\\. Define skepticism and its role in scientific psychology.\n\n<\/i-callout>\n\nSome people wonder whether the scientific approach to psychology is necessary. Can we not reach the same conclusions based on common sense or intuition? Certainly we all have intuitive beliefs about people\u2019s behavior, thoughts, and feelings\u2014and these beliefs are collectively referred to as **folk\u00a0psychology**. Although much of our folk psychology is probably reasonably accurate, it is clear that much of it is not. For example, most people believe that anger can be relieved by \u201cletting it out\u201d\u2014perhaps by punching something or screaming loudly. Scientific research, however, has shown that this approach tends to leave people feeling more angry, not less (Bushman, 2002)\\[1\\]. Likewise, most people believe that no one would confess to a crime that they had not committed unless perhaps that person was being physically tortured. But again, extensive empirical research has shown that false confessions are surprisingly common and occur for a variety of reasons (Kassin & Gudjonsson, 2004). \u00a0\n\n<i-callout>\n\n_**Some Great Myths**_\n\nIn _50 Great Myths of Popular Psychology_, psychologist Scott Lilienfeld and colleagues discuss several widely held commonsense beliefs about human behavior that scientific research has shown to be incorrect (Lilienfeld, Lynn, Ruscio, & Beyerstein, 2010)[\\[3\\]](https:\/\/kpu.pressbooks.pub\/psychmethods4e\/chapter\/science-and-common-sense\/#footnote-27-3). Here is a short list:\n\n* \u201cPeople use only 10% of their brain power.\u201d\n* \u201cMost people experience a midlife crisis in their 40\u2019s or 50\u2019s.\u201d\n* \u201cStudents learn best when teaching styles are matched to their learning styles.\u201d\n* \u201cLow self-esteem is a major cause of psychological problems.\u201d\n* \u201cPsychiatric admissions and crimes increase during full moons.\u201d\n\n<\/i-callout>\n\nHow can so many of our intuitive beliefs about human behavior be so wrong? Notice that this is an empirical question, and it just so happens that psychologists have conducted scientific research on it and identified many contributing factors (Gilovich, 1991)\\[4\\]. One is that forming detailed and accurate beliefs requires powers of observation, memory, and analysis to an extent that we do not naturally possess. It would be nearly impossible to count the number of words spoken by the women and men we happen to encounter, estimate the number of words they spoke per day, average these numbers for both groups, and compare them\u2014all in our heads. This is why we tend to rely on mental shortcuts (what psychologists refer to as **heuristics**) in forming and maintaining our beliefs. For example, if a belief is widely shared\u2014especially if it is endorsed by \u201cexperts\u201d\u2014and it makes intuitive sense, we tend to assume it is true. This is compounded by the fact that we then tend to focus on cases that confirm our intuitive beliefs and not on cases that dis-confirm them. This is \u00a0called\u00a0**confirmation\u00a0bias**. For example, once we begin to believe that women are more talkative than men, we tend to notice and remember talkative women and silent men but ignore or forget silent women and talkative men. We also hold incorrect beliefs in part because it would be nice if they\u00a0_were_\u00a0true. For example, many people believe that calorie-reducing diets are an effective long-term treatment for obesity, yet a thorough review of the scientific evidence has shown that they are not (Mann et al., 2007)\\[5\\]. People may continue to believe in the effectiveness of dieting in part because it gives them hope for losing weight if they are obese or makes them feel good about their own \u201cself-control\u201d if they are not.\u00a0\n\nScientists\u2014especially psychologists\u2014understand that they are just as susceptible as anyone else to intuitive but incorrect beliefs. This is why they cultivate an attitude of\u00a0**skepticism**. Being skeptical does not mean being cynical or distrustful, nor does it mean questioning every belief or claim one comes across (which would be impossible anyway). Instead, it means pausing to consider alternatives and to search for evidence\u2014especially systematically collected empirical evidence\u2014when there is enough at stake to justify doing so. For example, imagine that you read a magazine article that claims that giving children a weekly allowance is a good way to help them develop financial responsibility. This is an interesting and potentially important claim (especially if you have children of your own). Taking an attitude of skepticism, however, would mean pausing to ask whether it might be instead that receiving an allowance merely teaches children to spend money\u2014perhaps even to be more materialistic. Taking an attitude of skepticism would also mean asking what evidence supports the original claim. Is the author a scientific researcher? Is any scientific evidence cited? If the issue was important enough, it might also mean turning to the research literature to see if anyone else had studied it.\n\nBecause there is often not enough evidence to fully evaluate a belief or claim, scientists also cultivate\u00a0a **tolerance\u00a0for\u00a0uncertainty**. They accept that there are many things that they simply do not know. For example, it turns out that there is no scientific evidence that receiving an allowance causes children to be more financially responsible, nor is there any scientific evidence that it causes them to be materialistic. Although this kind of uncertainty can be problematic from a practical perspective\u2014for example, making it difficult to decide what to do when our children ask for an allowance\u2014it is exciting from a scientific perspective. If we do not know the answer to an interesting and empirically testable question, science, and perhaps even you as a researcher, may be able to provide the answer.\n\n1.  Bushman, B. J. (2002). Does venting anger feed or extinguish the flame? Catharsis, rumination, distraction, anger, and aggressive responding. _Personality and Social Psychology Bulletin, 28_, 724\u2013731.\n2.  Kassin, S. M., & Gudjonsson, G. H. (2004). The psychology of confession evidence: A review of the literature and issues. _Psychological Science in the Public Interest, 5_, 33\u201367.\n3.  Lilienfeld, S. O., Lynn, S. J., Ruscio, J., & Beyerstein, B. L. (2010). _50 great myths of popular psychology_. Malden, MA: Wiley-Blackwell.\n4.  Gilovich, T. (1991). _How we know what isn\u2019t so: The fallibility of human reason in everyday life_. New York, NY: Free Press.\n5.  Mann, T., Tomiyama, A. J., Westling, E., Lew, A., Samuels, B., & Chatman, J. (2007). Medicare\u2019s search for effective obesity treatments: Diets are not the answer. _American Psychologist, 62_, 220\u2013233.","text":"Learning Objectives\n\n1. Explain the limitations of common sense when it comes to achieving a detailed and accurate understanding of human behavior.\n2. Give several examples of common sense or folk psychology that are incorrect.\n3. Define skepticism and its role in scientific psychology.\u00a0\n\nSome people wonder whether the scientific approach to psychology is necessary. Can we not reach the same conclusions based on common sense or intuition? Certainly we all have intuitive beliefs about people\u2019s behavior, thoughts, and feelings\u2014and these beliefs are collectively referred to as folk\u00a0psychology. Although much of our folk psychology is probably reasonably accurate, it is clear that much of it is not. For example, most people believe that anger can be relieved by \u201cletting it out\u201d\u2014perhaps by punching something or screaming loudly. Scientific research, however, has shown that this approach tends to leave people feeling more angry, not less (Bushman, 2002)[1]. Likewise, most people believe that no one would confess to a crime that they had not committed unless perhaps that person was being physically tortured. But again, extensive empirical research has shown that false confessions are surprisingly common and occur for a variety of reasons (Kassin & Gudjonsson, 2004). \u00a0\n\nSome Great Myths\n\nIn 50 Great Myths of Popular Psychology, psychologist Scott Lilienfeld and colleagues discuss several widely held commonsense beliefs about human behavior that scientific research has shown to be incorrect (Lilienfeld, Lynn, Ruscio, & Beyerstein, 2010)[3]. Here is a short list:\n\n\u201cPeople use only 10% of their brain power.\u201d\u201cMost people experience a midlife crisis in their 40\u2019s or 50\u2019s.\u201d\u201cStudents learn best when teaching styles are matched to their learning styles.\u201d\u201cLow self-esteem is a major cause of psychological problems.\u201d\u201cPsychiatric admissions and crimes increase during full moons.\u201d\n\nHow can so many of our intuitive beliefs about human behavior be so wrong? Notice that this is an empirical question, and it just so happens that psychologists have conducted scientific research on it and identified many contributing factors (Gilovich, 1991)[4]. One is that forming detailed and accurate beliefs requires powers of observation, memory, and analysis to an extent that we do not naturally possess. It would be nearly impossible to count the number of words spoken by the women and men we happen to encounter, estimate the number of words they spoke per day, average these numbers for both groups, and compare them\u2014all in our heads. This is why we tend to rely on mental shortcuts (what psychologists refer to as heuristics) in forming and maintaining our beliefs. For example, if a belief is widely shared\u2014especially if it is endorsed by \u201cexperts\u201d\u2014and it makes intuitive sense, we tend to assume it is true. This is compounded by the fact that we then tend to focus on cases that confirm our intuitive beliefs and not on cases that dis-confirm them. This is \u00a0called\u00a0confirmation\u00a0bias. For example, once we begin to believe that women are more talkative than men, we tend to notice and remember talkative women and silent men but ignore or forget silent women and talkative men. We also hold incorrect beliefs in part because it would be nice if they\u00a0were\u00a0true. For example, many people believe that calorie-reducing diets are an effective long-term treatment for obesity, yet a thorough review of the scientific evidence has shown that they are not (Mann et al., 2007)[5]. People may continue to believe in the effectiveness of dieting in part because it gives them hope for losing weight if they are obese or makes them feel good about their own \u201cself-control\u201d if they are not.\u00a0\n\nScientists\u2014especially psychologists\u2014understand that they are just as susceptible as anyone else to intuitive but incorrect beliefs. This is why they cultivate an attitude of\u00a0skepticism. Being skeptical does not mean being cynical or distrustful, nor does it mean questioning every belief or claim one comes across (which would be impossible anyway). Instead, it means pausing to consider alternatives and to search for evidence\u2014especially systematically collected empirical evidence\u2014when there is enough at stake to justify doing so. For example, imagine that you read a magazine article that claims that giving children a weekly allowance is a good way to help them develop financial responsibility. This is an interesting and potentially important claim (especially if you have children of your own). Taking an attitude of skepticism, however, would mean pausing to ask whether it might be instead that receiving an allowance merely teaches children to spend money\u2014perhaps even to be more materialistic. Taking an attitude of skepticism would also mean asking what evidence supports the original claim. Is the author a scientific researcher? Is any scientific evidence cited? If the issue was important enough, it might also mean turning to the research literature to see if anyone else had studied it.\n\nBecause there is often not enough evidence to fully evaluate a belief or claim, scientists also cultivate\u00a0a tolerance\u00a0for\u00a0uncertainty. They accept that there are many things that they simply do not know. For example, it turns out that there is no scientific evidence that receiving an allowance causes children to be more financially responsible, nor is there any scientific evidence that it causes them to be materialistic. Although this kind of uncertainty can be problematic from a practical perspective\u2014for example, making it difficult to decide what to do when our children ask for an allowance\u2014it is exciting from a scientific perspective. If we do not know the answer to an interesting and empirically testable question, science, and perhaps even you as a researcher, may be able to provide the answer.\n\nBushman, B. J. (2002). Does venting anger feed or extinguish the flame? Catharsis, rumination, distraction, anger, and aggressive responding. Personality and Social Psychology Bulletin, 28, 724\u2013731.Kassin, S. M., & Gudjonsson, G. H. (2004). The psychology of confession evidence: A review of the literature and issues. Psychological Science in the Public Interest, 5, 33\u201367.Lilienfeld, S. O., Lynn, S. J., Ruscio, J., & Beyerstein, B. L. (2010). 50 great myths of popular psychology. Malden, MA: Wiley-Blackwell.Gilovich, T. (1991). How we know what isn\u2019t so: The fallibility of human reason in everyday life. New York, NY: Free Press.Mann, T., Tomiyama, A. J., Westling, E., Lew, A., Samuels, B., & Chatman, J. (2007). Medicare\u2019s search for effective obesity treatments: Diets are not the answer. American Psychologist, 62, 220\u2013233. \u00a0","contextuality":{"text":"The necessity of a scientific approach in psychology is often questioned, with many relying on common sense or intuition\u2014known as folk psychology\u2014for understanding human behavior. However, scientific research frequently contradicts these intuitive _______, revealing inaccuracies. For instance, the ______ that expressing anger can alleviate it has been ________, as has the notion that false ___________ are rare. Common myths, such as using only 10% of our brain or the effectiveness of _______-reducing diets, persist due to heuristics and confirmation bias. Psychologists emphasize __________ and the pursuit of empirical evidence to _________ these misconceptions. Additionally, they embrace uncertainty, welcoming __________ questions as opportunities for scientific exploration.","gaps":[["beliefs",248,7],["belief",299,6],["debunked",354,8],["confessions",393,11],["calorie",489,7],["skepticism",586,10],["challenge",638,9],["unanswered",720,10]]},"contextuality_plus":{"text":"The necessity of a scientific approach in psychology is often questioned, with many relying on common sense or intuition\u2014known as folk psychology\u2014for understanding human behavior. However, scientific research frequently contradicts these _________ beliefs, revealing inaccuracies. For instance, the belief that expressing anger can _________ it has been debunked, as has the notion that false ___________ are rare. Common myths, such as using only 10% of our brain or the effectiveness of _______-reducing diets, persist due to heuristics and confirmation bias. Psychologists emphasize __________ and the pursuit of empirical evidence to _________ these misconceptions. Additionally, they embrace ___________, welcoming unanswered questions as opportunities for scientific ___________.","gaps":[["intuitive",238,9],["alleviate",332,9],["confessions",393,11],["calorie",489,7],["skepticism",586,10],["challenge",638,9],["uncertainty",697,11],["exploration",773,11]]},"keyword":{"text":"The necessity of a scientific approach in psychology is often questioned, with many relying on common sense or intuition\u2014known as folk psychology\u2014for understanding human behavior. However, __________ research frequently contradicts these intuitive beliefs, revealing inaccuracies. For instance, the belief that expressing anger can alleviate it has been debunked, as has the notion that false confessions are rare. Common myths, such as using only 10% of our brain or the effectiveness of calorie-reducing diets, persist due to heuristics and confirmation bias. Psychologists emphasize skepticism and the pursuit of empirical evidence to challenge these misconceptions. Additionally, they embrace uncertainty, welcoming unanswered questions as opportunities for __________ exploration.","gaps":[["scientific",189,10],["scientific",762,10]]}}
{"volume":"research-methods-in-psychology","page":"16-from-moral-principles-to-ethics-codes","summary":"The evolution of ethical standards in research has been significantly shaped by historical events and documents. The Nuremberg Code, established in 1947, emphasized informed consent and risk-benefit analysis, setting a precedent for ethical conduct in research. This was expanded by the Declaration of Helsinki in 1964, which introduced the necessity for independent review of research protocols. In the U.S., the Belmont Report of 1978 highlighted principles such as justice, respect for persons, and beneficence, leading to the Federal Policy for the Protection of Human Subjects and the establishment of Institutional Review Boards (IRBs). Concurrently, the APA\u2019s Ethics Code, particularly Standard 8, addresses key research concerns, including informed consent, deception, and humane treatment of animal subjects, while promoting scholarly integrity. Together, these frameworks ensure ethical practices in human and animal research by mandating oversight, informed consent, and fair treatment, thereby safeguarding participants and maintaining the integrity of scientific inquiry.","markdown":"<i-callout variant=\"info\" title=\"Learning Objectives\">\n\n_1\\._ Describe the history of ethics codes for scientific research with human participants.\n\n2\\. Summarize the American Psychological Association Ethics Code\u2014especially as it relates to informed consent, deception, debriefing, research with nonhuman animals, and scholarly integrity.\n\n<\/i-callout>\n\nThe general moral principles of weighing risks against benefits, acting with integrity, seeking justice, and respecting people\u2019s rights and dignity provide a useful starting point for thinking about the ethics of psychological research because essentially everyone agrees on them. As we have seen, however, even people who agree on these general principles can disagree about specific ethical issues that arise in the course of conducting research. This is why there also exist more detailed and enforceable ethics codes that provide guidance on important issues that arise frequently. In this section, we begin with a brief historical overview of such ethics codes and then look closely at the one that is most relevant to psychological research\u2014that of the American Psychological Association (APA).\n\nOne of the earliest ethics codes was the Nuremberg Code\u2014a set of 10 principles written in 1947 in conjunction with the trials of Nazi physicians accused of shockingly cruel research on concentration camp prisoners during World War II. It provided a standard against which to compare the behavior of the men on trial\u2014many of whom were eventually convicted and either imprisoned or sentenced to death. The Nuremberg Code was particularly clear about the importance of carefully weighing risks against benefits and the need for informed consent. The Declaration of Helsinki is a similar ethics code that was created by the World Medical Council in 1964. Among the standards that it added to the Nuremberg Code was that research with human participants should be based on a written protocol\u2014a detailed description of the research\u2014that is reviewed by an independent committee. The Declaration of Helsinki has been revised several times, most recently in 2004.\n\nIn the United States, concerns about the Tuskegee study and others led to the publication in 1978 of a set of federal guidelines called the Belmont Report. The Belmont Report explicitly recognized the principle of seeking justice, including the importance of conducting research in a way that distributes risks and benefits fairly across different groups at the societal level. It also recognized the importance of respect for persons, which acknowledges individuals\u2019 autonomy and protection for those with diminished autonomy (e.g., prisoners, children), and translates to the need for informed consent. Finally, it recognized the principle of beneficence, which underscores the importance of maximizing the benefits of research while minimizing harms to participants and society. The Belmont Report became the basis of a set of laws\u2014the Federal Policy for the Protection of Human Subjects\u2014that apply to research conducted, supported, or regulated by the federal government. An extremely important part of these regulations is that universities, hospitals, and other institutions that receive support from the federal government must establish an institutional review board (IRB)\u2014a committee that is responsible for reviewing research protocols for potential ethical problems. An IRB must consist of at least five people with varying backgrounds, including members of different professions, scientists and nonscientists, men and women, and at least one person not otherwise affiliated with the institution. The IRB helps to make sure that the risks of the proposed research are minimized, the benefits outweigh the risks, the research is carried out in a fair manner, and the informed consent procedure is adequate.\u00a0\n\nThe federal regulations also distinguish research that poses three levels of risk. Exempt research is the lowest level or risk and includes research on the effectiveness of normal educational activities, the use of standard psychological measures and surveys of a nonsensitive nature that are administered in a way that maintains confidentiality, and research using existing data from public sources. It is called exempt because once approved, it is exempt from regular, continuous review. Expedited research poses a somewhat higher risk than exempt, but still exposes participants to risks that are no greater than minimal risk (those encountered by healthy people in daily life or during routine physical or psychological examinations). Expedited review is done by by one member of the IRB or by a separate committee under the authority of the IRB that can only approve minimal risk research (many departments of psychology have such separate committees). Finally, research that does not qualify for exempt or expedited review is greater than minimal risk research must be reviewed by the full board of IRB members.\n\nThe link that follows the list\u2014from the Office of Human Subjects Research at the National Institutes of Health\u2014allows you to read the ethics codes discussed in this section in their entirety. They are all highly recommended and, with the exception of the Federal Policy, short and easy to read.\n\nThe Nuremberg Code\n\nThe Declaration of Helsinki\n\nThe Belmont Report\n\nFederal Policy for the Protection of Human Subjects\n\nhttps:\/\/www.hhs.gov\/ohrp\/international\/ethical-codes-and-research-standards\/index.html\n\n### APA Ethics Code\u00a0\n\nThe APA\u2019s\u00a0_Ethical Principles of Psychologists and Code of Conduct_\u00a0(also known as the\u00a0**APA\u00a0Ethics\u00a0Code**) was first published in 1953 and has been revised several times since then, most recently in 2010. It includes about 150 specific ethical standards that psychologists and their students are expected to follow. Much of the APA Ethics Code concerns the clinical practice of psychology\u2014advertising one\u2019s services, setting and collecting fees, having personal relationships with clients, and so on. For our purposes, the most relevant part is _Standard 8: Research and Publication_. Although\u00a0_Standard 8_ is reproduced here in its entirety, we will consider some of its most important aspects\u2014informed consent, deception, debriefing, the use of nonhuman animal subjects, and scholarly integrity\u2014in more detail.\n\n\u00a0\n\n<i-callout>\n\n_APA Ethics Code Standard 8: Research and Publication_\n\n8.01 Institutional Approval\n\nWhen institutional approval is required, psychologists provide accurate information about their research proposals and obtain approval prior to conducting the research. They conduct the research in accordance with the approved research protocol.\n\n8.02 Informed Consent to Research\n\n1.  When obtaining informed consent as required in Standard 3.10, Informed Consent, psychologists inform participants about (1) the purpose of the research, expected duration, and procedures; (2) their right to decline to participate and to withdraw from the research once participation has begun; (3) the foreseeable consequences of declining or withdrawing; (4) reasonably foreseeable factors that may be expected to influence their willingness to participate such as potential risks, discomfort, or adverse effects; (5) any prospective research benefits; (6) limits of confidentiality; (7) incentives for participation; and (8) whom to contact for questions about the research and research participants\u2019 rights. They provide opportunity for the prospective participants to ask questions and receive answers. (See also Standards 8.03, Informed Consent for Recording Voices and Images in Research; 8.05, Dispensing With Informed Consent for Research; and 8.07, Deception in Research.)\n2.  Psychologists conducting intervention research involving the use of experimental treatments clarify to participants at the outset of the research (1) the experimental nature of the treatment; (2) the services that will or will not be available to the control group(s) if appropriate; (3) the means by which assignment to treatment and control groups will be made; (4) available treatment alternatives if an individual does not wish to participate in the research or wishes to withdraw once a study has begun; and (5) compensation for or monetary costs of participating including, if appropriate, whether reimbursement from the participant or a third-party payor will be sought. (See also Standard 8.02a, Informed Consent to Research.)\n\n8.03 Informed Consent for Recording Voices and Images in Research\n\nPsychologists obtain informed consent from research participants prior to recording their voices or images for data collection unless (1) the research consists solely of naturalistic observations in public places, and it is not anticipated that the recording will be used in a manner that could cause personal identification or harm, or (2) the research design includes deception, and consent for the use of the recording is obtained during debriefing. (See also Standard 8.07, Deception in Research.)\n\n8.04 Client\/Patient, Student, and Subordinate Research Participants\n\n1.  When psychologists conduct research with clients\/patients, students, or subordinates as participants, psychologists take steps to protect the prospective participants from adverse consequences of declining or withdrawing from participation.\n2.  When research participation is a course requirement or an opportunity for extra credit, the prospective participant is given the choice of equitable alternative activities.\n\n8.05 Dispensing With Informed Consent for Research\n\nPsychologists may dispense with informed consent only (1) where research would not reasonably be assumed to create distress or harm and involves (a) the study of normal educational practices, curricula, or classroom management methods conducted in educational settings; (b) only anonymous questionnaires, naturalistic observations, or archival research for which disclosure of responses would not place participants at risk of criminal or civil liability or damage their financial standing, employability, or reputation, and confidentiality is protected; or (c) the study of factors related to job or organization effectiveness conducted in organizational settings for which there is no risk to participants\u2019 employability, and confidentiality is protected or (2) where otherwise permitted by law or federal or institutional regulations.\n\n8.06 Offering Inducements for Research Participation\n\n1.  Psychologists make reasonable efforts to avoid offering excessive or inappropriate financial or other inducements for research participation when such inducements are likely to coerce participation.\n2.  When offering professional services as an inducement for research participation, psychologists clarify the nature of the services, as well as the risks, obligations, and limitations. (See also Standard 6.05, Barter With Clients\/Patients.)\n\n8.07 Deception in Research\n\n1.  Psychologists do not conduct a study involving deception unless they have determined that the use of deceptive techniques is justified by the study\u2019s significant prospective scientific, educational, or applied value and that effective nondeceptive alternative procedures are not feasible.\n2.  Psychologists do not deceive prospective participants about research that is reasonably expected to cause physical pain or severe emotional distress.\n3.  Psychologists explain any deception that is an integral feature of the design and conduct of an experiment to participants as early as is feasible, preferably at the conclusion of their participation, but no later than at the conclusion of the data collection, and permit participants to withdraw their data. (See also Standard 8.08, Debriefing.)\n\n8.08 Debriefing\n\n1.  Psychologists provide a prompt opportunity for participants to obtain appropriate information about the nature, results, and conclusions of the research, and they take reasonable steps to correct any misconceptions that participants may have of which the psychologists are aware.\n2.  If scientific or humane values justify delaying or withholding this information, psychologists take reasonable measures to reduce the risk of harm.\n3.  When psychologists become aware that research procedures have harmed a participant, they take reasonable steps to minimize the harm.\n\n8.09 Humane Care and Use of Animals in Research\n\n1.  Psychologists acquire, care for, use, and dispose of animals in compliance with current federal, state, and local laws and regulations, and with professional standards.\n2.  Psychologists trained in research methods and experienced in the care of laboratory animals supervise all procedures involving animals and are responsible for ensuring appropriate consideration of their comfort, health, and humane treatment.\n3.  Psychologists ensure that all individuals under their supervision who are using animals have received instruction in research methods and in the care, maintenance, and handling of the species being used, to the extent appropriate to their role. (See also Standard 2.05, Delegation of Work to Others.)\n4.  Psychologists make reasonable efforts to minimize the discomfort, infection, illness, and pain of animal subjects.\n5.  Psychologists use a procedure subjecting animals to pain, stress, or privation only when an alternative procedure is unavailable and the goal is justified by its prospective scientific, educational, or applied value.\n6.  Psychologists perform surgical procedures under appropriate anesthesia and follow techniques to avoid infection and minimize pain during and after surgery.\n7.  When it is appropriate that an animal\u2019s life be terminated, psychologists proceed rapidly, with an effort to minimize pain and in accordance with accepted procedures.\n\n8.10 Reporting Research Results\n\n1.  Psychologists do not fabricate data. (See also Standard 5.01a, Avoidance of False or Deceptive Statements.)\n2.  If psychologists discover significant errors in their published data, they take reasonable steps to correct such errors in a correction, retraction, erratum, or other appropriate publication means.\n\n8.11 Plagiarism\n\nPsychologists do not present portions of another\u2019s work or data as their own, even if the other work or data source is cited occasionally.\n\n8.12 Publication Credit\n\n1.  Psychologists take responsibility and credit, including authorship credit, only for work they have actually performed or to which they have substantially contributed. (See also Standard 8.12b, Publication Credit.)\n2.  Principal authorship and other publication credits accurately reflect the relative scientific or professional contributions of the individuals involved, regardless of their relative status. Mere possession of an institutional position, such as department chair, does not justify authorship credit. Minor contributions to the research or to the writing for publications are acknowledged appropriately, such as in footnotes or in an introductory statement.\n3.  Except under exceptional circumstances, a student is listed as principal author on any multiple-authored article that is substantially based on the student\u2019s doctoral dissertation. Faculty advisors discuss publication credit with students as early as feasible and throughout the research and publication process as appropriate. (See also Standard 8.12b, Publication Credit.)\n\n8.13 Duplicate Publication of Data\n\nPsychologists do not publish, as original data, data that have been previously published. This does not preclude republishing data when they are accompanied by proper acknowledgment.\n\n8.14 Sharing Research Data for Verification\n\n1.  After research results are published, psychologists do not withhold the data on which their conclusions are based from other competent professionals who seek to verify the substantive claims through reanalysis and who intend to use such data only for that purpose, provided that the confidentiality of the participants can be protected and unless legal rights concerning proprietary data preclude their release. This does not preclude psychologists from requiring that such individuals or groups be responsible for costs associated with the provision of such information.\n2.  Psychologists who request data from other psychologists to verify the substantive claims through reanalysis may use shared data only for the declared purpose. Requesting psychologists obtain prior written agreement for all other uses of the data.\n\n8.15 Reviewers\n\nPsychologists who review material submitted for presentation, publication, grant, or research proposal review respect the confidentiality of and the proprietary rights in such information of those who submitted it.\n\nSource: You can read the full APA Ethics Code at http:\/\/www.apa.org\/ethics\/code\/index.aspx.\n\n<\/i-callout>\n\n### Informed Consent\u00a0\n\nStandards 8.02 to 8.05 are about informed consent. Again, informed consent means obtaining and documenting people\u2019s agreement to participate in a study, having informed them of everything that might reasonably be expected to affect their decision. This includes details of the procedure, the risks and benefits of the research, the fact that they have the right to decline to participate or to withdraw from the study, the consequences of doing so, and any legal limits to confidentiality. For example, some states require researchers who learn of child abuse or other crimes to report this information to authorities.\n\nAlthough the process of obtaining informed consent often involves having participants read and sign a\u00a0[**consent\u00a0form**](https:\/\/kpu.pressbooks.pub\/psychmethods4e\/chapter\/from-moral-principles-to-ethics-codes\/#term_49_947), it is important to understand that this is not all it is. Although having participants read and sign a consent form might be enough when they are competent adults with the necessary ability and motivation, many participants do not actually read consent forms or read them but do not understand them. For example, participants often mistake consent forms for legal documents and mistakenly believe that by signing them they give up their right to sue the researcher (Mann, 1994).\\[1\\] Even with competent adults, therefore, it is good practice to tell participants about the risks and benefits, demonstrate the procedure, ask them if they have questions, and remind them of their right to withdraw at any time\u2014in addition to having them read and sign a consent form.\n\nNote also that there are situations in which informed consent is not necessary. These include situations in which the research is not expected to cause any harm and the procedure is straightforward or the study is conducted in the context of people\u2019s ordinary activities. For example, if you wanted to sit outside a public building and observe whether people hold the door open for people behind them, you would not need to obtain their informed consent. Similarly, if a college instructor wanted to compare two legitimate teaching methods across two sections of his research methods course, he would not need to obtain informed consent from his students.\n\n### Deception\n\n[**Deception**](https:\/\/kpu.pressbooks.pub\/psychmethods4e\/chapter\/from-moral-principles-to-ethics-codes\/#term_49_948) of participants in psychological research can take a variety of forms: misinforming participants about the purpose of a study, using confederates, using phony equipment like Milgram\u2019s shock generator, and presenting participants with false feedback about their performance (e.g., telling them they did poorly on a test when they actually did well). Deception also includes not informing participants of the full design or true purpose of the research even if they are not actively misinformed (Sieber, Iannuzzo, & Rodriguez, 1995).\\[2\\] For example, a study on incidental learning\u2014learning without conscious effort\u2014might involve having participants read through a list of words in preparation for a \u201cmemory test\u201d later. Although participants are likely to assume that the memory test will require them to recall the words, it might instead require them to recall the contents of the room or the appearance of the research assistant.\n\nSome researchers have argued that deception of research participants is rarely if ever ethically justified. Among their arguments are that it prevents participants from giving truly informed consent, fails to respect their dignity as human beings, has the potential to upset them, makes them distrustful and therefore less honest in their responding, and damages the reputation of researchers in the field (Baumrind, 1985).\\[3\\]\n\nNote, however, that the APA Ethics Code takes a more moderate approach\u2014allowing deception when the benefits of the study outweigh the risks, participants cannot reasonably be expected to be harmed, the research question cannot be answered without the use of deception, and participants are informed about the deception as soon as possible. This approach acknowledges that not all forms of deception are equally bad. Compare, for example, Milgram\u2019s study in which he deceived his participants in several significant ways that resulted in their experiencing severe psychological stress with an incidental learning study in which a \u201cmemory test\u201d turns out to be slightly different from what participants were expecting. It also acknowledges that some scientifically and socially important research questions can be difficult or impossible to answer without deceiving participants. Knowing that a study concerns the extent to which they obey authority, act aggressively toward a peer, or help a stranger is likely to change the way people behave so that the results no longer generalize to the real world.\n\n### Debriefing\u00a0\n\nStandard 8.08 is about\u00a0[**debriefing**](https:\/\/kpu.pressbooks.pub\/psychmethods4e\/chapter\/from-moral-principles-to-ethics-codes\/#term_49_949). This is the process of informing research participants as soon as possible of the purpose of the study, revealing any deception, and correcting any other misconceptions they might have as a result of participating. Debriefing also involves minimizing harm that might have occurred. For example, an experiment on the effects of being in a sad mood on memory might involve inducing a sad mood in participants by having them think sad thoughts, watch a sad video, and\/or listen to sad music. Debriefing would be the time to return participants\u2019 moods to normal by having them think happy thoughts, watch a happy video, or listen to happy music.\n\n### Nonhuman Animal Subjects\n\nStandard 8.09 is about the humane treatment and care of nonhuman animal subjects. Although most contemporary research in psychology does not involve nonhuman animal subjects, a significant minority of it does\u2014especially in the study of learning and conditioning, behavioral neuroscience, and the development of drug and surgical therapies for psychological disorders.\n\nThe use of nonhuman animal subjects in psychological research is similar to the use of deception in that there are those who argue that it is rarely, if ever, ethically acceptable (Bowd & Shapiro, 1993).\\[4\\] Clearly, nonhuman animals are incapable of giving informed consent. Yet they can be subjected to numerous procedures that are likely to cause them suffering. They can be confined, deprived of food and water, subjected to pain, operated on, and ultimately euthanized. (Of course, they can also be observed benignly in natural or zoo-like settings.) Others point out that psychological research on nonhuman animals has resulted in many important benefits to humans, including the development of behavioral therapies for many disorders, more effective pain control methods, and antipsychotic drugs (Miller, 1985).\\[5\\] It has also resulted in benefits to nonhuman animals, including alternatives to shooting and poisoning as means of controlling them.\n\nAs with deception, the APA acknowledges that the benefits of research on nonhuman animals can outweigh the costs, in which case it is ethically acceptable. However, researchers must use alternative methods when they can. When they cannot, they must acquire and care for their subjects humanely and minimize the harm to them. For more information on the APA\u2019s position on nonhuman animal subjects, see the website of the APA\u2019s Committee on Animal Research and Ethics (http:\/\/www.apa.org\/science\/leadership\/care\/index.aspx).\n\n### Scholarly Integrity\n\nStandards 8.10 to 8.15 are about scholarly integrity. These include the obvious points that researchers must not fabricate data or plagiarize. Plagiarism means using others\u2019 words or ideas without proper acknowledgment. Proper acknowledgment generally means indicating direct quotations with quotation marks\u00a0_and_\u00a0providing a citation to the source of any quotation or idea used. Self-plagiarism is also considered unethical and refers to publishing the same material more than once. In other words, researchers should not borrow prior phrasing from their other published works, just as students should not submit the same work to more than one class.\n\nThe remaining standards make some less obvious but equally important points. Researchers should not publish the same data a second time as though it were new, they should share their data with other researchers, and as peer reviewers, they should keep the unpublished research they review confidential. Note that the authors\u2019 names on published research\u2014and the order in which those names appear\u2014should reflect the importance of each person\u2019s contribution to the research. It would be unethical, for example, to include as an author someone who had made only minor contributions to the research (e.g., analyzing some of the data) or for a faculty member to make himself or herself the first author on research that was largely conducted by a student.\n\n1.  Mann, T. (1994). Informed consent for psychological research: Do subjects comprehend consent forms and understand their legal rights?\u00a0_Psychological Science, 5_, 140\u2013143.\n2.  Sieber, J. E., Iannuzzo, R., & Rodriguez, B. (1995). Deception methods in psychology: Have they changed in 23 years?\u00a0_Ethics & Behavior, 5_, 67\u201385.\n3.  Baumrind, D. (1985). Research using intentional deception: Ethical issues revisited. _American Psychologist, 40_, 165\u2013174.\n4.  Bowd, A. D., & Shapiro, K. J. (1993). The case against animal laboratory research in psychology.\u00a0_Journal of Social Issues, 49_, 133\u2013142.\n5.  Miller, N. E. (1985). The value of behavioral research on animals.\u00a0_American Psychologist, 40_, 423\u2013440.","text":"Learning Objectives\n\n1. Describe the history of ethics codes for scientific research with human participants.\n2. Summarize the American Psychological Association Ethics Code\u2014especially as it relates to informed consent, deception, debriefing, research with nonhuman animals, and scholarly integrity.\u00a0\n\nThe general moral principles of weighing risks against benefits, acting with integrity, seeking justice, and respecting people\u2019s rights and dignity provide a useful starting point for thinking about the ethics of psychological research because essentially everyone agrees on them. As we have seen, however, even people who agree on these general principles can disagree about specific ethical issues that arise in the course of conducting research. This is why there also exist more detailed and enforceable ethics codes that provide guidance on important issues that arise frequently. In this section, we begin with a brief historical overview of such ethics codes and then look closely at the one that is most relevant to psychological research\u2014that of the American Psychological Association (APA).\u00a0\n\nOne of the earliest ethics codes was the Nuremberg Code\u2014a set of 10 principles written in 1947 in conjunction with the trials of Nazi physicians accused of shockingly cruel research on concentration camp prisoners during World War II. It provided a standard against which to compare the behavior of the men on trial\u2014many of whom were eventually convicted and either imprisoned or sentenced to death. The Nuremberg Code was particularly clear about the importance of carefully weighing risks against benefits and the need for informed consent. The Declaration of Helsinki is a similar ethics code that was created by the World Medical Council in 1964. Among the standards that it added to the Nuremberg Code was that research with human participants should be based on a written protocol\u2014a detailed description of the research\u2014that is reviewed by an independent committee. The Declaration of Helsinki has been revised several times, most recently in 2004.\nIn the United States, concerns about the Tuskegee study and others led to the publication in 1978 of a set of federal guidelines called the Belmont Report. The Belmont Report explicitly recognized the principle of seeking justice, including the importance of conducting research in a way that distributes risks and benefits fairly across different groups at the societal level. It also recognized the importance of respect for persons, which acknowledges individuals\u2019 autonomy and protection for those with diminished autonomy (e.g., prisoners, children), and translates to the need for informed consent. Finally, it recognized the principle of beneficence, which underscores the importance of maximizing the benefits of research while minimizing harms to participants and society. The Belmont Report became the basis of a set of laws\u2014the Federal Policy for the Protection of Human Subjects\u2014that apply to research conducted, supported, or regulated by the federal government. An extremely important part of these regulations is that universities, hospitals, and other institutions that receive support from the federal government must establish an institutional review board (IRB)\u2014a committee that is responsible for reviewing research protocols for potential ethical problems. An IRB must consist of at least five people with varying backgrounds, including members of different professions, scientists and nonscientists, men and women, and at least one person not otherwise affiliated with the institution. The IRB helps to make sure that the risks of the proposed research are minimized, the benefits outweigh the risks, the research is carried out in a fair manner, and the informed consent procedure is adequate.\u00a0\nThe federal regulations also distinguish research that poses three levels of risk. Exempt research is the lowest level or risk and includes research on the effectiveness of normal educational activities, the use of standard psychological measures and surveys of a nonsensitive nature that are administered in a way that maintains confidentiality, and research using existing data from public sources. It is called exempt because once approved, it is exempt from regular, continuous review. Expedited research poses a somewhat higher risk than exempt, but still exposes participants to risks that are no greater than minimal risk (those encountered by healthy people in daily life or during routine physical or psychological examinations). Expedited review is done by by one member of the IRB or by a separate committee under the authority of the IRB that can only approve minimal risk research (many departments of psychology have such separate committees). Finally, research that does not qualify for exempt or expedited review is greater than minimal risk research must be reviewed by the full board of IRB members.\n\nThe link that follows the list\u2014from the Office of Human Subjects Research at the National Institutes of Health\u2014allows you to read the ethics codes discussed in this section in their entirety. They are all highly recommended and, with the exception of the Federal Policy, short and easy to read.\nThe Nuremberg Code\nThe Declaration of Helsinki\nThe Belmont Report\nFederal Policy for the Protection of Human Subjects\nhttps:\/\/www.hhs.gov\/ohrp\/international\/ethical-codes-and-research-standards\/index.html\n\nAPA Ethics Code\u00a0\n\nThe APA\u2019s\u00a0Ethical Principles of Psychologists and Code of Conduct\u00a0(also known as the\u00a0APA\u00a0Ethics\u00a0Code) was first published in 1953 and has been revised several times since then, most recently in 2010. It includes about 150 specific ethical standards that psychologists and their students are expected to follow. Much of the APA Ethics Code concerns the clinical practice of psychology\u2014advertising one\u2019s services, setting and collecting fees, having personal relationships with clients, and so on. For our purposes, the most relevant part is Standard 8: Research and Publication. Although\u00a0Standard 8 is reproduced here in its entirety, we will consider some of its most important aspects\u2014informed consent, deception, debriefing, the use of nonhuman animal subjects, and scholarly integrity\u2014in more detail.\n\n\n\u00a0\n\nAPA Ethics Code Standard 8: Research and Publication\n\n8.01 Institutional Approval\n\nWhen institutional approval is required, psychologists provide accurate information about their research proposals and obtain approval prior to conducting the research. They conduct the research in accordance with the approved research protocol.\n\n8.02 Informed Consent to Research\n\nWhen obtaining informed consent as required in Standard 3.10, Informed Consent, psychologists inform participants about (1) the purpose of the research, expected duration, and procedures; (2) their right to decline to participate and to withdraw from the research once participation has begun; (3) the foreseeable consequences of declining or withdrawing; (4) reasonably foreseeable factors that may be expected to influence their willingness to participate such as potential risks, discomfort, or adverse effects; (5) any prospective research benefits; (6) limits of confidentiality; (7) incentives for participation; and (8) whom to contact for questions about the research and research participants\u2019 rights. They provide opportunity for the prospective participants to ask questions and receive answers. (See also Standards 8.03, Informed Consent for Recording Voices and Images in Research; 8.05, Dispensing With Informed Consent for Research; and 8.07, Deception in Research.)Psychologists conducting intervention research involving the use of experimental treatments clarify to participants at the outset of the research (1) the experimental nature of the treatment; (2) the services that will or will not be available to the control group(s) if appropriate; (3) the means by which assignment to treatment and control groups will be made; (4) available treatment alternatives if an individual does not wish to participate in the research or wishes to withdraw once a study has begun; and (5) compensation for or monetary costs of participating including, if appropriate, whether reimbursement from the participant or a third-party payor will be sought. (See also Standard 8.02a, Informed Consent to Research.)\n\n8.03 Informed Consent for Recording Voices and Images in Research\n\nPsychologists obtain informed consent from research participants prior to recording their voices or images for data collection unless (1) the research consists solely of naturalistic observations in public places, and it is not anticipated that the recording will be used in a manner that could cause personal identification or harm, or (2) the research design includes deception, and consent for the use of the recording is obtained during debriefing. (See also Standard 8.07, Deception in Research.)\n\n8.04 Client\/Patient, Student, and Subordinate Research Participants\n\nWhen psychologists conduct research with clients\/patients, students, or subordinates as participants, psychologists take steps to protect the prospective participants from adverse consequences of declining or withdrawing from participation.When research participation is a course requirement or an opportunity for extra credit, the prospective participant is given the choice of equitable alternative activities.\n\n8.05 Dispensing With Informed Consent for Research\n\nPsychologists may dispense with informed consent only (1) where research would not reasonably be assumed to create distress or harm and involves (a) the study of normal educational practices, curricula, or classroom management methods conducted in educational settings; (b) only anonymous questionnaires, naturalistic observations, or archival research for which disclosure of responses would not place participants at risk of criminal or civil liability or damage their financial standing, employability, or reputation, and confidentiality is protected; or (c) the study of factors related to job or organization effectiveness conducted in organizational settings for which there is no risk to participants\u2019 employability, and confidentiality is protected or (2) where otherwise permitted by law or federal or institutional regulations.\n\n8.06 Offering Inducements for Research Participation\n\nPsychologists make reasonable efforts to avoid offering excessive or inappropriate financial or other inducements for research participation when such inducements are likely to coerce participation.When offering professional services as an inducement for research participation, psychologists clarify the nature of the services, as well as the risks, obligations, and limitations. (See also Standard 6.05, Barter With Clients\/Patients.)\n\n8.07 Deception in Research\n\nPsychologists do not conduct a study involving deception unless they have determined that the use of deceptive techniques is justified by the study\u2019s significant prospective scientific, educational, or applied value and that effective nondeceptive alternative procedures are not feasible.Psychologists do not deceive prospective participants about research that is reasonably expected to cause physical pain or severe emotional distress.Psychologists explain any deception that is an integral feature of the design and conduct of an experiment to participants as early as is feasible, preferably at the conclusion of their participation, but no later than at the conclusion of the data collection, and permit participants to withdraw their data. (See also Standard 8.08, Debriefing.)\n\n8.08 Debriefing\n\nPsychologists provide a prompt opportunity for participants to obtain appropriate information about the nature, results, and conclusions of the research, and they take reasonable steps to correct any misconceptions that participants may have of which the psychologists are aware.If scientific or humane values justify delaying or withholding this information, psychologists take reasonable measures to reduce the risk of harm.When psychologists become aware that research procedures have harmed a participant, they take reasonable steps to minimize the harm.\n\n8.09 Humane Care and Use of Animals in Research\n\nPsychologists acquire, care for, use, and dispose of animals in compliance with current federal, state, and local laws and regulations, and with professional standards.Psychologists trained in research methods and experienced in the care of laboratory animals supervise all procedures involving animals and are responsible for ensuring appropriate consideration of their comfort, health, and humane treatment.Psychologists ensure that all individuals under their supervision who are using animals have received instruction in research methods and in the care, maintenance, and handling of the species being used, to the extent appropriate to their role. (See also Standard 2.05, Delegation of Work to Others.)Psychologists make reasonable efforts to minimize the discomfort, infection, illness, and pain of animal subjects.Psychologists use a procedure subjecting animals to pain, stress, or privation only when an alternative procedure is unavailable and the goal is justified by its prospective scientific, educational, or applied value.Psychologists perform surgical procedures under appropriate anesthesia and follow techniques to avoid infection and minimize pain during and after surgery.When it is appropriate that an animal\u2019s life be terminated, psychologists proceed rapidly, with an effort to minimize pain and in accordance with accepted procedures.\n\n8.10 Reporting Research Results\n\nPsychologists do not fabricate data. (See also Standard 5.01a, Avoidance of False or Deceptive Statements.)If psychologists discover significant errors in their published data, they take reasonable steps to correct such errors in a correction, retraction, erratum, or other appropriate publication means.\n\n8.11 Plagiarism\n\nPsychologists do not present portions of another\u2019s work or data as their own, even if the other work or data source is cited occasionally.\n\n8.12 Publication Credit\n\nPsychologists take responsibility and credit, including authorship credit, only for work they have actually performed or to which they have substantially contributed. (See also Standard 8.12b, Publication Credit.)Principal authorship and other publication credits accurately reflect the relative scientific or professional contributions of the individuals involved, regardless of their relative status. Mere possession of an institutional position, such as department chair, does not justify authorship credit. Minor contributions to the research or to the writing for publications are acknowledged appropriately, such as in footnotes or in an introductory statement.Except under exceptional circumstances, a student is listed as principal author on any multiple-authored article that is substantially based on the student\u2019s doctoral dissertation. Faculty advisors discuss publication credit with students as early as feasible and throughout the research and publication process as appropriate. (See also Standard 8.12b, Publication Credit.)\n\n8.13 Duplicate Publication of Data\n\nPsychologists do not publish, as original data, data that have been previously published. This does not preclude republishing data when they are accompanied by proper acknowledgment.\n\n8.14 Sharing Research Data for Verification\n\nAfter research results are published, psychologists do not withhold the data on which their conclusions are based from other competent professionals who seek to verify the substantive claims through reanalysis and who intend to use such data only for that purpose, provided that the confidentiality of the participants can be protected and unless legal rights concerning proprietary data preclude their release. This does not preclude psychologists from requiring that such individuals or groups be responsible for costs associated with the provision of such information.Psychologists who request data from other psychologists to verify the substantive claims through reanalysis may use shared data only for the declared purpose. Requesting psychologists obtain prior written agreement for all other uses of the data.\n\n8.15 Reviewers\n\nPsychologists who review material submitted for presentation, publication, grant, or research proposal review respect the confidentiality of and the proprietary rights in such information of those who submitted it.\n\nSource: You can read the full APA Ethics Code at http:\/\/www.apa.org\/ethics\/code\/index.aspx.\n\nInformed Consent\u00a0\n\nStandards 8.02 to 8.05 are about informed consent. Again, informed consent means obtaining and documenting people\u2019s agreement to participate in a study, having informed them of everything that might reasonably be expected to affect their decision. This includes details of the procedure, the risks and benefits of the research, the fact that they have the right to decline to participate or to withdraw from the study, the consequences of doing so, and any legal limits to confidentiality. For example, some states require researchers who learn of child abuse or other crimes to report this information to authorities.\n\nAlthough the process of obtaining informed consent often involves having participants read and sign a\u00a0consent\u00a0form, it is important to understand that this is not all it is. Although having participants read and sign a consent form might be enough when they are competent adults with the necessary ability and motivation, many participants do not actually read consent forms or read them but do not understand them. For example, participants often mistake consent forms for legal documents and mistakenly believe that by signing them they give up their right to sue the researcher (Mann, 1994).[1] Even with competent adults, therefore, it is good practice to tell participants about the risks and benefits, demonstrate the procedure, ask them if they have questions, and remind them of their right to withdraw at any time\u2014in addition to having them read and sign a consent form.\n\nNote also that there are situations in which informed consent is not necessary. These include situations in which the research is not expected to cause any harm and the procedure is straightforward or the study is conducted in the context of people\u2019s ordinary activities. For example, if you wanted to sit outside a public building and observe whether people hold the door open for people behind them, you would not need to obtain their informed consent. Similarly, if a college instructor wanted to compare two legitimate teaching methods across two sections of his research methods course, he would not need to obtain informed consent from his students.\n\nDeception\n\nDeception of participants in psychological research can take a variety of forms: misinforming participants about the purpose of a study, using confederates, using phony equipment like Milgram\u2019s shock generator, and presenting participants with false feedback about their performance (e.g., telling them they did poorly on a test when they actually did well). Deception also includes not informing participants of the full design or true purpose of the research even if they are not actively misinformed (Sieber, Iannuzzo, & Rodriguez, 1995).[2] For example, a study on incidental learning\u2014learning without conscious effort\u2014might involve having participants read through a list of words in preparation for a \u201cmemory test\u201d later. Although participants are likely to assume that the memory test will require them to recall the words, it might instead require them to recall the contents of the room or the appearance of the research assistant.\n\nSome researchers have argued that deception of research participants is rarely if ever ethically justified. Among their arguments are that it prevents participants from giving truly informed consent, fails to respect their dignity as human beings, has the potential to upset them, makes them distrustful and therefore less honest in their responding, and damages the reputation of researchers in the field (Baumrind, 1985).[3]\n\nNote, however, that the APA Ethics Code takes a more moderate approach\u2014allowing deception when the benefits of the study outweigh the risks, participants cannot reasonably be expected to be harmed, the research question cannot be answered without the use of deception, and participants are informed about the deception as soon as possible. This approach acknowledges that not all forms of deception are equally bad. Compare, for example, Milgram\u2019s study in which he deceived his participants in several significant ways that resulted in their experiencing severe psychological stress with an incidental learning study in which a \u201cmemory test\u201d turns out to be slightly different from what participants were expecting. It also acknowledges that some scientifically and socially important research questions can be difficult or impossible to answer without deceiving participants. Knowing that a study concerns the extent to which they obey authority, act aggressively toward a peer, or help a stranger is likely to change the way people behave so that the results no longer generalize to the real world.\n\nDebriefing\u00a0\n\nStandard 8.08 is about\u00a0debriefing. This is the process of informing research participants as soon as possible of the purpose of the study, revealing any deception, and correcting any other misconceptions they might have as a result of participating. Debriefing also involves minimizing harm that might have occurred. For example, an experiment on the effects of being in a sad mood on memory might involve inducing a sad mood in participants by having them think sad thoughts, watch a sad video, and\/or listen to sad music. Debriefing would be the time to return participants\u2019 moods to normal by having them think happy thoughts, watch a happy video, or listen to happy music.\n\nNonhuman Animal Subjects\n\nStandard 8.09 is about the humane treatment and care of nonhuman animal subjects. Although most contemporary research in psychology does not involve nonhuman animal subjects, a significant minority of it does\u2014especially in the study of learning and conditioning, behavioral neuroscience, and the development of drug and surgical therapies for psychological disorders.\n\nThe use of nonhuman animal subjects in psychological research is similar to the use of deception in that there are those who argue that it is rarely, if ever, ethically acceptable (Bowd & Shapiro, 1993).[4] Clearly, nonhuman animals are incapable of giving informed consent. Yet they can be subjected to numerous procedures that are likely to cause them suffering. They can be confined, deprived of food and water, subjected to pain, operated on, and ultimately euthanized. (Of course, they can also be observed benignly in natural or zoo-like settings.) Others point out that psychological research on nonhuman animals has resulted in many important benefits to humans, including the development of behavioral therapies for many disorders, more effective pain control methods, and antipsychotic drugs (Miller, 1985).[5] It has also resulted in benefits to nonhuman animals, including alternatives to shooting and poisoning as means of controlling them.\n\nAs with deception, the APA acknowledges that the benefits of research on nonhuman animals can outweigh the costs, in which case it is ethically acceptable. However, researchers must use alternative methods when they can. When they cannot, they must acquire and care for their subjects humanely and minimize the harm to them. For more information on the APA\u2019s position on nonhuman animal subjects, see the website of the APA\u2019s Committee on Animal Research and Ethics (http:\/\/www.apa.org\/science\/leadership\/care\/index.aspx).\n\nScholarly Integrity\n\nStandards 8.10 to 8.15 are about scholarly integrity. These include the obvious points that researchers must not fabricate data or plagiarize. Plagiarism means using others\u2019 words or ideas without proper acknowledgment. Proper acknowledgment generally means indicating direct quotations with quotation marks\u00a0and\u00a0providing a citation to the source of any quotation or idea used. Self-plagiarism is also considered unethical and refers to publishing the same material more than once. In other words, researchers should not borrow prior phrasing from their other published works, just as students should not submit the same work to more than one class.\n\nThe remaining standards make some less obvious but equally important points. Researchers should not publish the same data a second time as though it were new, they should share their data with other researchers, and as peer reviewers, they should keep the unpublished research they review confidential. Note that the authors\u2019 names on published research\u2014and the order in which those names appear\u2014should reflect the importance of each person\u2019s contribution to the research. It would be unethical, for example, to include as an author someone who had made only minor contributions to the research (e.g., analyzing some of the data) or for a faculty member to make himself or herself the first author on research that was largely conducted by a student.\n\n\u00a0\n\nMann, T. (1994). Informed consent for psychological research: Do subjects comprehend consent forms and understand their legal rights?\u00a0Psychological Science, 5, 140\u2013143.Sieber, J. E., Iannuzzo, R., & Rodriguez, B. (1995). Deception methods in psychology: Have they changed in 23 years?\u00a0Ethics & Behavior, 5, 67\u201385.Baumrind, D. (1985). Research using intentional deception: Ethical issues revisited. American Psychologist, 40, 165\u2013174.Bowd, A. D., & Shapiro, K. J. (1993). The case against animal laboratory research in psychology.\u00a0Journal of Social Issues, 49, 133\u2013142.Miller, N. E. (1985). The value of behavioral research on animals.\u00a0American Psychologist, 40, 423\u2013440.\u00a0","contextuality":{"text":"The evolution of ethical standards in research has been significantly shaped by historical events and documents. The Nuremberg Code, ___________ in 1947, emphasized informed consent and risk-benefit analysis, setting a precedent for ethical _______ in research. This was expanded by the Declaration of Helsinki in 1964, which introduced the _________ for independent review of research protocols. In the U.S., the Belmont Report of 1978 highlighted __________ such as justice, respect for persons, and beneficence, leading to the Federal Policy for the Protection of Human Subjects and the _____________ of Institutional Review Boards (IRBs). Concurrently, the APA\u2019s Ethics Code, particularly Standard 8, addresses key research ________, including informed consent, deception, and humane treatment of animal subjects, while _________ scholarly integrity. Together, these frameworks ensure ethical practices in human and animal research by _________ oversight, informed consent, and fair _________, thereby safeguarding participants and maintaining the _________ of scientific inquiry.","gaps":[["established",133,11],["conduct",241,7],["necessity",341,9],["principles",449,10],["establishment",590,13],["concerns",728,8],["promoting",824,9],["mandating",939,9],["treatment",987,9],["integrity",1052,9]]},"contextuality_plus":{"text":"The evolution of ethical standards in research has been significantly shaped by historical events and documents. The Nuremberg Code, established in 1947, __________ informed consent and risk-benefit analysis, setting a precedent for ethical conduct in research. This was ________ by the Declaration of Helsinki in 1964, which introduced the necessity for independent review of research _________. In the U.S., the Belmont Report of 1978 highlighted principles such as justice, respect for _______, and beneficence, leading to the Federal Policy for the Protection of Human Subjects and the _____________ of Institutional Review Boards (IRBs). Concurrently, the APA\u2019s Ethics Code, particularly Standard 8, addresses key research ________, including informed consent, deception, and humane treatment of animal subjects, while promoting _________ integrity. Together, these frameworks ensure ethical _________ in human and animal research by mandating _________, informed consent, and fair treatment, thereby safeguarding participants and maintaining the integrity of scientific _______.","gaps":[["emphasized",154,10],["expanded",271,8],["protocols",386,9],["persons",489,7],["establishment",590,13],["concerns",728,8],["scholarly",834,9],["practices",897,9],["oversight",949,9],["inquiry",1076,7]]},"keyword":{"text":"The evolution of ethical standards in research has been significantly shaped by historical events and documents. The Nuremberg ____, established in 1947, emphasized ________ consent and risk-benefit analysis, setting a precedent for _______ conduct in research. This was expanded by the Declaration of Helsinki in 1964, which introduced the necessity for independent ______ of research protocols. In the U.S., the Belmont Report of 1978 highlighted principles such as justice, respect for persons, and beneficence, leading to the Federal Policy for the Protection of Human Subjects and the establishment of Institutional Review Boards (IRBs). Concurrently, the APA\u2019s Ethics ____, particularly Standard 8, addresses key research concerns, including ________ consent, deception, and humane treatment of animal subjects, while promoting scholarly integrity. Together, these frameworks ensure _______ practices in human and animal ________ by mandating oversight, ________ consent, and fair treatment, thereby safeguarding participants and maintaining the integrity of scientific inquiry.","gaps":[["review",367,6],["informed",165,8],["informed",748,8],["informed",960,8],["Code",127,4],["Code",674,4],["ethical",233,7],["ethical",889,7],["research",927,8]]}}
{"volume":"communication-for-business-success","page":"18-4-divergent-cultural-characteristics","summary":"The text explores the inherent inequalities and diverse cultural perspectives that shape human experiences and interactions. It contrasts individualistic cultures, such as the United States, where personal freedom and independence are emphasized, with collectivist cultures, which prioritize community and group needs. The work of Geert Hofstede is highlighted, focusing on how different cultures perceive individualism, collectivism, and other dimensions like time orientation and masculinity-femininity. Uncertainty reduction theory by Charles Berger and Richard Calabrese is discussed, demonstrating how communication evolves as uncertainty decreases. The text also examines the cultural significance of materialism versus relational values, showing how these differences influence personal and business interactions across the globe. Understanding these cultural nuances is essential for effective intercultural communication and navigating global business landscapes.","markdown":"<i-callout variant=\"info\" title=\"Learning Objectives\">\n\n1\\. Discuss divergent cultural characteristics and list several examples of such characteristics in the culture(s) you identify with.\n\n<\/i-callout>\n\nWe are not created equal. We are born light- or dark-skinned, to parents of education or parents without access to education, and we grow up short or tall, slender or stocky. Our life chances or options are in many ways determined by our birth. The Victorian \u201crags to riches\u201d novels that Horatio Alger wrote promoted the ideal that individuals can overcome all obstacles, raising themselves up by their bootstraps. Some people do have amazing stories, but even if you are quick to point out that Microsoft founder Bill Gates became fabulously successful despite his lack of a college education, know that his example is exception, not the rule. We all may use the advantages of our circumstances to improve our lives, but the type and extent of those advantages vary greatly across the planet.\n\nCultures reflect this inequality, this diversity, and the divergent range of values, symbols, and meanings across communities. Can you tie a knot? Perhaps you can tie your shoes, but can you tie a knot to secure a line to a boat, to secure a heavy load on a cart or truck, or to bundle a bale of hay? You may not be able to, but if you were raised in a culture that place a high value on knot-tying for specific purposes, you would learn that which your community values. We all have viewpoints, but they are shaped by our interactions with our communities. Let\u2019s examine several points of divergence across cultures.\n\nPeople in individualistic cultures value individual freedom and personal independence, and cultures always have stories to reflect their values. You may recall the story of Superman, or John McLean in the Diehard series, and note how one person overcomes all obstacles. Through personal ingenuity, in spite of challenges, one person rises successfully to conquer or vanquish those obstacles. Sometimes there is an assist, as in basketball or football, where another person lends a hand, but still the story repeats itself again and again, reflecting the cultural viewpoint.\n\nThe Dutch researcher Geert Hofstede explored the concepts of individualism and collectivism across diverse cultures (Hofstede, G., 1982; Hofstede, G., 2001; Hofstede, G., 2005). He found that in individualistic cultures like the United States, people perceived their world primarily from their own viewpoint. They perceived themselves as empowered individuals, capable of making their own decisions, and able to make an impact on their own lives.\n\nCultural viewpoint is not an either\/or dichotomy, but rather a continuum or range. You may belong to some communities that express individualistic cultural values, while others place the focus on a collective viewpoint. Collectivist cultures (Hofstede, G., 1982), including many in Asia and South America, focus on the needs of the nation, community, family, or group of workers.\u00a0\n\nOwnership and private property is one way to examine this difference. In some cultures, property is almost exclusively private, while others tend toward community ownership. The collectively owned resource returns benefits to the community. Water, for example, has long been viewed as a community resource, much like air, but that has been changing as business and organizations have purchased water rights and gained control over resources.\u00a0\n\nPublic lands, such as parks, are often considered public, and individual exploitation of them is restricted. Copper, a metal with a variety of industrial applications, is collectively owned in Chile, with profits deposited in the general government fund. While public and private initiatives exist, the cultural viewpoint is our topic. How does someone raised in a culture that emphasizes the community interact with someone raised in a primarily individualistic culture? How could tensions be expressed and how might interactions be influenced by this point of divergence?\n\nDo you know the rules of your business or organization? Did you learn them from an employee manual or by observing the conduct of others? Your response may include both options, but not all cultures communicate rules in the same way. Carley Dodd discusses this difference and has found quite a range of difference. In an explicit-rule culture, where rules are clearly communicated so that everyone is aware of them, the guidelines and agenda for a meeting are announced prior to the gathering. In an implicit-rule culture, where rules are often understood and communicated nonverbally, there may be no agenda. Everyone knows why they are gathered and what role each member plays, even though the expectations may not be clearly stated. Power, status, and behavioral expectations may all be understood, and to the person from outside this culture, it may prove a challenge to understand the rules of the context.\n\nOutsiders often communicate their \u201cotherness\u201d by not knowing where to stand, when to sit, or how to initiate a conversation if the rules are not clearly stated. While it may help to know that implicit-rule cultures are often more tolerant of deviation from the understood rules, the newcomer will be wise to learn by observing quietly\u2014and to do as much research ahead of the event as possible.\n\nWhen we meet each other for the first time, we often use what we have previously learned to understand our current context. We also do this to reduce our uncertainty. Some cultures, such as the United States and Britain, are highly tolerant of uncertainty, while others go to great lengths to reduce the element of surprise. Cultures in the Arab world, for example, are high in uncertainty avoidance; they tend to be resistant to change and reluctant to take risks. Whereas a U.S. business negotiator might enthusiastically agree to try a new procedure, the Egyptian counterpart would likely refuse to get involved until all the details are worked out.\n\nCharles Berger and Richard Calabrese developed uncertainty reduction theory to examine this dynamic aspect of communication. Here are seven axioms of uncertainty:\n\n1.  There is a high level of uncertainty at first. As we get to know one another, our verbal communication increases and our uncertainty begins to decrease.\n2.  Following verbal communication, nonverbal communication increases, uncertainty continues to decrease, and more nonverbal displays of affiliation, like nodding one\u2019s head to indicate agreement, will start to be expressed.\n3.  When experiencing high levels of uncertainty, we tend to increase our information-seeking behavior, perhaps asking questions to gain more insight. As our understanding increases, uncertainty decreases, as does the information-seeking behavior.\n4.  When experiencing high levels of uncertainty, the communication interaction is not as personal or intimate. As uncertainty is reduced, intimacy increases.\n5.  When experiencing high levels of uncertainty, communication will feature more reciprocity, or displays of respect. As uncertainty decreases, reciprocity may diminish.\n6.  Differences between people increase uncertainty, while similarities decrease it.\n7.  Higher levels of uncertainty are associated with a decrease in the indication of liking the other person, while reductions in uncertainty are associated with liking the other person more.\n\nEdward T. Hall and Mildred Reed Hall state that monochronic time-oriented cultures consider one thing at a time, whereas polychronic time-oriented cultures schedule many things at one time, and time is considered in a more fluid sense. In monochromatic time, interruptions are to be avoided, and everything has its own specific time. Even the multitasker from a monochromatic culture will, for example, recognize the value of work first before play or personal time. The United States, Germany, and Switzerland are often noted as countries that value a monochromatic time orientation.\n\nPolychromatic time looks a little more complicated, with business and family mixing with dinner and dancing. Greece, Italy, Chile, and Saudi Arabia are countries where one can observe this perception of time; business meetings may be scheduled at a fixed time, but when they actually begin may be another story. Also note that the dinner invitation for 8 p.m. may in reality be more like 9 p.m. If you were to show up on time, you might be the first person to arrive and find that the hosts are not quite ready to receive you.\n\nWhen in doubt, always ask before the event; many people from polychromatic cultures will be used to foreigner\u2019s tendency to be punctual, even compulsive, about respecting established times for events. The skilled business communicator is aware of this difference and takes steps to anticipate it. The value of time in different cultures is expressed in many ways, and your understanding can help you communicate more effectively.\n\nDo you want your reward right now or can you dedicate yourself to a long-term goal? You may work in a culture whose people value immediate results and grow impatient when those results do not materialize. Geert Hofstede discusses this relationship of time orientation to a culture as a \u201ctime horizon,\u201d and it underscores the perspective of the individual within a cultural context. Many countries in Asia, influenced by the teachings of Confucius, value a long-term orientation, whereas other countries, including the United States, have a more short-term approach to life and results. Native American cultures are known for holding a long-term orientation, as illustrated by the proverb attributed to the Iroquois that decisions require contemplation of their impact seven generations removed.\n\nIf you work within a culture that has a short-term orientation, you may need to place greater emphasis on reciprocation of greetings, gifts, and rewards. For example, if you send a thank-you note the morning after being treated to a business dinner, your host will appreciate your promptness. While there may be a respect for tradition, there is also an emphasis on personal representation and honor, a reflection of identity and integrity. Personal stability and consistency are also valued in a short-term oriented culture, contributing to an overall sense of predictability and familiarity.\n\nLong-term orientation is often marked by persistence, thrift and frugality, and an order to relationships based on age and status. A sense of shame for the family and community is also observed across generations. What an individual does reflects on the family and is carried by immediate and extended family members.\n\nThere was a time when many cultures and religions valued a female figurehead, and with the rise of Western cultures we have observed a shift toward a masculine ideal. Each carries with it a set of cultural expectations and norms for gender behavior and gender roles across life, including business.\n\nHofstede describes the masculine-feminine dichotomy not in terms of whether men or women hold the power in a given culture, but rather the extent to which that culture values certain traits that may be considered masculine or feminine. Thus, \u201cthe assertive pole has been called \u2018masculine\u2019 and the modest, caring pole \u2018feminine.\u2019 The women in feminine countries have the same modest, caring values as the men; in the masculine countries they are somewhat assertive and competitive, but not as much as the men, so that these countries show a gap between men\u2019s values and women\u2019s values\u201d (Hofstede, G., 2009).\n\nWe can observe this difference in where people gather, how they interact, and how they dress. We can see it during business negotiations, where it may make an important difference in the success of the organizations involved. Cultural expectations precede the interaction, so someone who doesn\u2019t match those expectations may experience tension. Business in the United States has a masculine orientation\u2014assertiveness and competition are highly valued. In other cultures, such as Sweden, business values are more attuned to modesty (lack of self-promotion) and taking care of society\u2019s weaker members. This range of difference is one aspect of intercultural communication that requires significant attention when the business communicator enters a new environment.\n\nIn the United States, business correspondence is expected to be short and to the point. \u201cWhat can I do for you?\u201d is a common question when a business person receives a call from a stranger; it is an accepted way of asking the caller to state his or her business. In some cultures it is quite appropriate to make direct personal observation, such as \u201cYou\u2019ve changed your hairstyle,\u201d while for others it may be observed, but never spoken of in polite company. In indirect cultures, such as those in Latin America, business conversations may start with discussions of the weather, or family, or topics other than business as the partners gain a sense of each other, long before the topic of business is raised. Again, the skilled business communicator researches the new environment before entering it, as a social faux pas, or error, can have a significant impact.\n\nDoes the car someone drives say something about them? You may consider that many people across the planet do not own a vehicle and that a car or truck is a statement of wealth. But beyond that, do the make and model reflect their personality? If you are from a materialistic culture, you may be inclined to say yes. If you are from a culture that values relationships rather than material objects, you may say no or focus on how the vehicle serves the family. From rocks that display beauty and wealth\u2014what we call jewelry\u2014to what you eat\u2014will it be lobster ravioli or prime rib?\u2014we express our values and cultural differences with our purchase decisions.\n\nMembers of a materialistic culture place emphasis on external goods and services as a representation of self, power, and social rank. If you consider the plate of food before you, and consider the labor required to harvest the grain, butcher the animal, and cook the meal, you are focusing more on the relationships involved with its production than the foods themselves. Caviar may be a luxury, and it may communicate your ability to acquire and offer a delicacy, but it also represents an effort. Cultures differ in how they view material objects and their relationship to them, and some value people and relationships more than the objects themselves. The United States and Japan are often noted as materialistic cultures, while many Scandinavian nations feature cultures that place more emphasis on relationships.\n\nHow comfortable are you with critiquing your boss\u2019s decisions? If you are from a low-power distance culture, your answer might be \u201cno problem.\u201d In low-power distance cultures, according to Hofstede, people relate to one another more as equals and less as a reflection of dominant or subordinate roles, regardless of their actual formal roles as employee and manager, for example.\n\nIn a high-power distance culture, you would probably be much less likely to challenge the decision, to provide an alternative, or to give input. If you are working with people from a high-power distance culture, you may need to take extra care to elicit feedback and involve them in the discussion because their cultural framework may preclude their participation. They may have learned that less powerful people must accept decisions without comment, even if they have a concern or know there is a significant problem. Unless you are sensitive to cultural orientation and power distance, you may lose valuable information.\n\nCultures have distinct orientations when it comes to rules, uncertainty, time and time horizon, masculinity, directness, materialism, and power distance.\n\nBerger, C., & Calabrese, R. (1975). _Some explorations in initial interactions and beyond: Toward a developmental theory of interpersonal communication._ Human communication Research, 1, 99\u2013112.\n\nDodd, C. (1998). _Dynamics of intercultural communication_ (5th ed.). New York, NY: Harper & Row.\n\nHall, M. R., & Hall, E. T. (1987). _Hidden differences: Doing business with the Japanese._ New York, NY: Doubleday.\n\nHofstede, G. (1982). _Culture\u2019s consequences_ (2nd ed.). Newbury Park, CA: Sage.\n\nHofstede, G. (2001). _Culture\u2019s consequences: Comparing values, behaviors, institutions, and organizations across nations_ (2nd ed.). Thousand Oaks, CA: Sage.\n\nHofstede, G. (2005). _Cultures and organizations: Software of the mind_ (2nd ed.). New York, NY: McGraw-Hill.","text":"Learning Objectives\n\n1. Discuss divergent cultural characteristics and list several examples of such characteristics in the culture(s) you identify with.\n\nWe are not created equal. We are born light- or dark-skinned, to parents of education or parents without access to education, and we grow up short or tall, slender or stocky. Our life chances or options are in many ways determined by our birth. The Victorian \u201crags to riches\u201d novels that Horatio Alger wrote promoted the ideal that individuals can overcome all obstacles, raising themselves up by their bootstraps. Some people do have amazing stories, but even if you are quick to point out that Microsoft founder Bill Gates became fabulously successful despite his lack of a college education, know that his example is exception, not the rule. We all may use the advantages of our circumstances to improve our lives, but the type and extent of those advantages vary greatly across the planet.\n\nCultures reflect this inequality, this diversity, and the divergent range of values, symbols, and meanings across communities. Can you tie a knot? Perhaps you can tie your shoes, but can you tie a knot to secure a line to a boat, to secure a heavy load on a cart or truck, or to bundle a bale of hay? You may not be able to, but if you were raised in a culture that place a high value on knot-tying for specific purposes, you would learn that which your community values. We all have viewpoints, but they are shaped by our interactions with our communities. Let\u2019s examine several points of divergence across cultures.\n\nPeople in individualistic cultures value individual freedom and personal independence, and cultures always have stories to reflect their values. You may recall the story of Superman, or John McLean in the Diehard series, and note how one person overcomes all obstacles. Through personal ingenuity, in spite of challenges, one person rises successfully to conquer or vanquish those obstacles. Sometimes there is an assist, as in basketball or football, where another person lends a hand, but still the story repeats itself again and again, reflecting the cultural viewpoint.\n\nThe Dutch researcher Geert Hofstede explored the concepts of individualism and collectivism across diverse cultures (Hofstede, G., 1982; Hofstede, G., 2001; Hofstede, G., 2005). He found that in individualistic cultures like the United States, people perceived their world primarily from their own viewpoint. They perceived themselves as empowered individuals, capable of making their own decisions, and able to make an impact on their own lives.\n\nCultural viewpoint is not an either\/or dichotomy, but rather a continuum or range. You may belong to some communities that express individualistic cultural values, while others place the focus on a collective viewpoint. Collectivist cultures (Hofstede, G., 1982), including many in Asia and South America, focus on the needs of the nation, community, family, or group of workers.\u00a0\n\nOwnership and private property is one way to examine this difference. In some cultures, property is almost exclusively private, while others tend toward community ownership. The collectively owned resource returns benefits to the community. Water, for example, has long been viewed as a community resource, much like air, but that has been changing as business and organizations have purchased water rights and gained control over resources.\u00a0\n\nPublic lands, such as parks, are often considered public, and individual exploitation of them is restricted. Copper, a metal with a variety of industrial applications, is collectively owned in Chile, with profits deposited in the general government fund. While public and private initiatives exist, the cultural viewpoint is our topic. How does someone raised in a culture that emphasizes the community interact with someone raised in a primarily individualistic culture? How could tensions be expressed and how might interactions be influenced by this point of divergence?\n\nDo you know the rules of your business or organization? Did you learn them from an employee manual or by observing the conduct of others? Your response may include both options, but not all cultures communicate rules in the same way. Carley Dodd discusses this difference and has found quite a range of difference. In an explicit-rule culture, where rules are clearly communicated so that everyone is aware of them, the guidelines and agenda for a meeting are announced prior to the gathering. In an implicit-rule culture, where rules are often understood and communicated nonverbally, there may be no agenda. Everyone knows why they are gathered and what role each member plays, even though the expectations may not be clearly stated. Power, status, and behavioral expectations may all be understood, and to the person from outside this culture, it may prove a challenge to understand the rules of the context.\n\nOutsiders often communicate their \u201cotherness\u201d by not knowing where to stand, when to sit, or how to initiate a conversation if the rules are not clearly stated. While it may help to know that implicit-rule cultures are often more tolerant of deviation from the understood rules, the newcomer will be wise to learn by observing quietly\u2014and to do as much research ahead of the event as possible.\n\nWhen we meet each other for the first time, we often use what we have previously learned to understand our current context. We also do this to reduce our uncertainty. Some cultures, such as the United States and Britain, are highly tolerant of uncertainty, while others go to great lengths to reduce the element of surprise. Cultures in the Arab world, for example, are high in uncertainty avoidance; they tend to be resistant to change and reluctant to take risks. Whereas a U.S. business negotiator might enthusiastically agree to try a new procedure, the Egyptian counterpart would likely refuse to get involved until all the details are worked out.\n\nCharles Berger and Richard Calabrese developed uncertainty reduction theory to examine this dynamic aspect of communication. Here are seven axioms of uncertainty:\n\nThere is a high level of uncertainty at first. As we get to know one another, our verbal communication increases and our uncertainty begins to decrease.Following verbal communication, nonverbal communication increases, uncertainty continues to decrease, and more nonverbal displays of affiliation, like nodding one\u2019s head to indicate agreement, will start to be expressed.When experiencing high levels of uncertainty, we tend to increase our information-seeking behavior, perhaps asking questions to gain more insight. As our understanding increases, uncertainty decreases, as does the information-seeking behavior.When experiencing high levels of uncertainty, the communication interaction is not as personal or intimate. As uncertainty is reduced, intimacy increases.When experiencing high levels of uncertainty, communication will feature more reciprocity, or displays of respect. As uncertainty decreases, reciprocity may diminish.Differences between people increase uncertainty, while similarities decrease it.Higher levels of uncertainty are associated with a decrease in the indication of liking the other person, while reductions in uncertainty are associated with liking the other person more.\n\nEdward T. Hall and Mildred Reed Hall state that monochronic time-oriented cultures consider one thing at a time, whereas polychronic time-oriented cultures schedule many things at one time, and time is considered in a more fluid sense. In monochromatic time, interruptions are to be avoided, and everything has its own specific time. Even the multitasker from a monochromatic culture will, for example, recognize the value of work first before play or personal time. The United States, Germany, and Switzerland are often noted as countries that value a monochromatic time orientation.\n\nPolychromatic time looks a little more complicated, with business and family mixing with dinner and dancing. Greece, Italy, Chile, and Saudi Arabia are countries where one can observe this perception of time; business meetings may be scheduled at a fixed time, but when they actually begin may be another story. Also note that the dinner invitation for 8 p.m. may in reality be more like 9 p.m. If you were to show up on time, you might be the first person to arrive and find that the hosts are not quite ready to receive you.\n\nWhen in doubt, always ask before the event; many people from polychromatic cultures will be used to foreigner\u2019s tendency to be punctual, even compulsive, about respecting established times for events. The skilled business communicator is aware of this difference and takes steps to anticipate it. The value of time in different cultures is expressed in many ways, and your understanding can help you communicate more effectively.\n\nDo you want your reward right now or can you dedicate yourself to a long-term goal? You may work in a culture whose people value immediate results and grow impatient when those results do not materialize. Geert Hofstede discusses this relationship of time orientation to a culture as a \u201ctime horizon,\u201d and it underscores the perspective of the individual within a cultural context. Many countries in Asia, influenced by the teachings of Confucius, value a long-term orientation, whereas other countries, including the United States, have a more short-term approach to life and results. Native American cultures are known for holding a long-term orientation, as illustrated by the proverb attributed to the Iroquois that decisions require contemplation of their impact seven generations removed.\n\nIf you work within a culture that has a short-term orientation, you may need to place greater emphasis on reciprocation of greetings, gifts, and rewards. For example, if you send a thank-you note the morning after being treated to a business dinner, your host will appreciate your promptness. While there may be a respect for tradition, there is also an emphasis on personal representation and honor, a reflection of identity and integrity. Personal stability and consistency are also valued in a short-term oriented culture, contributing to an overall sense of predictability and familiarity.\n\nLong-term orientation is often marked by persistence, thrift and frugality, and an order to relationships based on age and status. A sense of shame for the family and community is also observed across generations. What an individual does reflects on the family and is carried by immediate and extended family members.\n\nThere was a time when many cultures and religions valued a female figurehead, and with the rise of Western cultures we have observed a shift toward a masculine ideal. Each carries with it a set of cultural expectations and norms for gender behavior and gender roles across life, including business.\n\nHofstede describes the masculine-feminine dichotomy not in terms of whether men or women hold the power in a given culture, but rather the extent to which that culture values certain traits that may be considered masculine or feminine. Thus, \u201cthe assertive pole has been called \u2018masculine\u2019 and the modest, caring pole \u2018feminine.\u2019 The women in feminine countries have the same modest, caring values as the men; in the masculine countries they are somewhat assertive and competitive, but not as much as the men, so that these countries show a gap between men\u2019s values and women\u2019s values\u201d (Hofstede, G., 2009).\n\nWe can observe this difference in where people gather, how they interact, and how they dress. We can see it during business negotiations, where it may make an important difference in the success of the organizations involved. Cultural expectations precede the interaction, so someone who doesn\u2019t match those expectations may experience tension. Business in the United States has a masculine orientation\u2014assertiveness and competition are highly valued. In other cultures, such as Sweden, business values are more attuned to modesty (lack of self-promotion) and taking care of society\u2019s weaker members. This range of difference is one aspect of intercultural communication that requires significant attention when the business communicator enters a new environment.\n\nIn the United States, business correspondence is expected to be short and to the point. \u201cWhat can I do for you?\u201d is a common question when a business person receives a call from a stranger; it is an accepted way of asking the caller to state his or her business. In some cultures it is quite appropriate to make direct personal observation, such as \u201cYou\u2019ve changed your hairstyle,\u201d while for others it may be observed, but never spoken of in polite company. In indirect cultures, such as those in Latin America, business conversations may start with discussions of the weather, or family, or topics other than business as the partners gain a sense of each other, long before the topic of business is raised. Again, the skilled business communicator researches the new environment before entering it, as a social faux pas, or error, can have a significant impact.\n\nDoes the car someone drives say something about them? You may consider that many people across the planet do not own a vehicle and that a car or truck is a statement of wealth. But beyond that, do the make and model reflect their personality? If you are from a materialistic culture, you may be inclined to say yes. If you are from a culture that values relationships rather than material objects, you may say no or focus on how the vehicle serves the family. From rocks that display beauty and wealth\u2014what we call jewelry\u2014to what you eat\u2014will it be lobster ravioli or prime rib?\u2014we express our values and cultural differences with our purchase decisions.\n\nMembers of a materialistic culture place emphasis on external goods and services as a representation of self, power, and social rank. If you consider the plate of food before you, and consider the labor required to harvest the grain, butcher the animal, and cook the meal, you are focusing more on the relationships involved with its production than the foods themselves. Caviar may be a luxury, and it may communicate your ability to acquire and offer a delicacy, but it also represents an effort. Cultures differ in how they view material objects and their relationship to them, and some value people and relationships more than the objects themselves. The United States and Japan are often noted as materialistic cultures, while many Scandinavian nations feature cultures that place more emphasis on relationships.\n\nHow comfortable are you with critiquing your boss\u2019s decisions? If you are from a low-power distance culture, your answer might be \u201cno problem.\u201d In low-power distance cultures, according to Hofstede, people relate to one another more as equals and less as a reflection of dominant or subordinate roles, regardless of their actual formal roles as employee and manager, for example.\n\nIn a high-power distance culture, you would probably be much less likely to challenge the decision, to provide an alternative, or to give input. If you are working with people from a high-power distance culture, you may need to take extra care to elicit feedback and involve them in the discussion because their cultural framework may preclude their participation. They may have learned that less powerful people must accept decisions without comment, even if they have a concern or know there is a significant problem. Unless you are sensitive to cultural orientation and power distance, you may lose valuable information.\n\nCultures have distinct orientations when it comes to rules, uncertainty, time and time horizon, masculinity, directness, materialism, and power distance.\n\nBerger, C., & Calabrese, R. (1975). Some explorations in initial interactions and beyond: Toward a developmental theory of interpersonal communication. Human communication Research, 1, 99\u2013112.\n\nDodd, C. (1998). Dynamics of intercultural communication (5th ed.). New York, NY: Harper & Row.\n\nHall, M. R., & Hall, E. T. (1987). Hidden differences: Doing business with the Japanese. New York, NY: Doubleday.\n\nHofstede, G. (1982). Culture\u2019s consequences (2nd ed.). Newbury Park, CA: Sage.\n\nHofstede, G. (2001). Culture\u2019s consequences: Comparing values, behaviors, institutions, and organizations across nations (2nd ed.). Thousand Oaks, CA: Sage.\n\nHofstede, G. (2005). Cultures and organizations: Software of the mind (2nd ed.). New York, NY: McGraw-Hill.","contextuality":{"text":"The text explores the inherent inequalities and diverse cultural perspectives that shape human experiences and interactions. It contrasts individualistic ________, such as the United States, where personal _______ and independence are emphasized, with collectivist ________, which prioritize community and group needs. The work of Geert Hofstede is highlighted, focusing on how different ________ perceive individualism, collectivism, and other dimensions ____ time orientation and masculinity-femininity. Uncertainty reduction theory by Charles Berger and Richard Calabrese is discussed, demonstrating how communication _______ as uncertainty decreases. The text also ________ the cultural significance of materialism versus relational values, showing how these ___________ influence personal and business interactions across the globe. _____________ these cultural nuances is essential for effective intercultural communication and __________ global business landscapes.","gaps":[["cultures",154,8],["freedom",206,7],["cultures",265,8],["cultures",388,8],["like",456,4],["evolves",621,7],["examines",669,8],["differences",763,11],["Understanding",838,13],["navigating",934,10]]},"contextuality_plus":{"text":"The text explores the inherent inequalities and diverse cultural perspectives that shape human experiences and interactions. It contrasts individualistic ________, such as the United States, where personal _______ and independence are emphasized, with collectivist ________, which prioritize community and group needs. The work of Geert Hofstede is highlighted, focusing on how different ________ perceive individualism, collectivism, and other __________ like time orientation and masculinity-femininity. ___________ reduction theory by Charles Berger and Richard Calabrese is discussed, demonstrating how communication evolves as ___________ decreases. The text also examines the cultural significance of materialism versus relational values, showing how these ___________ influence personal and business interactions across the globe. _____________ these cultural nuances is essential for effective intercultural communication and __________ global business landscapes.","gaps":[["cultures",154,8],["freedom",206,7],["cultures",265,8],["cultures",388,8],["dimensions",445,10],["Uncertainty",506,11],["uncertainty",632,11],["differences",763,11],["Understanding",838,13],["navigating",934,10]]},"keyword":{"text":"The text explores the inherent inequalities and diverse cultural perspectives that shape human experiences and interactions. It contrasts individualistic ________, such as the United States, where personal freedom and independence are emphasized, with collectivist ________, which prioritize community and group needs. The work of Geert Hofstede is highlighted, focusing on how different ________ perceive individualism, collectivism, and other dimensions like time orientation and masculinity-femininity. Uncertainty reduction theory by Charles Berger and Richard Calabrese is discussed, demonstrating how communication evolves as uncertainty decreases. The text also examines the ________ significance of materialism versus relational values, showing how these differences influence personal and business interactions across the globe. Understanding these ________ nuances is essential for effective inter________ communication and navigating global business landscapes.","gaps":[["cultural",682,8],["cultural",858,8],["cultural",907,8],["cultures",154,8],["cultures",265,8],["cultures",388,8]]}}
{"volume":"formal-operator-training-guidelines","page":"4-control-room-operator-cro-training","summary":"To qualify as a Control Room Operator (CRO), candidates must meet specific training requirements outlined in the provided document. Those who fail the CRO fundamentals course can retake it with STL approval, and further failures require recommendations and support from various authorities before retesting. Candidates must also be qualified on related field jobs prior to Post-CRO Fundamentals training, during which they are mentored and undergo unit-specific console training using simulators. Successful completion of training is verified through the LMS system, and all documentation must be properly filed. CROs must maintain qualifications by working shifts or completing simulator sessions each quarter, and they are required to pass refresher exams every three years with at least an 80% score. Failure to meet training or exam requirements is addressed by a Board of Review to determine corrective actions.","markdown":"**Any deviation to program content or required times must be approved by the business unit Area Section Head and documented on a KR-988, Training Program Waiver form. This waiver is used for extenuating circumstances and requests are considered by the Area Section Head on a case-by-case basis.**\n\nFor a list of CRO training requirements, press control and click\u00a0[HERE](https:\/\/www.pasms.chevron.net\/operations\/training\/Console%20Operator%20Requirements.pdf?t=1515164738231). Refer to this document for specific minimum requirements that must be completed to become qualified on a console.\n\nIf an operator is unsuccessful in the CRO fundamentals course, they will be permitted to retake the course with STL approval. If an operator is unsuccessful a second time, the Area Section Head and Refinery Business Manager must recommend the operator for re-testing. The Operations PDC must support this recommendation before the operator is permitted to retest. A third failure may exclude the candidate from future consideration for a CRO position.\n\nOperators must be qualified on field jobs associated with the console before beginning Post-CRO Fundamentals training.\n\nThe operator will be assigned a mentor\/trainer\/trainers who will work together as the operator begins to learn the dynamics of the control room operator job. Together they will review all Post-CRO Fundamentals requirements listed in the link above, then contact the Unit Field Trainer to begin their unit specific console training.\u00a0 The UFT will provide a training folder and track the progress to ensure all training is complete within the time allotted.\u00a0\n\nThe console training team maintains and provides unit support with Honeywell simulators for the FCC, ISO I, Coker, Crude I, Sulfur SRU2\/3 & SCOT and Yokogawa simulators for RDS, HCR, FPU, IDW, CCR\/PSA, RSU, ISO II and Crude II. The use of these simulators during console operator training is required along with simulation exercise sheets. If no console simulator is available, training will be conducted using procedures and simulation sheets at the console or HO station without making changes to the process.\n\nSuccessfully completing a QRB is required for all console operators on the first console.\u00a0All completed, original\u00a0training paperwork, including the KR-981A and KR\u2011984A (QRB) must be forwarded to the LMS Specialist in L&D to receive electronic credit in LMS.\u00a0The LMS Specialist will ensure documents are properly filed.\u00a0 UFT\u2019s will review and ensure all training has been completed and send package to LMS Specialist who will give credit in LMS and forward to central records. It is recommended the STL retain copies in the employee\u2019s desk file. Failure to meet the outlined requirements or failure to complete training within the specified timeline in Table 9 below, will be handled by a Board of Review to address the specific deficiencies and to recommend a course of corrective action.\n\n| FQO: General time expectations for completing training requirements |     |     |     |     |     |     |\n| --- | --- | --- | --- | --- | --- | --- |\n| **1st Console** | 1st Month | 2nd Month | 3rd Month | 4th Month | 5th Month | 6th Month |\n| **Completion Goal** | Solo CL<br><br>Best Practice CL<br><br>Simulation Exercise Sheet(s)<br><br>STL Ca | 4 Sit Prob<br><br>3 Dem Tests | 8 Sit Prob<br><br>6 Dem Tests | 12 Sit Prob<br><br>9 Dem Tests | 15 Sit Prob<br><br>10 Dem Tests | QRB |\n| **Other Console** | 1st Month | 2nd Month | 3rd Month | 4th Month |     |     |\n| **Completion Goal** | Solo CL<br><br>Best Practice CL<br><br>Simulation Exercise Sheet(s)<br><br>STL Ca | Reviewed<br><br>5 Sit Prob &<br><br>3 Dem Tests | Reviewed<br><br>10 Sit Prob &<br><br>6 Dem Tests | Reviewed<br><br>15 Sit Prob &<br><br>10 Dem Tests |     |     |\n| **FQO: Maximum time expectations for completing any job requirements: 12 months after STL Checklist completion** |     |     |     |     |     |     |\n\nTable\u00a07: FQO CRO Job Training Timeline\n\nCROs and HOs who have maintained their console qualification must work a minimum of one 12-hour shift per quarter on each console, or complete one 10-hour session on the console simulator if available. (With STL approval HOs and CROs may complete the 12-hour shift while shadowing with a qualified CRO. This is due to process and DCS changes that may have occurred to ensure this training is completed in a safe manner).\n\n#### Console Simulators\n\nSimulator training is offered weekly for refresher, unusual events, procedure reviews and CRO unit schools.\u00a0\n\n#### Simulations Sheets and Procedure Reviews\n\nIt is highly recommended that ALL CROs complete a minimum of 4 simulation exercise sheets annually conducting procedure reviews using the console simulators or HO station without making changes to the process.\n\nRefresher Examinations\n\nRefresher examinations are required every three years and are designed to demonstrate the console operator\u2019s knowledge of a specific job. Each console job has a refresher examination comprised of 25 to 40 questions. Each CRO must complete a refresher exam for each job on which they are qualified with a minimum passing score of 80%. If the CRO is unsuccessful, a Board of Review will be convened to address the specific deficiencies and to recommend a course of corrective action.\n\nL&D offers a series of Console training enhancement courses including, Advanced Refinery Distillation Skills for advanced console operators. Space is limited and participants are chosen by area leadership.\n\nWhen a Qualified CRO is transferred to a different area within the refinery, the CRO must complete all the requirements specified in the outlines above for Fully Qualified Operators or as a Trainee before serving as a Qualified CRO in the new area.","text":"Any deviation to program content or required times must be approved by the business unit Area Section Head and documented on a KR-988, Training Program Waiver form. This waiver is used for extenuating circumstances and requests are considered by the Area Section Head on a case-by-case basis.\n\nFor a list of CRO training requirements, press control and click\u00a0HERE. Refer to this document for specific minimum requirements that must be completed to become qualified on a console.\n\nIf an operator is unsuccessful in the CRO fundamentals course, they will be permitted to retake the course with STL approval. If an operator is unsuccessful a second time, the Area Section Head and Refinery Business Manager must recommend the operator for re-testing. The Operations PDC must support this recommendation before the operator is permitted to retest. A third failure may exclude the candidate from future consideration for a CRO position.\n\nOperators must be qualified on field jobs associated with the console before beginning Post-CRO Fundamentals training.\n\nThe operator will be assigned a mentor\/trainer\/trainers who will work together as the operator begins to learn the dynamics of the control room operator job. Together they will review all Post-CRO Fundamentals requirements listed in the link above, then contact the Unit Field Trainer to begin their unit specific console training.\u00a0 The UFT will provide a training folder and track the progress to ensure all training is complete within the time allotted.\u00a0\n\nThe console training team maintains and provides unit support with Honeywell simulators for the FCC, ISO I, Coker, Crude I, Sulfur SRU2\/3 & SCOT and Yokogawa simulators for RDS, HCR, FPU, IDW, CCR\/PSA, RSU, ISO II and Crude II. The use of these simulators during console operator training is required along with simulation exercise sheets. If no console simulator is available, training will be conducted using procedures and simulation sheets at the console or HO station without making changes to the process.\u00a0\n\nSuccessfully completing a QRB is required for all console operators on the first console.\u00a0All completed, original\u00a0training paperwork, including the KR-981A and KR\u2011984A (QRB) must be forwarded to the LMS Specialist in L&D to receive electronic credit in LMS.\u00a0The LMS Specialist will ensure documents are properly filed.\u00a0 UFT\u2019s will review and ensure all training has been completed and send package to LMS Specialist who will give credit in LMS and forward to central records. It is recommended the STL retain copies in the employee\u2019s desk file. Failure to meet the outlined requirements or failure to complete training within the specified timeline in Table 9 below, will be handled by a Board of Review to address the specific deficiencies and to recommend a course of corrective action.\n\nTable\u00a07: FQO CRO Job Training Timeline\n\nCROs and HOs who have maintained their console qualification must work a minimum of one 12-hour shift per quarter on each console, or complete one 10-hour session on the console simulator if available. (With STL approval HOs and CROs may complete the 12-hour shift while shadowing with a qualified CRO. This is due to process and DCS changes that may have occurred to ensure this training is completed in a safe manner).\u00a0\n\nConsole Simulators\nSimulator training is offered weekly for refresher, unusual events, procedure reviews and CRO unit schools.\u00a0\nSimulations Sheets and Procedure Reviews\nIt is highly recommended that ALL CROs complete a minimum of 4 simulation exercise sheets annually conducting procedure reviews using the console simulators or HO station without making changes to the process.\n\nRefresher Examinations\nRefresher examinations are required every three years and are designed to demonstrate the console operator\u2019s knowledge of a specific job. Each console job has a refresher examination comprised of 25 to 40 questions. Each CRO must complete a refresher exam for each job on which they are qualified with a minimum passing score of 80%. If the CRO is unsuccessful, a Board of Review will be convened to address the specific deficiencies and to recommend a course of corrective action.\n\nL&D offers a series of Console training enhancement courses including, Advanced Refinery Distillation Skills for advanced console operators. Space is limited and participants are chosen by area leadership.\u00a0\n\nWhen a Qualified CRO is transferred to a different area within the refinery, the CRO must complete all the requirements specified in the outlines above for Fully Qualified Operators or as a Trainee before serving as a Qualified CRO in the new area.","contextuality":{"text":"To qualify as a Control Room Operator (CRO), candidates must meet specific training requirements outlined in the provided document. Those who fail the CRO fundamentals course can retake it with STL ________, and further failures require recommendations and support from various authorities before _________. Candidates must also be qualified on related field jobs _____ to Post-CRO Fundamentals training, during which they are ________ and undergo unit-specific console training using __________. Successful completion of training is verified through the LMS ______, and all documentation must be properly filed. CROs must maintain ______________ by working shifts or completing simulator sessions each quarter, and they are required to pass refresher exams every three _____ with at least an 80% score. Failure to meet training or exam ____________ is addressed by a Board of Review to _________ corrective actions.","gaps":[["approval",198,8],["retesting",297,9],["prior",364,5],["mentored",427,8],["simulators",485,10],["system",559,6],["qualifications",632,14],["years",770,5],["requirements",837,12],["determine",887,9]]},"contextuality_plus":{"text":"To qualify as a Control Room Operator (CRO), candidates must meet specific training requirements outlined in the provided document. Those who fail the CRO fundamentals course can ______ it with STL approval, and further failures require _______________ and support from various authorities before retesting. __________ must also be qualified on related field jobs prior to Post-CRO Fundamentals training, during which they are mentored and _______ unit-specific console training using simulators. Successful completion of training is ________ through the LMS system, and all documentation must be properly filed. CROs must maintain ______________ by working shifts or completing simulator sessions each _______, and they are required to pass refresher _____ every three years with at least an 80% score. Failure to meet training or ____ requirements is addressed by a Board of Review to _________ corrective actions.","gaps":[["retake",179,6],["recommendations",237,15],["Candidates",308,10],["undergo",440,7],["verified",534,8],["qualifications",632,14],["quarter",703,7],["exams",752,5],["exam",832,4],["determine",887,9]]},"keyword":{"text":"To qualify as a Control Room Operator (CRO), candidates must meet specific training requirements outlined in the provided document. Those who fail the CRO ____________ course can retake it with STL approval, and further failures require recommendations and support from various authorities before retesting. Candidates must also be qualified on related field jobs prior to Post-___ Fundamentals training, during which they are mentored and undergo unit-________ console training using simulators. Successful completion of ________ is verified through the LMS system, and all ________ation must be properly filed. ___s must maintain qualifications by working shifts or completing simulator sessions each quarter, and they are required to pass refresher exams every three years with at least an 80% score. Failure to meet ________ or exam requirements is addressed by a Board of Review to determine corrective actions.","gaps":[["specific",453,8],["fundamentals",155,12],["document",575,8],["CRO",378,3],["CRO",613,3],["training",522,8],["training",820,8]]}}
{"volume":"research-methods-in-psychology","page":"3-goals-of-science","summary":"The exploration of human behavior and the natural world has fueled scientific inquiry, with psychology leveraging this curiosity to build a knowledge base grounded in research. Science aims to describe, predict, and explain phenomena, with each goal requiring careful observation and analysis. In psychology, descriptions stem from detailed observations, such as understanding the conditions treated by medical marijuana through patient surveys. The goal of prediction uses established relationships to forecast behaviors, like predicting pain management in medical marijuana users. Finally, explanation seeks to uncover causal mechanisms, such as investigating how marijuana alleviates pain, either through reducing inflammation or distress. Science continuously expands our understanding, revealing deeper insights into human behavior and the natural world.","markdown":"<i-callout variant=\"info\" title=\"Learning Objectives\">\n\n1\\. Describe the three goals of science and give an example for each.\n\n2\\. Distinguish between basic research and applied research.\n\n<\/i-callout>\n\nPeople have always been curious about the natural world, including themselves and their behavior (in fact, this is probably why you are studying psychology in the first place). Science grew out of this natural curiosity and has become the best way to achieve detailed and accurate knowledge. Keep in mind that most of the phenomena and theories that fill psychology textbooks are the products of scientific research. In a typical introductory psychology textbook, for example, one can learn about specific cortical areas for language and perception, principles of classical and operant conditioning, biases in reasoning and judgment, and people\u2019s surprising tendency to obey those in positions of authority. And scientific research continues because what we know right now only scratches the surface of what we\u00a0can\u00a0know.\n\nThe first and most basic goal of science is **to** **describe**. This goal is achieved by making careful observations. As an example, perhaps I am interested in better understanding the medical conditions that medical marijuana patients use marijuana to treat. In this case, I could try to access records at several large medical marijuana licensing centers to see which conditions people are getting licensed to use medical marijuana. Or I could survey a large sample of medical marijuana patients and ask them to report which medical conditions they use marijuana to treat or manage. Indeed, research involving surveys of medical marijuana patients has been conducted and has found that the primary symptom medical marijuana patients use marijuana to treat is pain, followed by anxiety and depression (Sexton, Cuttler, Finnell, & Mischley, 2016).\\[1\\].\n\nThe second goal of science is **to predict**. Once we have observed with some regularity that two behaviors or events are systematically related to one another we can use that information to predict whether an event or behavior will occur in a certain situation. Once I know that most medical marijuana patients use marijuana to treat pain I can use that information to predict that an individual who uses medical marijuana likely experiences pain. Of course, my predictions will not be 100% accurate but if the relationship between medical marijuana use and pain is strong then my predictions will have greater than chance accuracy.\u00a0\n\nThe third and ultimate goal of science is **to explain**. This goal involves determining the causes of behavior. For example, researchers might try to understand the mechanisms through which marijuana reduces pain. Does marijuana reduce inflammation which in turn reduces pain? Or does marijuana simply reduce the distress associated with pain rather than reducing pain itself? As you can see these questions tap at the underlying mechanisms and causal relationships.\n\n1.  Sexton, M., Cuttler, C., Finnell, J., & Mischley, L (2016). A cross-sectional survey of medical cannabis users: Patterns of use and perceived efficacy. _Cannabis and Cannabinoid Research, 1_,\u00a0131-138. doi: 10.1089\/can.2016.0007.","text":"Learning Objectives\n\n1. Describe the three goals of science and give an example for each.\n2. Distinguish between basic research and applied research. \u00a0\n\nPeople have always been curious about the natural world, including themselves and their behavior (in fact, this is probably why you are studying psychology in the first place). Science grew out of this natural curiosity and has become the best way to achieve detailed and accurate knowledge. Keep in mind that most of the phenomena and theories that fill psychology textbooks are the products of scientific research. In a typical introductory psychology textbook, for example, one can learn about specific cortical areas for language and perception, principles of classical and operant conditioning, biases in reasoning and judgment, and people\u2019s surprising tendency to obey those in positions of authority. And scientific research continues because what we know right now only scratches the surface of what we\u00a0can\u00a0know. \u00a0\n\nThe first and most basic goal of science is to describe. This goal is achieved by making careful observations. As an example, perhaps I am interested in better understanding the medical conditions that medical marijuana patients use marijuana to treat. In this case, I could try to access records at several large medical marijuana licensing centers to see which conditions people are getting licensed to use medical marijuana. Or I could survey a large sample of medical marijuana patients and ask them to report which medical conditions they use marijuana to treat or manage. Indeed, research involving surveys of medical marijuana patients has been conducted and has found that the primary symptom medical marijuana patients use marijuana to treat is pain, followed by anxiety and depression (Sexton, Cuttler, Finnell, & Mischley, 2016).[1].\n\nThe second goal of science is to predict. Once we have observed with some regularity that two behaviors or events are systematically related to one another we can use that information to predict whether an event or behavior will occur in a certain situation. Once I know that most medical marijuana patients use marijuana to treat pain I can use that information to predict that an individual who uses medical marijuana likely experiences pain. Of course, my predictions will not be 100% accurate but if the relationship between medical marijuana use and pain is strong then my predictions will have greater than chance accuracy.\u00a0\n\nThe third and ultimate goal of science is to explain. This goal involves determining the causes of behavior. For example, researchers might try to understand the mechanisms through which marijuana reduces pain. Does marijuana reduce inflammation which in turn reduces pain? Or does marijuana simply reduce the distress associated with pain rather than reducing pain itself? As you can see these questions tap at the underlying mechanisms and causal relationships.\n\nSexton, M., Cuttler, C., Finnell, J., & Mischley, L (2016). A cross-sectional survey of medical cannabis users: Patterns of use and perceived efficacy. Cannabis and Cannabinoid Research, 1,\u00a0131-138. doi: 10.1089\/can.2016.0007.\u00a0","contextuality":{"text":"The exploration of human behavior and the natural world has fueled scientific inquiry, with psychology leveraging this curiosity to build a knowledge base grounded in research. _______ aims to describe, predict, and _______ phenomena, with each goal requiring careful observation and ________. In psychology, descriptions stem from detailed ____________, such as understanding the conditions treated by medical marijuana through patient _______. The goal of prediction uses established relationships to forecast behaviors, like __________ pain management in medical marijuana users. Finally, explanation seeks to uncover ______ mechanisms, such as investigating how marijuana alleviates pain, either through reducing ____________ or distress. Science continuously expands our understanding, revealing ______ insights into human behavior and the natural _____.","gaps":[["Science",177,7],["explain",216,7],["analysis",284,8],["observations",341,12],["surveys",437,7],["predicting",528,10],["causal",621,6],["inflammation",717,12],["deeper",801,6],["world",853,5]]},"contextuality_plus":{"text":"The exploration of human behavior and the natural world has fueled scientific inquiry, with psychology leveraging this curiosity to build a knowledge base grounded in research. Science aims to describe, predict, and _______ phenomena, with each goal requiring careful ___________ and analysis. In psychology, descriptions ____ from detailed observations, such as understanding the conditions treated by medical marijuana through patient _______. The goal of prediction uses established _____________ to forecast behaviors, like predicting pain management in medical marijuana users. Finally, ___________ seeks to uncover causal mechanisms, such as _____________ how marijuana alleviates pain, either through reducing ____________ or distress. Science continuously expands our understanding, revealing ______ insights into human behavior and the natural _____.","gaps":[["explain",216,7],["observation",268,11],["stem",322,4],["surveys",437,7],["relationships",486,13],["explanation",592,11],["investigating",648,13],["inflammation",717,12],["deeper",801,6],["world",853,5]]},"keyword":{"text":"The exploration of human behavior and the natural world has fueled scientific inquiry, with psychology leveraging this curiosity to build a knowledge base grounded in research. Science aims to describe, predict, and explain phenomena, with each goal requiring careful observation and analysis. In psychology, descriptions stem from detailed observations, such as understanding the conditions treated by medical _________ through patient surveys. The goal of prediction uses established relationships to forecast behaviors, like predicting pain management in medical _________ users. Finally, explanation seeks to uncover causal mechanisms, such as investigating how _________ alleviates pain, either through reducing inflammation or distress. Science continuously expands our understanding, revealing deeper insights into human behavior and the natural world.","gaps":[["marijuana",411,9],["marijuana",566,9],["marijuana",666,9]]}}
{"volume":"research-methods-in-psychology-demo","page":"19-understanding-psychological-measurement-1","summary":"The text delves into the concept of measurement, highlighting its application across different fields, including psychology, where it is often referred to as psychometrics. Measurement involves assigning scores to individuals to represent certain characteristics, which can be straightforward, like age or weight, or more complex, like psychological constructs such as extraversion or intelligence. Constructs, not directly observable, are measured through systematic procedures and operational definitions, often involving self-report, behavioral, or physiological measures. The text also discusses Stevens's four levels of measurement\u2014nominal, ordinal, interval, and ratio\u2014that determine how scores are assigned and interpreted. Each level communicates varying degrees of quantitative information, guiding the statistical procedures that can be used with the data. These levels help in making meaningful comparisons and drawing conclusions from the measured data, emphasizing the importance of a true zero in ratio scales for making ratio-based statements.","markdown":"<i-callout variant=\"info\" title=\"Learning Objectives\">\n\n1\\. Define measurement and give several examples of measurement in psychology.\n\n2\\. Explain what a psychological construct is and give several examples.\n\n3\\. Distinguish conceptual from operational definitions, give examples of each, and create simple operational definitions.\n\n4\\. Distinguish the four levels of measurement, give examples of each, and explain why this distinction is important.\n\n<\/i-callout>\n\n**Measurement**\u00a0is the assignment of scores to individuals so that the scores represent some characteristic of the individuals. This very general definition is consistent with the kinds of measurement that everyone is familiar with\u2014for example, weighing oneself by stepping onto a bathroom scale, or checking the internal temperature of a roasting turkey using a meat thermometer. It is also consistent with measurement in the other sciences. In physics, for example, one might measure the potential energy of an object in Earth\u2019s gravitational field by finding its mass and height (which of course requires measuring\u00a0_those_\u00a0variables) and then multiplying them together along with the gravitational acceleration of Earth (9.8 m\/s2). The result of this procedure is a score that represents the object\u2019s potential energy. \u00a0\u00a0\n\nThis general definition of measurement is consistent with measurement in psychology too. (Psychological measurement is often referred to as **psychometrics**.) Imagine, for example, that a cognitive psychologist wants to measure a person\u2019s working memory capacity\u2014their ability to hold in mind and think about several pieces of information all at the same time. To do this, she might use a backward digit span task, in which she reads a list of two digits to the person and asks them to repeat them in reverse order. She then repeats this several times, increasing the length of the list by one digit each time, until the person makes an error. The length of the longest list for which the person responds correctly is the score and represents their working memory capacity. Or imagine a clinical psychologist who is interested in how depressed a person is. He administers the Beck Depression Inventory, which is a 21-item self-report questionnaire in which the person rates the extent to which they have felt sad, lost energy, and experienced other symptoms of depression over the past 2 weeks. The sum of these 21 ratings is the score and represents the person\u2019s current level of depression.\n\nThe important point here is that measurement does not require any particular instruments or procedures. What it\u00a0_does_\u00a0require is\u00a0_some_\u00a0systematic procedure for assigning scores to individuals or objects so that those scores represent the characteristic of interest.\n\nMany variables studied by psychologists are straightforward and simple to measure. These include age, height, weight, and birth order. You can ask people how old they are and be reasonably sure that they know and will tell you. Although people might not know or want to tell you how much they weigh, you can have them step onto a bathroom scale. Other variables studied by psychologists\u2014perhaps the majority\u2014are not so straightforward or simple to measure. We cannot accurately assess people\u2019s level of intelligence by looking at them, and we certainly cannot put their self-esteem on a bathroom scale. These kinds of variables are called\u00a0**constructs**\u00a0(pronounced\u00a0_CON-structs_) and include personality traits (e.g., extraversion), emotional states (e.g., fear), attitudes (e.g., toward taxes), and abilities (e.g., athleticism).\u00a0\n\nPsychological constructs cannot be observed directly. One reason is that they often represent\u00a0_tendencies_\u00a0to think, feel, or act in certain ways. For example, to say that a particular university student is highly extraverted does not necessarily mean that she is behaving in an extraverted way right now. In fact, she might be sitting quietly by herself, reading a book. Instead, it means that she has a general tendency to behave in extraverted ways (e.g., being outgoing, enjoying social interactions) across a variety of situations. Another reason psychological constructs cannot be observed directly is that they often involve internal processes. Fear, for example, involves the activation of certain central and peripheral nervous system structures, along with certain kinds of thoughts, feelings, and behaviors\u2014none of which is necessarily obvious to an outside observer. Notice also that neither extraversion nor fear \u201creduces to\u201d any particular thought, feeling, act, or physiological structure or process. Instead, each is a kind of summary of a complex set of behaviors and internal processes.\n\n_**The Big Five**_\n\nThe Big Five is a set of five broad dimensions that capture much of the variation in human personality. Each of the Big Five can even be defined in terms of six more specific constructs called \u201cfacets\u201d (Costa & McCrae, 1992)[\\[1\\]](https:\/\/kpu.pressbooks.pub\/psychmethods4e\/chapter\/understanding-psychological-measurement\/#footnote-54-1).\n\n|     |     |     |     |     |     |     |\n| --- | --- | --- | --- | --- | --- | --- |\n| Openness to experience | Fantasy | Aesthetics | Feelings | Actions | Ideas | Values |\n| Conscientiousness |     |     |     |     |     |     |\n| Extroversion |     |     |     |     |     |     |\n| Agreeableness |     |     |     |     |     |     |\n| Neuroticism |     |     |     |     |     |     |\n\nTable 4.1 The Big Five Personality Dimensions\n\n|     |     |     |     |     |     |     |\n| --- | --- | --- | --- | --- | --- | --- |\n| **Openness to experience** | **Fantasy** | **Aesthetics** | **Feelings** | **Actions** | **Ideas** | **Values** |\n| **Conscientiousness** | Competence | Order | Dutifulness | Achievement Striving | Self-discipline | Deliberation |\n| **Extroversion** | Warmth | Gregariousness | Assertiveness | Activity | Excitement seeking | Positive emotions |\n| **Agreeableness** | Trust | Straight-forwardness | Altruism | Compliance | Modesty | Tender mindedness |\n| **Neuroticism** | Worry | Anger | Discouragement | Self-consciousness | Impulsivity | Vulnerability |\n\nThe\u00a0**conceptual\u00a0definition**\u00a0of a psychological construct describes the behaviors and internal processes that make up that construct, along with how it relates to other variables. For example, a conceptual definition of neuroticism (another one of the Big Five) would be that it is people\u2019s tendency to experience negative emotions such as anxiety, anger, and sadness across a variety of situations. This definition might also include that it has a strong genetic component, remains fairly stable over time, and is positively correlated with the tendency to experience pain and other physical symptoms.\n\nStudents sometimes wonder why, when researchers want to understand a construct like self-esteem or neuroticism, they do not simply look it up in the dictionary. One reason is that many scientific constructs do not have counterparts in everyday language (e.g., working memory capacity). More important, researchers are in the business of developing definitions that are more detailed and precise\u2014and that more accurately describe the way the world is\u2014than the informal definitions in the dictionary. As we will see, they do this by proposing conceptual definitions, testing them empirically, and revising them as necessary. Sometimes they throw them out altogether. This is why the research literature often includes different conceptual definitions of the same construct. In some cases, an older conceptual definition has been replaced by a newer one that fits and works better. In others, researchers are still in the process of deciding which of various conceptual definitions is the best.\n\nAn\u00a0**operational\u00a0definition**\u00a0is a definition of a variable in terms of precisely how it is to be measured. These measures generally fall into one of three broad categories.\u00a0**Self-report\u00a0measures**\u00a0are those in which participants report on their own thoughts, feelings, and actions, as with the Rosenberg Self-Esteem Scale (Rosenberg, 1965)\\[2\\]. **Behavioral measures**\u00a0are those in which some other aspect of participants\u2019 behavior is observed and recorded. This is an extremely broad category that includes the observation of people\u2019s behavior both in highly structured laboratory tasks and in more natural settings. A good example of the former would be measuring working memory capacity using the backward digit span task. A good example of the latter is a famous operational definition of physical aggression from researcher Albert Bandura and his colleagues (Bandura, Ross, & Ross, 1961)\\[3\\]. They let each of several children play for 20 minutes in a room that contained a clown-shaped punching bag called a Bobo doll. They filmed each child and counted the number of acts of physical aggression the child committed. These included hitting the doll with a mallet, punching it, and kicking it. Their operational definition, then, was the number of these specifically defined acts that the child committed during the 20-minute period. Finally,\u00a0**physiological\u00a0measures**\u00a0are those that involve recording any of a wide variety of physiological processes, including heart rate and blood pressure, galvanic skin response, hormone levels, and electrical activity and blood flow in the brain.\u00a0\n\nFor any given variable or construct, there will be multiple operational definitions. Stress is a good example. A rough conceptual definition is that stress is an adaptive response to a perceived danger or threat that involves physiological, cognitive, affective, and behavioral components. But researchers have operationally defined it in several ways. The Social Readjustment Rating Scale (Holmes & Rahe, 1967)\\[4\\] is a self-report questionnaire on which people identify stressful events that they have experienced in the past year and assigns points for each one depending on its severity. For example, a man who has been divorced (73 points), changed jobs (36 points), and had a change in sleeping habits (16 points) in the past year would have a total score of 125. The Hassles and Uplifts Scale (Delongis, Coyne, Dakof, Folkman & Lazarus, 1982) \\[5\\]\u00a0is similar but focuses on everyday stressors like misplacing things and being concerned about one\u2019s weight. The Perceived Stress Scale (Cohen, Kamarck, & Mermelstein, 1983) \\[6\\] is another self-report measure that focuses on people\u2019s feelings of stress (e.g., \u201cHow often have you felt nervous and stressed?\u201d). Researchers have also operationally defined stress in terms of several physiological variables including blood pressure and levels of the stress hormone cortisol.\n\nWhen psychologists use multiple operational definitions of the same construct\u2014either within a study or across studies\u2014they are using **converging\u00a0operations**. The idea is that the various operational definitions are \u201cconverging\u201d or coming together on the same construct. When scores based on several different operational definitions are closely related to each other and produce similar patterns of results, this constitutes good evidence that the construct is being measured effectively and that it is useful. The various measures of stress, for example, are all correlated with each other and have all been shown to be correlated with other variables such as immune system functioning (also measured in a variety of ways) (Segerstrom & Miller, 2004)\\[7\\]. This is what allows researchers eventually to draw useful general conclusions, such as \u201cstress is negatively correlated with immune system functioning,\u201d as opposed to more specific and less useful ones, such as \u201cpeople\u2019s scores on the Perceived Stress Scale are negatively correlated with their white blood counts.\u201d\n\nThe psychologist S. S. Stevens suggested that scores can be assigned to individuals in a way that communicates more or less quantitative information about the variable of interest (Stevens, 1946)\\[8\\]. For example, the officials at a 100-m race could simply rank order the runners as they crossed the finish line (first, second, etc.), or they could time each runner to the nearest tenth of a second using a stopwatch (11.5 s, 12.1 s, etc.). In either case, they would be measuring the runners\u2019 times by systematically assigning scores to represent those times. But while the rank ordering procedure communicates the fact that the second-place runner took longer to finish than the first-place finisher, the stopwatch procedure also communicates\u00a0_how much_\u00a0longer the second-place finisher took. Stevens actually suggested four different [**levels\u00a0of\u00a0measurement**](https:\/\/kpu.pressbooks.pub\/psychmethods4e\/chapter\/understanding-psychological-measurement\/#term_54_963) (which he called \u201cscales of measurement\u201d) that correspond to four types of information that can be communicated by a set of scores, and the statistical procedures that can be used with the information.\u00a0\n\nThe\u00a0**nominal\u00a0level**\u00a0of measurement is used for categorical variables and involves assigning scores that are category labels. Category labels communicate whether any two individuals are the same or different in terms of the variable being measured. For example, if you ask your participants about their marital status, you are engaged in nominal-level measurement. Or if you ask your participants to indicate which of several ethnicities they identify themselves with, you are again engaged in nominal-level measurement. The essential point about nominal scales is that they do not imply any ordering among the responses. For example, when classifying people according to their favorite color, there is no sense in which green is placed \u201cahead of\u201d blue. Responses are merely categorized. Nominal scales thus embody the lowest level of measurement\\[9\\].\n\nThe remaining three levels of measurement are used for quantitative variables. The\u00a0[**ordinal\u00a0level**](https:\/\/kpu.pressbooks.pub\/psychmethods4e\/chapter\/understanding-psychological-measurement\/#term_54_965)\u00a0of measurement involves assigning scores so that they represent the rank order of the individuals. Ranks communicate not only whether any two individuals are the same or different in terms of the variable being measured but also whether one individual is higher or lower on that variable. For example, a researcher wishing to measure consumers\u2019 satisfaction with their microwave ovens might ask them to specify their feelings as either \u201cvery dissatisfied,\u201d \u201csomewhat dissatisfied,\u201d \u201csomewhat satisfied,\u201d or \u201cvery satisfied.\u201d The items in this scale are ordered, ranging from least to most satisfied. This is what distinguishes ordinal from nominal scales. Unlike nominal scales, ordinal scales allow comparisons of the degree to which two individuals rate the variable. For example, our satisfaction ordering makes it meaningful to assert that one person is more satisfied than another with their microwave ovens. Such an assertion reflects the first person\u2019s use of a verbal label that comes later in the list than the label chosen by the second person.\n\nOn the other hand, ordinal scales fail to capture important information that will be present in the other levels of measurement we examine. In particular, the difference between two levels of an ordinal scale cannot be assumed to be the same as the difference between two other levels (just like you cannot assume that the gap between the runners in first and second place is equal to the gap between the runners in second and third place). In our satisfaction scale, for example, the difference between the responses \u201cvery dissatisfied\u201d and \u201csomewhat dissatisfied\u201d is probably not equivalent to the difference between \u201csomewhat dissatisfied\u201d and \u201csomewhat satisfied.\u201d Nothing in our measurement procedure allows us to determine whether the two differences reflect the same difference in psychological satisfaction. Statisticians express this point by saying that the differences between adjacent scale values do not necessarily represent equal intervals on the underlying scale giving rise to the measurements. (In our case, the underlying scale is the true feeling of satisfaction, which we are trying to measure.)\n\nThe\u00a0**interval\u00a0level**\u00a0of measurement involves assigning scores using numerical scales in which intervals have the same interpretation throughout. As an example, consider either the Fahrenheit or Celsius temperature scales. The difference between 30 degrees and 40 degrees represents the same temperature difference as the difference between 80 degrees and 90 degrees. This is because each 10-degree interval has the same physical meaning (in terms of the kinetic energy of molecules).\n\nInterval scales are not perfect, however. In particular, they do not have a true zero point even if one of the scaled values happens to carry the name \u201czero.\u201d The Fahrenheit scale illustrates the issue. Zero degrees Fahrenheit does not represent the complete absence of temperature (the absence of any molecular kinetic energy). In reality, the label \u201czero\u201d is applied to its temperature for quite accidental reasons connected to the history of temperature measurement. Since an interval scale has no true zero point, it does not make sense to compute ratios of temperatures. For example, there is no sense in which the ratio of 40 to 20 degrees Fahrenheit is the same as the ratio of 100 to 50 degrees; no interesting physical property is preserved across the two ratios. After all, if the \u201czero\u201d label were applied at the temperature that Fahrenheit happens to label as 10 degrees, the two ratios would instead be 30 to 10 and 90 to 40, no longer the same! For this reason, it does not make sense to say that 80 degrees is \u201ctwice as hot\u201d as 40 degrees. Such a claim would depend on an arbitrary decision about where to \u201cstart\u201d the temperature scale, namely, what temperature to call zero (whereas the claim is intended to make a more fundamental assertion about the underlying physical reality).\n\nIn psychology, the intelligence quotient (IQ) is often considered to be measured at the interval level. While it is technically possible to receive a score of 0 on an IQ test, such a score would not indicate the complete absence of IQ. Moreover, a person with an IQ score of 140 does not have twice the IQ of a person with a score of 70. However, the difference between IQ scores of 80 and 100 is the same as the difference between IQ scores of 120 and 140.\n\nFinally, the\u00a0**ratio\u00a0level**\u00a0of measurement involves assigning scores in such a way that there is a true zero point that represents the complete absence of the quantity. Height measured in meters and weight measured in kilograms are good examples. So are counts of discrete objects or events such as the number of siblings one has or the number of questions a student answers correctly on an exam. You can think of a ratio scale as the three earlier scales rolled up in one. Like a nominal scale, it provides a name or category for each object (the numbers serve as labels). Like an ordinal scale, the objects are ordered (in terms of the ordering of the numbers). Like an interval scale, the same difference at two places on the scale has the same meaning. However, in addition, the same ratio at two places on the scale also carries the same meaning (see Table 4.1).\n\nThe Fahrenheit scale for temperature has an arbitrary zero point and is therefore not a ratio scale. However, zero on the Kelvin scale is absolute zero. This makes the Kelvin scale a ratio scale. For example, if one temperature is twice as high as another as measured on the Kelvin scale, then it has twice the kinetic energy of the other temperature.\n\nAnother example of a ratio scale is the amount of money you have in your pocket right now (25 cents, 50 cents, etc.). Money is measured on a ratio scale because, in addition to having the properties of an interval scale, it has a true zero point: if you have zero money, this actually implies the absence of money. Since money has a true zero point, it makes sense to say that someone with 50 cents has twice as much money as someone with 25 cents.\n\nStevens\u2019s levels of measurement are important for at least two reasons. First, they emphasize the generality of the concept of measurement. Although people do not normally think of categorizing or ranking individuals as measurement, in fact, they are as long as they are done so that they represent some characteristic of the individuals. Second, the levels of measurement can serve as a rough guide to the statistical procedures that can be used with the data and the conclusions that can be drawn from them. With nominal-level measurement, for example, the only available measure of central tendency is the mode. With ordinal-level measurement, the median or mode can be used as indicators of central tendency. Interval and ratio-level measurement are typically considered the most desirable because they permit for any indicators of central tendency to be computed (i.e., mean, median, or mode). Also, ratio-level measurement is the only level that allows meaningful statements about ratios of scores. Once again, one cannot say that someone with an IQ of 140 is twice as intelligent as someone with an IQ of 70 because IQ is measured at the interval level, but one can say that someone with six siblings has twice as many as someone with three because number of siblings is measured at the ratio level.\n\n|     |     |     |     |     |\n| --- | --- | --- | --- | --- |\n| **Level of Measurement** | **Category labels** | **Rank order** | **Equal intervals** | **True zero** |\n| **NOMINAL** | X   |     |     |     |\n| **ORDINAL** | X   | X   |     |     |\n| **INTERVAL** | X   | X   | X   |     |\n| **RATIO** | X   | X   | X   | X   |\n\n_**Table 4.1\u00a0Summary of Levels of Measurements**_\n\n1.  Costa, P. T., Jr., & McCrae, R. R. (1992). Normal personality assessment in clinical practice: The NEO Personality Inventory. _Psychological Assessment, 4_, 5\u201313.\n2.  Rosenberg, M. (1965). _Society and the adolescent self-image_. Princeton, NJ: Princeton University Press\n3.  Bandura, A., Ross, D., & Ross, S. A. (1961). Transmission of aggression through imitation of aggressive models. _Journal of Abnormal and Social Psychology, 63_, 575\u2013582.\n4.  Holmes, T. H., & Rahe, R. H. (1967). The Social Readjustment Rating Scale. _Journal of Psychosomatic Research, 11_(2), 213-218.\n5.  Delongis, A., Coyne, J. C., Dakof, G., Folkman, S., & Lazarus, R. S. (1982). Relationships of daily hassles, uplifts, and major life events to health status. _Health Psychology, 1_(2), 119-136.\n6.  Cohen, S., Kamarck, T., & Mermelstein, R. (1983). A global measure of perceived stress. _Journal of Health and Social Behavior, 24_, 386-396.\n7.  Segerstrom, S. E., & Miller, G. E. (2004). Psychological stress and the human immune system: A meta-analytic study of 30 years of inquiry. _Psychological Bulletin, 130_, 601\u2013630.\n8.  Stevens, S. S. (1946). On the theory of scales of measurement. _Science, 103_, 677\u2013680.\n9.  Levels of Measurement. Retrieved from http:\/\/wikieducator.org\/Introduction\\_to\\_Research\\_Methods\\_In\\_Psychology\/Theories\\_and\\_Measurement\/Levels\\_of_Measurement","text":"Learning Objectives\n\n1. Define measurement and give several examples of measurement in psychology.\n2. Explain what a psychological construct is and give several examples.\n3. Distinguish conceptual from operational definitions, give examples of each, and create simple operational definitions.\n4. Distinguish the four levels of measurement, give examples of each, and explain why this distinction is important.\u00a0\n\nMeasurement\u00a0is the assignment of scores to individuals so that the scores represent some characteristic of the individuals. This very general definition is consistent with the kinds of measurement that everyone is familiar with\u2014for example, weighing oneself by stepping onto a bathroom scale, or checking the internal temperature of a roasting turkey using a meat thermometer. It is also consistent with measurement in the other sciences. In physics, for example, one might measure the potential energy of an object in Earth\u2019s gravitational field by finding its mass and height (which of course requires measuring\u00a0those\u00a0variables) and then multiplying them together along with the gravitational acceleration of Earth (9.8 m\/s2). The result of this procedure is a score that represents the object\u2019s potential energy. \u00a0\u00a0\n\nThis general definition of measurement is consistent with measurement in psychology too. (Psychological measurement is often referred to as psychometrics.) Imagine, for example, that a cognitive psychologist wants to measure a person\u2019s working memory capacity\u2014their ability to hold in mind and think about several pieces of information all at the same time. To do this, she might use a backward digit span task, in which she reads a list of two digits to the person and asks them to repeat them in reverse order. She then repeats this several times, increasing the length of the list by one digit each time, until the person makes an error. The length of the longest list for which the person responds correctly is the score and represents their working memory capacity. Or imagine a clinical psychologist who is interested in how depressed a person is. He administers the Beck Depression Inventory, which is a 21-item self-report questionnaire in which the person rates the extent to which they have felt sad, lost energy, and experienced other symptoms of depression over the past 2 weeks. The sum of these 21 ratings is the score and represents the person\u2019s current level of depression.\n\nThe important point here is that measurement does not require any particular instruments or procedures. What it\u00a0does\u00a0require is\u00a0some\u00a0systematic procedure for assigning scores to individuals or objects so that those scores represent the characteristic of interest.\n\nMany variables studied by psychologists are straightforward and simple to measure. These include age, height, weight, and birth order. You can ask people how old they are and be reasonably sure that they know and will tell you. Although people might not know or want to tell you how much they weigh, you can have them step onto a bathroom scale. Other variables studied by psychologists\u2014perhaps the majority\u2014are not so straightforward or simple to measure. We cannot accurately assess people\u2019s level of intelligence by looking at them, and we certainly cannot put their self-esteem on a bathroom scale. These kinds of variables are called\u00a0constructs\u00a0(pronounced\u00a0CON-structs) and include personality traits (e.g., extraversion), emotional states (e.g., fear), attitudes (e.g., toward taxes), and abilities (e.g., athleticism).\u00a0\n\nPsychological constructs cannot be observed directly. One reason is that they often represent\u00a0tendencies\u00a0to think, feel, or act in certain ways. For example, to say that a particular university student is highly extraverted does not necessarily mean that she is behaving in an extraverted way right now. In fact, she might be sitting quietly by herself, reading a book. Instead, it means that she has a general tendency to behave in extraverted ways (e.g., being outgoing, enjoying social interactions) across a variety of situations. Another reason psychological constructs cannot be observed directly is that they often involve internal processes. Fear, for example, involves the activation of certain central and peripheral nervous system structures, along with certain kinds of thoughts, feelings, and behaviors\u2014none of which is necessarily obvious to an outside observer. Notice also that neither extraversion nor fear \u201creduces to\u201d any particular thought, feeling, act, or physiological structure or process. Instead, each is a kind of summary of a complex set of behaviors and internal processes.\n\nThe Big Five\n\nThe Big Five is a set of five broad dimensions that capture much of the variation in human personality. Each of the Big Five can even be defined in terms of six more specific constructs called \u201cfacets\u201d (Costa & McCrae, 1992)[1].\n\n\u00a0\n\nTable 4.1 The Big Five Personality Dimensions\n\n\u00a0\n\nThe\u00a0conceptual\u00a0definition\u00a0of a psychological construct describes the behaviors and internal processes that make up that construct, along with how it relates to other variables. For example, a conceptual definition of neuroticism (another one of the Big Five) would be that it is people\u2019s tendency to experience negative emotions such as anxiety, anger, and sadness across a variety of situations. This definition might also include that it has a strong genetic component, remains fairly stable over time, and is positively correlated with the tendency to experience pain and other physical symptoms.\n\nStudents sometimes wonder why, when researchers want to understand a construct like self-esteem or neuroticism, they do not simply look it up in the dictionary. One reason is that many scientific constructs do not have counterparts in everyday language (e.g., working memory capacity). More important, researchers are in the business of developing definitions that are more detailed and precise\u2014and that more accurately describe the way the world is\u2014than the informal definitions in the dictionary. As we will see, they do this by proposing conceptual definitions, testing them empirically, and revising them as necessary. Sometimes they throw them out altogether. This is why the research literature often includes different conceptual definitions of the same construct. In some cases, an older conceptual definition has been replaced by a newer one that fits and works better. In others, researchers are still in the process of deciding which of various conceptual definitions is the best.\n\nAn\u00a0operational\u00a0definition\u00a0is a definition of a variable in terms of precisely how it is to be measured. These measures generally fall into one of three broad categories.\u00a0Self-report\u00a0measures\u00a0are those in which participants report on their own thoughts, feelings, and actions, as with the Rosenberg Self-Esteem Scale (Rosenberg, 1965)[2]. Behavioral measures\u00a0are those in which some other aspect of participants\u2019 behavior is observed and recorded. This is an extremely broad category that includes the observation of people\u2019s behavior both in highly structured laboratory tasks and in more natural settings. A good example of the former would be measuring working memory capacity using the backward digit span task. A good example of the latter is a famous operational definition of physical aggression from researcher Albert Bandura and his colleagues (Bandura, Ross, & Ross, 1961)[3]. They let each of several children play for 20 minutes in a room that contained a clown-shaped punching bag called a Bobo doll. They filmed each child and counted the number of acts of physical aggression the child committed. These included hitting the doll with a mallet, punching it, and kicking it. Their operational definition, then, was the number of these specifically defined acts that the child committed during the 20-minute period. Finally,\u00a0physiological\u00a0measures\u00a0are those that involve recording any of a wide variety of physiological processes, including heart rate and blood pressure, galvanic skin response, hormone levels, and electrical activity and blood flow in the brain.\u00a0\n\nFor any given variable or construct, there will be multiple operational definitions. Stress is a good example. A rough conceptual definition is that stress is an adaptive response to a perceived danger or threat that involves physiological, cognitive, affective, and behavioral components. But researchers have operationally defined it in several ways. The Social Readjustment Rating Scale (Holmes & Rahe, 1967)[4] is a self-report questionnaire on which people identify stressful events that they have experienced in the past year and assigns points for each one depending on its severity. For example, a man who has been divorced (73 points), changed jobs (36 points), and had a change in sleeping habits (16 points) in the past year would have a total score of 125. The Hassles and Uplifts Scale (Delongis, Coyne, Dakof, Folkman & Lazarus, 1982) [5]\u00a0is similar but focuses on everyday stressors like misplacing things and being concerned about one\u2019s weight. The Perceived Stress Scale (Cohen, Kamarck, & Mermelstein, 1983) [6] is another self-report measure that focuses on people\u2019s feelings of stress (e.g., \u201cHow often have you felt nervous and stressed?\u201d). Researchers have also operationally defined stress in terms of several physiological variables including blood pressure and levels of the stress hormone cortisol.\n\nWhen psychologists use multiple operational definitions of the same construct\u2014either within a study or across studies\u2014they are using converging\u00a0operations. The idea is that the various operational definitions are \u201cconverging\u201d or coming together on the same construct. When scores based on several different operational definitions are closely related to each other and produce similar patterns of results, this constitutes good evidence that the construct is being measured effectively and that it is useful. The various measures of stress, for example, are all correlated with each other and have all been shown to be correlated with other variables such as immune system functioning (also measured in a variety of ways) (Segerstrom & Miller, 2004)[7]. This is what allows researchers eventually to draw useful general conclusions, such as \u201cstress is negatively correlated with immune system functioning,\u201d as opposed to more specific and less useful ones, such as \u201cpeople\u2019s scores on the Perceived Stress Scale are negatively correlated with their white blood counts.\u201d\n\nThe psychologist S. S. Stevens suggested that scores can be assigned to individuals in a way that communicates more or less quantitative information about the variable of interest (Stevens, 1946)[8]. For example, the officials at a 100-m race could simply rank order the runners as they crossed the finish line (first, second, etc.), or they could time each runner to the nearest tenth of a second using a stopwatch (11.5 s, 12.1 s, etc.). In either case, they would be measuring the runners\u2019 times by systematically assigning scores to represent those times. But while the rank ordering procedure communicates the fact that the second-place runner took longer to finish than the first-place finisher, the stopwatch procedure also communicates\u00a0how much\u00a0longer the second-place finisher took. Stevens actually suggested four different levels\u00a0of\u00a0measurement (which he called \u201cscales of measurement\u201d) that correspond to four types of information that can be communicated by a set of scores, and the statistical procedures that can be used with the information.\u00a0\n\nThe\u00a0nominal\u00a0level\u00a0of measurement is used for categorical variables and involves assigning scores that are category labels. Category labels communicate whether any two individuals are the same or different in terms of the variable being measured. For example, if you ask your participants about their marital status, you are engaged in nominal-level measurement. Or if you ask your participants to indicate which of several ethnicities they identify themselves with, you are again engaged in nominal-level measurement. The essential point about nominal scales is that they do not imply any ordering among the responses. For example, when classifying people according to their favorite color, there is no sense in which green is placed \u201cahead of\u201d blue. Responses are merely categorized. Nominal scales thus embody the lowest level of measurement[9].\n\nThe remaining three levels of measurement are used for quantitative variables. The\u00a0ordinal\u00a0level\u00a0of measurement involves assigning scores so that they represent the rank order of the individuals. Ranks communicate not only whether any two individuals are the same or different in terms of the variable being measured but also whether one individual is higher or lower on that variable. For example, a researcher wishing to measure consumers\u2019 satisfaction with their microwave ovens might ask them to specify their feelings as either \u201cvery dissatisfied,\u201d \u201csomewhat dissatisfied,\u201d \u201csomewhat satisfied,\u201d or \u201cvery satisfied.\u201d The items in this scale are ordered, ranging from least to most satisfied. This is what distinguishes ordinal from nominal scales. Unlike nominal scales, ordinal scales allow comparisons of the degree to which two individuals rate the variable. For example, our satisfaction ordering makes it meaningful to assert that one person is more satisfied than another with their microwave ovens. Such an assertion reflects the first person\u2019s use of a verbal label that comes later in the list than the label chosen by the second person.\n\nOn the other hand, ordinal scales fail to capture important information that will be present in the other levels of measurement we examine. In particular, the difference between two levels of an ordinal scale cannot be assumed to be the same as the difference between two other levels (just like you cannot assume that the gap between the runners in first and second place is equal to the gap between the runners in second and third place). In our satisfaction scale, for example, the difference between the responses \u201cvery dissatisfied\u201d and \u201csomewhat dissatisfied\u201d is probably not equivalent to the difference between \u201csomewhat dissatisfied\u201d and \u201csomewhat satisfied.\u201d Nothing in our measurement procedure allows us to determine whether the two differences reflect the same difference in psychological satisfaction. Statisticians express this point by saying that the differences between adjacent scale values do not necessarily represent equal intervals on the underlying scale giving rise to the measurements. (In our case, the underlying scale is the true feeling of satisfaction, which we are trying to measure.)\n\nThe\u00a0interval\u00a0level\u00a0of measurement involves assigning scores using numerical scales in which intervals have the same interpretation throughout. As an example, consider either the Fahrenheit or Celsius temperature scales. The difference between 30 degrees and 40 degrees represents the same temperature difference as the difference between 80 degrees and 90 degrees. This is because each 10-degree interval has the same physical meaning (in terms of the kinetic energy of molecules).\n\nInterval scales are not perfect, however. In particular, they do not have a true zero point even if one of the scaled values happens to carry the name \u201czero.\u201d The Fahrenheit scale illustrates the issue. Zero degrees Fahrenheit does not represent the complete absence of temperature (the absence of any molecular kinetic energy). In reality, the label \u201czero\u201d is applied to its temperature for quite accidental reasons connected to the history of temperature measurement. Since an interval scale has no true zero point, it does not make sense to compute ratios of temperatures. For example, there is no sense in which the ratio of 40 to 20 degrees Fahrenheit is the same as the ratio of 100 to 50 degrees; no interesting physical property is preserved across the two ratios. After all, if the \u201czero\u201d label were applied at the temperature that Fahrenheit happens to label as 10 degrees, the two ratios would instead be 30 to 10 and 90 to 40, no longer the same! For this reason, it does not make sense to say that 80 degrees is \u201ctwice as hot\u201d as 40 degrees. Such a claim would depend on an arbitrary decision about where to \u201cstart\u201d the temperature scale, namely, what temperature to call zero (whereas the claim is intended to make a more fundamental assertion about the underlying physical reality).\n\nIn psychology, the intelligence quotient (IQ) is often considered to be measured at the interval level. While it is technically possible to receive a score of 0 on an IQ test, such a score would not indicate the complete absence of IQ. Moreover, a person with an IQ score of 140 does not have twice the IQ of a person with a score of 70. However, the difference between IQ scores of 80 and 100 is the same as the difference between IQ scores of 120 and 140.\n\nFinally, the\u00a0ratio\u00a0level\u00a0of measurement involves assigning scores in such a way that there is a true zero point that represents the complete absence of the quantity. Height measured in meters and weight measured in kilograms are good examples. So are counts of discrete objects or events such as the number of siblings one has or the number of questions a student answers correctly on an exam. You can think of a ratio scale as the three earlier scales rolled up in one. Like a nominal scale, it provides a name or category for each object (the numbers serve as labels). Like an ordinal scale, the objects are ordered (in terms of the ordering of the numbers). Like an interval scale, the same difference at two places on the scale has the same meaning. However, in addition, the same ratio at two places on the scale also carries the same meaning (see Table 4.1).\n\nThe Fahrenheit scale for temperature has an arbitrary zero point and is therefore not a ratio scale. However, zero on the Kelvin scale is absolute zero. This makes the Kelvin scale a ratio scale. For example, if one temperature is twice as high as another as measured on the Kelvin scale, then it has twice the kinetic energy of the other temperature.\n\nAnother example of a ratio scale is the amount of money you have in your pocket right now (25 cents, 50 cents, etc.). Money is measured on a ratio scale because, in addition to having the properties of an interval scale, it has a true zero point: if you have zero money, this actually implies the absence of money. Since money has a true zero point, it makes sense to say that someone with 50 cents has twice as much money as someone with 25 cents.\n\nStevens\u2019s levels of measurement are important for at least two reasons. First, they emphasize the generality of the concept of measurement. Although people do not normally think of categorizing or ranking individuals as measurement, in fact, they are as long as they are done so that they represent some characteristic of the individuals. Second, the levels of measurement can serve as a rough guide to the statistical procedures that can be used with the data and the conclusions that can be drawn from them. With nominal-level measurement, for example, the only available measure of central tendency is the mode. With ordinal-level measurement, the median or mode can be used as indicators of central tendency. Interval and ratio-level measurement are typically considered the most desirable because they permit for any indicators of central tendency to be computed (i.e., mean, median, or mode). Also, ratio-level measurement is the only level that allows meaningful statements about ratios of scores. Once again, one cannot say that someone with an IQ of 140 is twice as intelligent as someone with an IQ of 70 because IQ is measured at the interval level, but one can say that someone with six siblings has twice as many as someone with three because number of siblings is measured at the ratio level.\n\nTable 4.1\u00a0Summary of Levels of Measurements\n\n\u00a0\n\nCosta, P. T., Jr., & McCrae, R. R. (1992). Normal personality assessment in clinical practice: The NEO Personality Inventory. Psychological Assessment, 4, 5\u201313.Rosenberg, M. (1965). Society and the adolescent self-image. Princeton, NJ: Princeton University PressBandura, A., Ross, D., & Ross, S. A. (1961). Transmission of aggression through imitation of aggressive models. Journal of Abnormal and Social Psychology, 63, 575\u2013582.Holmes, T. H., & Rahe, R. H. (1967). The Social Readjustment Rating Scale. Journal of Psychosomatic Research, 11(2), 213-218.Delongis, A., Coyne, J. C., Dakof, G., Folkman, S., & Lazarus, R. S. (1982). Relationships of daily hassles, uplifts, and major life events to health status. Health Psychology, 1(2), 119-136.Cohen, S., Kamarck, T., & Mermelstein, R. (1983). A global measure of perceived stress. Journal of Health and Social Behavior, 24, 386-396.Segerstrom, S. E., & Miller, G. E. (2004). Psychological stress and the human immune system: A meta-analytic study of 30 years of inquiry. Psychological Bulletin, 130, 601\u2013630.Stevens, S. S. (1946). On the theory of scales of measurement. Science, 103, 677\u2013680. Levels of Measurement. Retrieved from http:\/\/wikieducator.org\/Introduction_to_Research_Methods_In_Psychology\/Theories_and_Measurement\/Levels_of_Measurement\u00a0","contextuality":{"text":"The text delves into the concept of measurement, highlighting its application across different fields, including psychology, where it is often referred to as psychometrics. Measurement involves _________ scores to individuals to represent certain characteristics, which can be straightforward, like age or weight, or more complex, like psychological constructs such as extraversion or intelligence. Constructs, not directly observable, are measured through systematic __________ and operational definitions, often involving self-report, behavioral, or _____________ measures. The text also discusses Stevens's four ______ of measurement\u2014nominal, ordinal, interval, and ratio\u2014that determine how scores are ________ and interpreted. Each level communicates varying degrees of quantitative information, guiding the ___________ procedures that can be used with the ____. These levels help in making meaningful comparisons and drawing ___________ from the measured data, emphasizing the __________ of a true zero in ratio scales for ______ ratio-based statements.","gaps":[["assigning",194,9],["procedures",468,10],["physiological",552,13],["levels",615,6],["assigned",705,8],["statistical",812,11],["data",861,4],["conclusions",930,11],["importance",982,10],["making",1028,6]]},"contextuality_plus":{"text":"The text delves into the concept of measurement, highlighting its application across different fields, including psychology, where it is often referred to as psychometrics. ___________ involves assigning scores to individuals to represent certain _______________, which can be straightforward, like age or weight, or more complex, like _____________ constructs such as extraversion or intelligence. Constructs, not directly observable, are measured through systematic __________ and operational definitions, often involving self-report, behavioral, or _____________ measures. The text also discusses Stevens's four levels of ___________\u2014nominal, ordinal, interval, and ratio\u2014that determine how scores are ________ and interpreted. Each level communicates varying degrees of quantitative information, guiding the ___________ procedures that can be used with the data. These levels help in making meaningful comparisons and drawing ___________ from the measured data, emphasizing the __________ of a true zero in ratio scales for making ratio-based statements.","gaps":[["Measurement",173,11],["characteristics",247,15],["psychological",336,13],["procedures",468,10],["physiological",552,13],["measurement",625,11],["assigned",705,8],["statistical",812,11],["conclusions",930,11],["importance",982,10]]},"keyword":{"text":"The text delves into the concept of measurement, highlighting its application across different fields, including psychology, where it is often referred to as psychometrics. Measurement involves assigning scores to individuals to represent certain characteristics, which can be straightforward, like age or weight, or more complex, like psychological constructs such as extraversion or intelligence. Constructs, not directly observable, are measured through systematic procedures and operational definitions, often involving self-report, behavioral, or physiological measures. The text also discusses Stevens's four levels of ___________\u2014nominal, ordinal, interval, and ratio\u2014that determine how scores are assigned and interpreted. Each level communicates varying degrees of quantitative information, guiding the statistical procedures that can be used with the data. These levels help in making meaningful comparisons and drawing conclusions from the measured data, emphasizing the importance of a true zero in ratio scales for making ratio-based statements.","gaps":[["measurement",625,11]]}}
{"volume":"research-methods-in-psychology-demo","page":"1-methods-of-knowing-1","summary":"The text discusses various methods of acquiring knowledge, including intuition, authority, rationalism, empiricism, and the scientific method. Intuition relies on gut feelings and instincts, which can be flawed due to cognitive biases. Authority involves accepting ideas from figures like parents or the media, though history shows such sources can be misleading. Rationalism uses logic to draw conclusions, but incorrect premises can lead to false results. Empiricism is based on observation and experience, yet can be deceptive due to sensory limitations. The scientific method, combining rationalism and structured observations, is the most reliable for producing valid knowledge, though it has limitations such as resource intensity and applicability only to empirical questions.","markdown":"<i-callout variant=\"info\" title=\"Learning Objectives\">\n\n1\\. Describe the 5 methods of acquiring knowledge.\u00a0\n\n2\\. Understand the benefits and problems with each.\n\n<\/i-callout>\n\nTake a minute to ponder some of what you know and how you acquired that knowledge. Perhaps you know that you should make your bed in the morning because your mother or father told you this is what you should do, perhaps you know that swans are white because all of the swans you have seen are white, or perhaps you know that your friend is lying to you because she is acting strange and won\u2019t look you in the eye. But should we trust knowledge from these sources? The methods of acquiring knowledge can be broken down into five categories each with its own strengths and weaknesses.\n\n### Intuition\n\nThe first method of knowing is intuition. When we use our intuition, we are relying on our guts, our emotions, and\/or our instincts to guide us. Rather than examining facts or using rational thought, intuition involves believing what feels true. The problem with relying on intuition is that our intuitions can be wrong because they are driven by cognitive and motivational biases rather than logical reasoning or scientific evidence. While the strange behavior of your friend may lead you to think s\/he is lying to you it may just be that s\/he is holding in a bit of gas or is preoccupied with some other issue that is irrelevant to you. However, weighing alternatives and thinking of all the different possibilities can be paralyzing for some people and sometimes decisions based on intuition are actually superior to those based on analysis (people interested in this idea should read Malcolm Gladwell\u2019s book Blink). \u00a0\n\n### Authority\n\nPerhaps one of the most common methods of acquiring knowledge is through authority. This method involves accepting new ideas because some authority figure states that they are true. These authorities include parents, the media, doctors, Priests and other religious authorities, the government, and professors. While in an ideal world we should be able to trust authority figures, history has taught us otherwise and many instances of atrocities against humanity are a consequence of people unquestioningly following authority (e.g., Salem Witch Trials, Nazi War Crimes). On a more benign level, while your parents may have told you that you should make your bed in the morning, making your bed provides the warm damp environment in which mites thrive. Keeping the sheets open provides a less hospitable environment for mites. These examples illustrate that the problem with using authority to obtain knowledge is that they may be wrong, they may just be using their intuition to arrive at their conclusions, and they may have their own reasons to mislead you. Nevertheless, much of the information we acquire is through authority because we don\u2019t have time to question and independently research every piece of knowledge we learn through authority. But we can learn to evaluate the credentials of authority figures, to evaluate the methods they used to arrive at their conclusions, and evaluate whether they have any reasons to mislead us.\n\n### Rationalism\n\nRationalism involves using logic and reasoning to acquire new knowledge. Using this method premises are stated and logical rules are followed to arrive at sound conclusions. For instance, if I am given the premise that all swans are white and the premise that this is a swan then I can come to the rational conclusion that this swan is white without actually seeing the swan. The problem with this method is that if the premises are wrong or there is an error in logic then the conclusion will not be valid. For instance, the premise that all swans are white is incorrect; there are black swans in Australia. Also, unless formally trained in the rules of logic it is easy to make an error. Nevertheless, if the premises are correct and logical rules are followed appropriately then this is sound means of acquiring knowledge.\u00a0\n\n### Empiricism\n\nEmpiricism involves acquiring knowledge through observation and experience. Once again many of you may have believed that all swans are white because you have only ever seen white swans. For centuries people believed the world is flat because it appears to be flat. These examples and the many visual illusions that trick our senses illustrate the problems with relying on empiricism alone to derive knowledge. We are limited in what we can experience and observe and our senses can deceive us. Moreover, our prior experiences can alter the way we perceive events. Nevertheless, empiricism is at the heart of the scientific method. Science relies on observations. But not just any observations, science relies on structured observations which is known as systematic empiricism.\n\n### The Scientific Method\n\nThe scientific method is a process of systematically collecting and evaluating evidence to test ideas and answer questions. While scientists may use intuition, authority, rationalism, and empiricism to generate new ideas they don\u2019t stop there. Scientists go a step further by using systematic empiricism to make careful observations under various controlled conditions in order to test their ideas and they use rationalism to arrive at valid conclusions. While the scientific method is the most likely of all of the methods to produce valid knowledge, like all methods of acquiring knowledge it also has its drawbacks. One major problem is that it is not always feasible to use the scientific method; this method can require considerable time and resources. Another problem with the scientific method is that it cannot be used to answer all questions. As described in the following section, the scientific method can only be used to address empirical questions. This book and your research methods course are designed to provide you with an in-depth examination of how psychologists use the scientific method to advance our understanding of human behavior and the mind.\n\n1.  Gladwell, M. E. (2005). _Blink: The power of thinking without thinking_. (9th ed.). New York: Little, Brown & Co.","text":"Learning Objectives\n\n1. Describe the 5 methods of acquiring knowledge.\u00a0\n2. Understand the benefits and problems with each.\u00a0\n\nTake a minute to ponder some of what you know and how you acquired that knowledge. Perhaps you know that you should make your bed in the morning because your mother or father told you this is what you should do, perhaps you know that swans are white because all of the swans you have seen are white, or perhaps you know that your friend is lying to you because she is acting strange and won\u2019t look you in the eye. But should we trust knowledge from these sources? The methods of acquiring knowledge can be broken down into five categories each with its own strengths and weaknesses.\u00a0\n\nIntuition\n\nThe first method of knowing is intuition. When we use our intuition, we are relying on our guts, our emotions, and\/or our instincts to guide us. Rather than examining facts or using rational thought, intuition involves believing what feels true. The problem with relying on intuition is that our intuitions can be wrong because they are driven by cognitive and motivational biases rather than logical reasoning or scientific evidence. While the strange behavior of your friend may lead you to think s\/he is lying to you it may just be that s\/he is holding in a bit of gas or is preoccupied with some other issue that is irrelevant to you. However, weighing alternatives and thinking of all the different possibilities can be paralyzing for some people and sometimes decisions based on intuition are actually superior to those based on analysis (people interested in this idea should read Malcolm Gladwell\u2019s book Blink). \u00a0\n\nAuthority\n\nPerhaps one of the most common methods of acquiring knowledge is through authority. This method involves accepting new ideas because some authority figure states that they are true. These authorities include parents, the media, doctors, Priests and other religious authorities, the government, and professors. While in an ideal world we should be able to trust authority figures, history has taught us otherwise and many instances of atrocities against humanity are a consequence of people unquestioningly following authority (e.g., Salem Witch Trials, Nazi War Crimes). On a more benign level, while your parents may have told you that you should make your bed in the morning, making your bed provides the warm damp environment in which mites thrive. Keeping the sheets open provides a less hospitable environment for mites. These examples illustrate that the problem with using authority to obtain knowledge is that they may be wrong, they may just be using their intuition to arrive at their conclusions, and they may have their own reasons to mislead you. Nevertheless, much of the information we acquire is through authority because we don\u2019t have time to question and independently research every piece of knowledge we learn through authority. But we can learn to evaluate the credentials of authority figures, to evaluate the methods they used to arrive at their conclusions, and evaluate whether they have any reasons to mislead us.\u00a0\n\nRationalism\n\nRationalism involves using logic and reasoning to acquire new knowledge. Using this method premises are stated and logical rules are followed to arrive at sound conclusions. For instance, if I am given the premise that all swans are white and the premise that this is a swan then I can come to the rational conclusion that this swan is white without actually seeing the swan. The problem with this method is that if the premises are wrong or there is an error in logic then the conclusion will not be valid. For instance, the premise that all swans are white is incorrect; there are black swans in Australia. Also, unless formally trained in the rules of logic it is easy to make an error. Nevertheless, if the premises are correct and logical rules are followed appropriately then this is sound means of acquiring knowledge.\u00a0\n\nEmpiricism\n\nEmpiricism involves acquiring knowledge through observation and experience. Once again many of you may have believed that all swans are white because you have only ever seen white swans. For centuries people believed the world is flat because it appears to be flat. These examples and the many visual illusions that trick our senses illustrate the problems with relying on empiricism alone to derive knowledge. We are limited in what we can experience and observe and our senses can deceive us. Moreover, our prior experiences can alter the way we perceive events. Nevertheless, empiricism is at the heart of the scientific method. Science relies on observations. But not just any observations, science relies on structured observations which is known as systematic empiricism.\n\nThe Scientific Method\n\nThe scientific method is a process of systematically collecting and evaluating evidence to test ideas and answer questions. While scientists may use intuition, authority, rationalism, and empiricism to generate new ideas they don\u2019t stop there. Scientists go a step further by using systematic empiricism to make careful observations under various controlled conditions in order to test their ideas and they use rationalism to arrive at valid conclusions. While the scientific method is the most likely of all of the methods to produce valid knowledge, like all methods of acquiring knowledge it also has its drawbacks. One major problem is that it is not always feasible to use the scientific method; this method can require considerable time and resources. Another problem with the scientific method is that it cannot be used to answer all questions. As described in the following section, the scientific method can only be used to address empirical questions. This book and your research methods course are designed to provide you with an in-depth examination of how psychologists use the scientific method to advance our understanding of human behavior and the mind.\n\nGladwell, M. E. (2005). Blink: The power of thinking without thinking. (9th ed.). New York: Little, Brown & Co. \u00a0","contextuality":{"text":"The text discusses various methods of acquiring knowledge, including intuition, authority, rationalism, empiricism, and the scientific method. Intuition relies on gut feelings and _________, which can be flawed due to cognitive biases. Authority ________ accepting ideas from figures like parents or the media, though history _____ such sources can be misleading. Rationalism uses logic to draw conclusions, but _________ premises can lead to false results. Empiricism is _____ on observation and experience, yet can be deceptive due to sensory ___________. The scientific method, combining rationalism and structured ____________, is the most reliable for producing valid knowledge, though it has ___________ such as resource intensity and applicability only to empirical _________.","gaps":[["instincts",180,9],["involves",246,8],["shows",326,5],["incorrect",412,9],["based",472,5],["limitations",545,11],["observations",618,12],["limitations",698,11],["questions",773,9]]},"contextuality_plus":{"text":"The text discusses various methods of acquiring knowledge, including intuition, authority, rationalism, empiricism, and the scientific method. Intuition relies on gut feelings and _________, which can be flawed due to cognitive biases. Authority ________ accepting ideas from figures like parents or the media, though history _____ such sources can be misleading. Rationalism uses logic to draw conclusions, but _________ premises can lead to false results. Empiricism is _____ on observation and experience, yet can be deceptive due to sensory ___________. The scientific method, combining rationalism and structured ____________, is the most reliable for producing valid knowledge, though it has ___________ such as resource intensity and applicability only to _________ questions.","gaps":[["instincts",180,9],["involves",246,8],["shows",326,5],["incorrect",412,9],["based",472,5],["limitations",545,11],["observations",618,12],["limitations",698,11],["empirical",763,9]]},"keyword":{"text":"The text discusses various methods of acquiring knowledge, including intuition, authority, rationalism, empiricism, and the scientific method. Intuition relies on gut feelings and instincts, which can be flawed due to cognitive biases. Authority involves accepting ideas from figures like parents or the media, though history shows such sources can be misleading. Rationalism uses logic to draw conclusions, but incorrect premises can lead to false results. Empiricism is based on observation and experience, yet can be deceptive due to sensory limitations. The __________ method, combining rationalism and structured observations, is the most reliable for producing valid _________, though it has limitations such as resource intensity and applicability only to empirical questions.","gaps":[["knowledge",673,9],["scientific",562,10]]}}
{"volume":"formal-operator-training-guidelines","page":"6-requalification-absence-from-job","summary":"The STL is tasked with ensuring that employees maintain current job qualifications, using a structured process outlined in a re-qualification matrix, which depends on the duration of an employee's absence from their role. The Board of Review, known as KR-100, assesses and determines the specific re-training requirements needed to re-establish qualifications. Re-training can vary significantly; employees absent for short periods might require minimal refreshment, while those away for over 18 months may need more comprehensive training. Essential components of re-training include reviewing procedural updates, shadowing qualified operators, solo work, completing assessments, and attending job-specific training. Upon fulfilling these requirements, employees are deemed re-qualified, with all documentation sent to the LMS Specialist for record-keeping.","markdown":"The STL is responsible for ensuring direct reports job qualifications remain current based on previously defined requirements.\n\nThe matrix below is provided for specific re-qualification requirements and is based on length of absence from the job. A Board of Review, KR-100, is required to determine details of re-qualification requirements. Training required to re-establish qualification may vary. For example, an operator is placed on special assignment for 18 months and the assignment is within their assigned business unit, then a minimum amount of re-training may be necessary. On the contrary, an operator who has been physically absent from their assigned business unit for more than 18 months may require more in-depth re-training. Some examples of re-training below can stand alone or be combined with others to ensure competency is achieved:\u00a0\n\n* Review all MOCs and procedure updates occurred during absence **(REQUIRED)**\n* Shadow Training with a qualified Operator for a specified period\n* Solo on the job for a specified period\n* Complete all or a portion of the various checklists, skill assessments, or exams\n* Attend Job School\n\nUpon completion of requirements determined by the Board of Review, the employee will be considered re-qualified and allowed to resume job duties. All documentation related to each job re-qualification, including a KR-982, is forwarded to the LMS Specialist in L&D for electronic credit and is then filed in Central Records.\n\n| Position | Reason for Loss | Re-qualification Requirements |\n| --- | --- | --- |\n| Trainees | Does not work a job for two 12-hour shifts within 12 months | To be determined by the trainee\u2019s STL, and the Operations O&M Training Team Lead. |\n| Trainees | Absent from work greater than 6 months | BOR includes requalifying employee, the ASH, STL and O&M Training Team Leader to determine requirements. |\n| FQOs, COs, HOs | Absence from work greater than 12 months | BOR includes requalifying employee, the ASH, STL and O&M Training Team Leader to determine requirements. |\n\nTable 8: Requalification Matrix","text":"The STL is responsible for ensuring direct reports job qualifications remain current based on previously defined requirements.\n\nThe matrix below is provided for specific re-qualification requirements and is based on length of absence from the job. A Board of Review, KR-100, is required to determine details of re-qualification requirements. Training required to re-establish qualification may vary. For example, an operator is placed on special assignment for 18 months and the assignment is within their assigned business unit, then a minimum amount of re-training may be necessary. On the contrary, an operator who has been physically absent from their assigned business unit for more than 18 months may require more in-depth re-training. Some examples of re-training below can stand alone or be combined with others to ensure competency is achieved:\u00a0\n\nReview all MOCs and procedure updates occurred during absence (REQUIRED)Shadow Training with a qualified Operator for a specified periodSolo on the job for a specified periodComplete all or a portion of the various checklists, skill assessments, or examsAttend Job School\n\nUpon completion of requirements determined by the Board of Review, the employee will be considered re-qualified and allowed to resume job duties. All documentation related to each job re-qualification, including a KR-982, is forwarded to the LMS Specialist in L&D for electronic credit and is then filed in Central Records.\n\nTable 8: Requalification Matrix","contextuality":{"text":"The STL is tasked with ensuring that employees maintain current job qualifications, using a structured process outlined in a re-qualification matrix, which depends on the duration of an employee's absence from their role. The Board of Review, known as KR-100, assesses and __________ the specific re-training requirements needed to re-_________ qualifications. Re-training can vary _____________; employees absent for short periods might _______ minimal refreshment, while those away for over 18 months may ____ more comprehensive training. Essential components of re-training include reviewing __________ updates, shadowing qualified operators, solo work, completing assessments, and attending ___-specific training. Upon fulfilling these ____________, employees are deemed re-qualified, with all documentation ____ to the LMS Specialist for record-_______.","gaps":[["determines",273,10],["establish",335,9],["significantly",382,13],["require",438,7],["need",507,4],["procedural",595,10],["job",695,3],["requirements",740,12],["sent",812,4],["keeping",850,7]]},"contextuality_plus":{"text":"The STL is tasked with ensuring that employees maintain current job qualifications, using a structured process outlined in a re-qualification matrix, which depends on the duration of an employee's absence from their role. The Board of Review, known as KR-100, assesses and __________ the specific re-training requirements needed to re-establish ______________. Re-training can vary significantly; _________ absent for short periods might require minimal refreshment, while those ____ for over 18 months may need more _____________ training. Essential components of re-training include reviewing __________ updates, shadowing qualified operators, solo work, __________ assessments, and attending job-specific training. Upon fulfilling these requirements, _________ are deemed re-qualified, with all documentation ____ to the LMS Specialist for record-_______.","gaps":[["determines",273,10],["qualifications",345,14],["employees",397,9],["away",479,4],["comprehensive",517,13],["procedural",595,10],["completing",657,10],["employees",754,9],["sent",812,4],["keeping",850,7]]},"keyword":{"text":"The STL is tasked with ensuring that employees maintain current job qualifications, using a structured process outlined in a re-qualification matrix, which depends on the duration of an employee's absence from their role. The Board of Review, known as KR-100, assesses and determines the specific ___________ requirements needed to re-establish ______________. Re-training can vary significantly; _________ absent for short periods might require minimal refreshment, while those away for over 18 months may need more comprehensive training. Essential components of ___________ include reviewing procedural updates, shadowing qualified operators, solo work, completing assessments, and attending job-specific training. Upon fulfilling these requirements, _________ are deemed re-qualified, with all documentation sent to the LMS Specialist for record-keeping.","gaps":[["employees",397,9],["employees",754,9],["re-training",297,11],["re-training",565,11],["qualifications",345,14]]}}
{"volume":"research-methods-in-psychology","page":"20-reliability-and-validity-of-measurement","summary":"Reliability and validity are key concepts in psychological measurement. Reliability refers to the consistency of a measure, encompassing test-retest reliability, internal consistency, and inter-rater reliability. Test-retest reliability assesses the consistency of scores over time, crucial for constructs assumed stable, like intelligence. Internal consistency evaluates the correlation of responses across items within a measure, often using Cronbach\u2019s \u03b1. Inter-rater reliability involves consistency across different observers\u2019 judgments. Validity concerns whether a measure accurately represents the intended variable, with types including face validity, content validity, and criterion validity. Criterion validity examines correlations with expected variables, while discriminant validity ensures scores are not correlated with distinct constructs. These concepts collectively ensure measures are both consistent and meaningful.","markdown":"<i-callout variant=\"info\" title=\"Learning Objectives\">\n\n1\\. Define reliability, including the different types and how they are assessed.\n\n2\\. Define validity, including the different types and how they are assessed.\n\n3\\. Describe the kinds of evidence that would be relevant to assessing the reliability and validity of a particular measure.\n\n<\/i-callout>\n\nAgain, measurement involves assigning scores to individuals so that they represent some characteristic of the individuals. But how do researchers know that the scores actually represent the characteristic, especially when it is a construct like intelligence, self-esteem, depression, or working memory capacity? The answer is that they conduct research using the measure to confirm that the scores make sense based on their understanding of the construct being measured. This is an extremely important point. Psychologists do not simply\u00a0_assume_\u00a0that their measures work. Instead, they collect data to _demonstrate_\u00a0that they work. If their research does not demonstrate that a measure works, they stop using it.\u00a0\n\nAs an informal example, imagine that you have been dieting for a month. Your clothes seem to be fitting more loosely, and several friends have asked if you have lost weight. If at this point your bathroom scale indicated that you had lost 10 pounds, this would make sense and you would continue to use the scale. But if it indicated that you had gained 10 pounds, you would rightly conclude that it was broken and either fix it or get rid of it. In evaluating a measurement method, psychologists consider two general dimensions: reliability and validity.\n\n**Reliability**\u00a0refers to the consistency of a measure. Psychologists consider three types of consistency: over time (test-retest reliability), across items (internal consistency), and across different researchers (inter-rater reliability).\u00a0\n\n### Test-Retest Reliability\n\nWhen researchers measure a construct that they assume to be consistent across time, then the scores they obtain should also be consistent across time.\u00a0**Test-retest\u00a0reliability**\u00a0is the extent to which this is actually the case. For example, intelligence is generally thought to be consistent across time. A person who is highly intelligent today will be highly intelligent next week. This means that any good measure of intelligence should produce roughly the same scores for this individual next week as it does today. Clearly, a measure that produces highly inconsistent scores over time cannot be a very good measure of a construct that is supposed to be consistent.\n\nAssessing test-retest reliability requires using the measure on a group of people at one time, using it again on the\u00a0_same_ group of people at a later time, and then looking at the test-retest correlation between the two sets of scores. This is typically done by graphing the data in a scatterplot and computing the correlation coefficient. Figure 4.2 shows the correlation between two sets of scores of several university students on the Rosenberg Self-Esteem Scale, administered two times, a week apart. The correlation coefficient for these data is +.95. In general, a test-retest correlation of +.80 or greater is considered to indicate good reliability.\n\n![20.4.2.png](https:\/\/pxeblicvfnzlnounkznu.supabase.co\/storage\/v1\/object\/public\/strapi\/files\/20.4.2.png-7a58a93ccab1647791b625a12e6ff801.png)\n\nAgain, high test-retest correlations make sense when the construct being measured is assumed to be consistent over time, which is the case for intelligence, self-esteem, and the Big Five personality dimensions. But other constructs are not assumed to be stable over time. The very nature of mood, for example, is that it changes. So a measure of mood that produced a low test-retest correlation over a period of a month would not be a cause for concern.\n\n### Internal Consistency\u00a0\n\nAnother kind of reliability is\u00a0**internal\u00a0consistency**, which is the consistency of people\u2019s responses across the items on a multiple-item measure. In general, all the items on such measures are supposed to reflect the same underlying construct, so people\u2019s scores on those items should be correlated with each other. On the Rosenberg Self-Esteem Scale, people who agree that they are a person of worth should tend to agree that they have a number of good qualities. If people\u2019s responses to the different items are not correlated with each other, then it would no longer make sense to claim that they are all measuring the same underlying construct. This is as true for behavioral and physiological measures as for self-report measures. For example, people might make a series of bets in a simulated game of roulette as a measure of their level of risk seeking. This measure would be internally consistent to the extent that individual participants\u2019 bets were consistently high or low across trials.\n\nLike test-retest reliability, internal consistency can only be assessed by collecting and analyzing data. One approach is to look at a\u00a0**split-half\u00a0correlation**. This involves splitting the items into two sets, such as the first and second halves of the items or the even- and odd-numbered items. Then a score is computed for each set of items, and the relationship between the two sets of scores is examined. For example,\u00a0Figure 4.3 shows the split-half correlation between several university students\u2019 scores on the even-numbered items and their scores on the odd-numbered items of the Rosenberg Self-Esteem Scale. The correlation coefficient for these data is +.88. A split-half correlation of +.80 or greater is generally considered good internal consistency.\n\n![20.4.3.png](https:\/\/pxeblicvfnzlnounkznu.supabase.co\/storage\/v1\/object\/public\/strapi\/files\/20.4.3.png-babf58637785f5053a670a27b11afc73.png)\n\nPerhaps the most common measure of internal consistency used by researchers in psychology is a statistic called\u00a0**Cronbach\u2019s\u00a0\u03b1**\u00a0(the Greek letter alpha). Conceptually, \u03b1 is the mean of all possible split-half correlations for a set of items. For example, there are 252 ways to split a set of 10 items into two sets of five. Cronbach\u2019s \u03b1 would be the mean of the 252 split-half correlations. Note that this is not how \u03b1 is actually computed, but it is a correct way of interpreting the meaning of this statistic. Again, a value of +.80 or greater is generally taken to indicate good internal consistency.\n\n### Interrater Reliability\n\nMany behavioral measures involve significant judgment on the part of an observer or a rater.\u00a0**Inter-rater\u00a0reliability**\u00a0is the extent to which different observers are consistent in their judgments. For example, if you were interested in measuring university students\u2019 social skills, you could make video recordings of them as they interacted with another student whom they are meeting for the first time. Then you could have two or more observers watch the videos and rate each student\u2019s level of social skills. To the extent that each participant does, in fact, have some level of social skills that can be detected by an attentive observer, different observers\u2019 ratings should be highly correlated with each other. Inter-rater reliability would also have been measured in Bandura\u2019s Bobo doll study. In this case, the observers\u2019 ratings of how many acts of aggression a particular child committed while playing with the Bobo doll should have been highly positively correlated. Interrater reliability is often assessed using Cronbach\u2019s \u03b1 when the judgments are quantitative or an analogous statistic called\u00a0Cohen\u2019s\u00a0\u03ba\u00a0(the Greek letter kappa) when they are categorical.\n\n**Validity**\u00a0is the extent to which the scores from a measure represent the variable they are intended to. But how do researchers make this judgment? We have already considered one factor that they take into account\u2014reliability. When a measure has good test-retest reliability and internal consistency, researchers should be more confident that the scores represent what they are supposed to. There has to be more to it, however, because a measure can be extremely reliable but have no validity whatsoever. As an absurd example, imagine someone who believes that people\u2019s index finger length reflects their self-esteem and therefore tries to measure self-esteem by holding a ruler up to people\u2019s index fingers. Although this measure would have extremely good test-retest reliability, it would have absolutely no validity. The fact that one person\u2019s index finger is a centimeter longer than another\u2019s would indicate nothing about which one had higher self-esteem.\u00a0\n\nDiscussions of validity usually divide it into several distinct \u201ctypes.\u201d But a good way to interpret these types is that they are other kinds of evidence\u2014in addition to reliability\u2014that should be taken into account when judging the validity of a measure. Here we consider three basic kinds: face validity, content validity, and criterion validity.\n\n### Face Validity\n\n**Face\u00a0validity**\u00a0is the extent to which a measurement method appears \u201con its face\u201d to measure the construct of interest. Most people would expect a self-esteem questionnaire to include items about whether they see themselves as a person of worth and whether they think they have good qualities. So a questionnaire that included these kinds of items would have good face validity. The finger-length method of measuring self-esteem, on the other hand, seems to have nothing to do with self-esteem and therefore has poor face validity. Although face validity can be assessed quantitatively\u2014for example, by having a large sample of people rate a measure in terms of whether it appears to measure what it is intended to\u2014it is usually assessed informally.\n\nFace validity is at best a very weak kind of evidence that a measurement method is measuring what it is supposed to. One reason is that it is based on people\u2019s intuitions about human behavior, which are frequently wrong. It is also the case that many established measures in psychology work quite well despite lacking face validity. The Minnesota Multiphasic Personality Inventory-2 (MMPI-2) measures many personality characteristics and disorders by having people decide whether each of over 567 different statements applies to them\u2014where many of the statements do not have any obvious relationship to the construct that they measure. For example, the items \u201cI enjoy detective or mystery stories\u201d and \u201cThe sight of blood doesn\u2019t frighten me or make me sick\u201d both measure the suppression of aggression. In this case, it is not the participants\u2019 literal answers to these questions that are of interest, but rather whether the pattern of the participants\u2019 responses to a series of questions matches those of individuals who tend to suppress their aggression.\n\n### Content Validity\n\n**Content\u00a0validity**\u00a0is the extent to which a measure \u201ccovers\u201d the construct of interest. For example, if a researcher conceptually defines test anxiety as involving both sympathetic nervous system activation (leading to nervous feelings) and negative thoughts, then his measure of test anxiety should include items about both nervous feelings and negative thoughts. Or consider that attitudes are usually defined as involving thoughts, feelings, and actions toward something. By this conceptual definition, a person has a positive attitude toward exercise to the extent that they think positive thoughts about exercising, feels good about exercising, and actually exercises. So to have good content validity, a measure of people\u2019s attitudes toward exercise would have to reflect all three of these aspects. Like face validity, content validity is not usually assessed quantitatively. Instead, it is assessed by carefully checking the measurement method against the conceptual definition of the construct.\n\n### Criterion Validity\u00a0\n\n**Criterion\u00a0validity** is the extent to which people\u2019s scores on a measure are correlated with other variables (known as criteria) that one would expect them to be correlated with. For example, people\u2019s scores on a new measure of test anxiety should be negatively correlated with their performance on an important school exam. If it were found that people\u2019s scores were in fact negatively correlated with their exam performance, then this would be a piece of evidence that these scores really represent people\u2019s test anxiety. But if it were found that people scored equally well on the exam regardless of their test anxiety scores, then this would cast doubt on the validity of the measure.\u00a0\n\nA **criterion** can be any variable that one has reason to think should be correlated with the construct being measured, and there will usually be many of them. For example, one would expect test anxiety scores to be negatively correlated with exam performance and course grades and positively correlated with general anxiety and with blood pressure during an exam. Or imagine that a researcher develops a new measure of physical risk taking. People\u2019s scores on this measure should be correlated with their participation in \u201cextreme\u201d activities such as snowboarding and rock climbing, the number of speeding tickets they have received, and even the number of broken bones they have had over the years. When the criterion is measured at the same time as the construct, criterion validity is referred to as **concurrent validity**; however, when the criterion is measured at some point in the future (after the construct has been measured), it is referred to as **predictive validity** (because scores on the measure have \u201cpredicted\u201d a future outcome).\n\nCriteria can also include other measures of the same construct. For example, one would expect new measures of test anxiety or physical risk taking to be positively correlated with existing established measures of the same constructs. This is known as **convergent validity**.\n\nAssessing convergent validity requires collecting data using the measure. Researchers John Cacioppo and Richard Petty did this when they created their self-report Need for Cognition Scale to measure how much people value and engage in thinking (Cacioppo & Petty, 1982)\\[1\\]. In a series of studies, they showed that people\u2019s scores were positively correlated with their scores on a standardized academic achievement test, and that their scores were negatively correlated with their scores on a measure of dogmatism (which represents a tendency toward obedience). In the years since it was created, the Need for Cognition Scale has been used in literally hundreds of studies and has been shown to be correlated with a wide variety of other variables, including the effectiveness of an advertisement, interest in politics, and juror decisions (Petty, Bri\u00f1ol, Loersch, & McCaslin, 2009)\\[2\\].\n\n### Discriminant Validity\n\n**Discriminant\u00a0validity**, on\u00a0the other hand, is the extent to which scores on a measure are _not_ correlated with measures of variables that are conceptually distinct. For example, self-esteem is a general attitude toward the self that is fairly stable over time. It is not the same as mood, which is how good or bad one happens to be feeling right now. So people\u2019s scores on a new measure of self-esteem should not be very highly correlated with their moods. If the new measure of self-esteem were highly correlated with a measure of mood, it could be argued that the new measure is not really measuring self-esteem; it is measuring mood instead.\n\nWhen they created the Need for Cognition Scale, Cacioppo and Petty also provided evidence of discriminant validity by showing that people\u2019s scores were not correlated with certain other variables. For example, they found only a weak correlation between people\u2019s need for cognition and a measure of their cognitive style\u2014the extent to which they tend to think analytically by breaking ideas into smaller parts or holistically in terms of \u201cthe big picture.\u201d They also found no correlation between people\u2019s need for cognition and measures of their test anxiety and their tendency to respond in socially desirable ways. All these low correlations provide evidence that the measure is reflecting a conceptually distinct construct.\n\n1.  Cacioppo, J. T., & Petty, R. E. (1982). The need for cognition. _Journal of Personality and Social Psychology, 42_, 116\u2013131.\n2.  Petty, R. E, Bri\u00f1ol, P., Loersch, C., & McCaslin, M. J. (2009). The need for cognition. In M. R. Leary & R. H. Hoyle (Eds.), _Handbook of individual differences in social behavior_ (pp. 318\u2013329). New York, NY: Guilford Press.","text":"Learning Objectives\n\n1. Define reliability, including the different types and how they are assessed.\n2. Define validity, including the different types and how they are assessed.\n3. Describe the kinds of evidence that would be relevant to assessing the reliability and validity of a particular measure.\u00a0\n\nAgain, measurement involves assigning scores to individuals so that they represent some characteristic of the individuals. But how do researchers know that the scores actually represent the characteristic, especially when it is a construct like intelligence, self-esteem, depression, or working memory capacity? The answer is that they conduct research using the measure to confirm that the scores make sense based on their understanding of the construct being measured. This is an extremely important point. Psychologists do not simply\u00a0assume\u00a0that their measures work. Instead, they collect data to demonstrate\u00a0that they work. If their research does not demonstrate that a measure works, they stop using it.\u00a0\n\nAs an informal example, imagine that you have been dieting for a month. Your clothes seem to be fitting more loosely, and several friends have asked if you have lost weight. If at this point your bathroom scale indicated that you had lost 10 pounds, this would make sense and you would continue to use the scale. But if it indicated that you had gained 10 pounds, you would rightly conclude that it was broken and either fix it or get rid of it. In evaluating a measurement method, psychologists consider two general dimensions: reliability and validity.\n\nReliability\u00a0refers to the consistency of a measure. Psychologists consider three types of consistency: over time (test-retest reliability), across items (internal consistency), and across different researchers (inter-rater reliability).\u00a0\n\nTest-Retest Reliability\n\nWhen researchers measure a construct that they assume to be consistent across time, then the scores they obtain should also be consistent across time.\u00a0Test-retest\u00a0reliability\u00a0is the extent to which this is actually the case. For example, intelligence is generally thought to be consistent across time. A person who is highly intelligent today will be highly intelligent next week. This means that any good measure of intelligence should produce roughly the same scores for this individual next week as it does today. Clearly, a measure that produces highly inconsistent scores over time cannot be a very good measure of a construct that is supposed to be consistent.\n\nAssessing test-retest reliability requires using the measure on a group of people at one time, using it again on the\u00a0same group of people at a later time, and then looking at the test-retest correlation between the two sets of scores. This is typically done by graphing the data in a scatterplot and computing the correlation coefficient. Figure 4.2 shows the correlation between two sets of scores of several university students on the Rosenberg Self-Esteem Scale, administered two times, a week apart. The correlation coefficient for these data is +.95. In general, a test-retest correlation of +.80 or greater is considered to indicate good reliability.\n\nAgain, high test-retest correlations make sense when the construct being measured is assumed to be consistent over time, which is the case for intelligence, self-esteem, and the Big Five personality dimensions. But other constructs are not assumed to be stable over time. The very nature of mood, for example, is that it changes. So a measure of mood that produced a low test-retest correlation over a period of a month would not be a cause for concern.\n\n\u00a0\n\nInternal Consistency\u00a0\n\nAnother kind of reliability is\u00a0internal\u00a0consistency, which is the consistency of people\u2019s responses across the items on a multiple-item measure. In general, all the items on such measures are supposed to reflect the same underlying construct, so people\u2019s scores on those items should be correlated with each other. On the Rosenberg Self-Esteem Scale, people who agree that they are a person of worth should tend to agree that they have a number of good qualities. If people\u2019s responses to the different items are not correlated with each other, then it would no longer make sense to claim that they are all measuring the same underlying construct. This is as true for behavioral and physiological measures as for self-report measures. For example, people might make a series of bets in a simulated game of roulette as a measure of their level of risk seeking. This measure would be internally consistent to the extent that individual participants\u2019 bets were consistently high or low across trials.\n\nLike test-retest reliability, internal consistency can only be assessed by collecting and analyzing data. One approach is to look at a\u00a0split-half\u00a0correlation. This involves splitting the items into two sets, such as the first and second halves of the items or the even- and odd-numbered items. Then a score is computed for each set of items, and the relationship between the two sets of scores is examined. For example,\u00a0Figure 4.3 shows the split-half correlation between several university students\u2019 scores on the even-numbered items and their scores on the odd-numbered items of the Rosenberg Self-Esteem Scale. The correlation coefficient for these data is +.88. A split-half correlation of +.80 or greater is generally considered good internal consistency.\n\nPerhaps the most common measure of internal consistency used by researchers in psychology is a statistic called\u00a0Cronbach\u2019s\u00a0\u03b1\u00a0(the Greek letter alpha). Conceptually, \u03b1 is the mean of all possible split-half correlations for a set of items. For example, there are 252 ways to split a set of 10 items into two sets of five. Cronbach\u2019s \u03b1 would be the mean of the 252 split-half correlations. Note that this is not how \u03b1 is actually computed, but it is a correct way of interpreting the meaning of this statistic. Again, a value of +.80 or greater is generally taken to indicate good internal consistency.\n\nInterrater Reliability\n\nMany behavioral measures involve significant judgment on the part of an observer or a rater.\u00a0Inter-rater\u00a0reliability\u00a0is the extent to which different observers are consistent in their judgments. For example, if you were interested in measuring university students\u2019 social skills, you could make video recordings of them as they interacted with another student whom they are meeting for the first time. Then you could have two or more observers watch the videos and rate each student\u2019s level of social skills. To the extent that each participant does, in fact, have some level of social skills that can be detected by an attentive observer, different observers\u2019 ratings should be highly correlated with each other. Inter-rater reliability would also have been measured in Bandura\u2019s Bobo doll study. In this case, the observers\u2019 ratings of how many acts of aggression a particular child committed while playing with the Bobo doll should have been highly positively correlated. Interrater reliability is often assessed using Cronbach\u2019s \u03b1 when the judgments are quantitative or an analogous statistic called\u00a0Cohen\u2019s\u00a0\u03ba\u00a0(the Greek letter kappa) when they are categorical.\n\nValidity\u00a0is the extent to which the scores from a measure represent the variable they are intended to. But how do researchers make this judgment? We have already considered one factor that they take into account\u2014reliability. When a measure has good test-retest reliability and internal consistency, researchers should be more confident that the scores represent what they are supposed to. There has to be more to it, however, because a measure can be extremely reliable but have no validity whatsoever. As an absurd example, imagine someone who believes that people\u2019s index finger length reflects their self-esteem and therefore tries to measure self-esteem by holding a ruler up to people\u2019s index fingers. Although this measure would have extremely good test-retest reliability, it would have absolutely no validity. The fact that one person\u2019s index finger is a centimeter longer than another\u2019s would indicate nothing about which one had higher self-esteem.\u00a0\n\nDiscussions of validity usually divide it into several distinct \u201ctypes.\u201d But a good way to interpret these types is that they are other kinds of evidence\u2014in addition to reliability\u2014that should be taken into account when judging the validity of a measure. Here we consider three basic kinds: face validity, content validity, and criterion validity.\n\nFace Validity\n\nFace\u00a0validity\u00a0is the extent to which a measurement method appears \u201con its face\u201d to measure the construct of interest. Most people would expect a self-esteem questionnaire to include items about whether they see themselves as a person of worth and whether they think they have good qualities. So a questionnaire that included these kinds of items would have good face validity. The finger-length method of measuring self-esteem, on the other hand, seems to have nothing to do with self-esteem and therefore has poor face validity. Although face validity can be assessed quantitatively\u2014for example, by having a large sample of people rate a measure in terms of whether it appears to measure what it is intended to\u2014it is usually assessed informally.\n\nFace validity is at best a very weak kind of evidence that a measurement method is measuring what it is supposed to. One reason is that it is based on people\u2019s intuitions about human behavior, which are frequently wrong. It is also the case that many established measures in psychology work quite well despite lacking face validity. The Minnesota Multiphasic Personality Inventory-2 (MMPI-2) measures many personality characteristics and disorders by having people decide whether each of over 567 different statements applies to them\u2014where many of the statements do not have any obvious relationship to the construct that they measure. For example, the items \u201cI enjoy detective or mystery stories\u201d and \u201cThe sight of blood doesn\u2019t frighten me or make me sick\u201d both measure the suppression of aggression. In this case, it is not the participants\u2019 literal answers to these questions that are of interest, but rather whether the pattern of the participants\u2019 responses to a series of questions matches those of individuals who tend to suppress their aggression.\n\nContent Validity\n\nContent\u00a0validity\u00a0is the extent to which a measure \u201ccovers\u201d the construct of interest. For example, if a researcher conceptually defines test anxiety as involving both sympathetic nervous system activation (leading to nervous feelings) and negative thoughts, then his measure of test anxiety should include items about both nervous feelings and negative thoughts. Or consider that attitudes are usually defined as involving thoughts, feelings, and actions toward something. By this conceptual definition, a person has a positive attitude toward exercise to the extent that they think positive thoughts about exercising, feels good about exercising, and actually exercises. So to have good content validity, a measure of people\u2019s attitudes toward exercise would have to reflect all three of these aspects. Like face validity, content validity is not usually assessed quantitatively. Instead, it is assessed by carefully checking the measurement method against the conceptual definition of the construct.\n\n\u00a0\n\nCriterion Validity\u00a0\n\nCriterion\u00a0validity is the extent to which people\u2019s scores on a measure are correlated with other variables (known as criteria) that one would expect them to be correlated with. For example, people\u2019s scores on a new measure of test anxiety should be negatively correlated with their performance on an important school exam. If it were found that people\u2019s scores were in fact negatively correlated with their exam performance, then this would be a piece of evidence that these scores really represent people\u2019s test anxiety. But if it were found that people scored equally well on the exam regardless of their test anxiety scores, then this would cast doubt on the validity of the measure.\u00a0\n\nA criterion can be any variable that one has reason to think should be correlated with the construct being measured, and there will usually be many of them. For example, one would expect test anxiety scores to be negatively correlated with exam performance and course grades and positively correlated with general anxiety and with blood pressure during an exam. Or imagine that a researcher develops a new measure of physical risk taking. People\u2019s scores on this measure should be correlated with their participation in \u201cextreme\u201d activities such as snowboarding and rock climbing, the number of speeding tickets they have received, and even the number of broken bones they have had over the years. When the criterion is measured at the same time as the construct, criterion validity is referred to as concurrent validity; however, when the criterion is measured at some point in the future (after the construct has been measured), it is referred to as predictive validity (because scores on the measure have \u201cpredicted\u201d a future outcome).\n\nCriteria can also include other measures of the same construct. For example, one would expect new measures of test anxiety or physical risk taking to be positively correlated with existing established measures of the same constructs. This is known as convergent validity.\n\nAssessing convergent validity requires collecting data using the measure. Researchers John Cacioppo and Richard Petty did this when they created their self-report Need for Cognition Scale to measure how much people value and engage in thinking (Cacioppo & Petty, 1982)[1]. In a series of studies, they showed that people\u2019s scores were positively correlated with their scores on a standardized academic achievement test, and that their scores were negatively correlated with their scores on a measure of dogmatism (which represents a tendency toward obedience). In the years since it was created, the Need for Cognition Scale has been used in literally hundreds of studies and has been shown to be correlated with a wide variety of other variables, including the effectiveness of an advertisement, interest in politics, and juror decisions (Petty, Bri\u00f1ol, Loersch, & McCaslin, 2009)[2].\n\nDiscriminant Validity\n\nDiscriminant\u00a0validity, on\u00a0the other hand, is the extent to which scores on a measure are not correlated with measures of variables that are conceptually distinct. For example, self-esteem is a general attitude toward the self that is fairly stable over time. It is not the same as mood, which is how good or bad one happens to be feeling right now. So people\u2019s scores on a new measure of self-esteem should not be very highly correlated with their moods. If the new measure of self-esteem were highly correlated with a measure of mood, it could be argued that the new measure is not really measuring self-esteem; it is measuring mood instead.\n\nWhen they created the Need for Cognition Scale, Cacioppo and Petty also provided evidence of discriminant validity by showing that people\u2019s scores were not correlated with certain other variables. For example, they found only a weak correlation between people\u2019s need for cognition and a measure of their cognitive style\u2014the extent to which they tend to think analytically by breaking ideas into smaller parts or holistically in terms of \u201cthe big picture.\u201d They also found no correlation between people\u2019s need for cognition and measures of their test anxiety and their tendency to respond in socially desirable ways. All these low correlations provide evidence that the measure is reflecting a conceptually distinct construct.\n\nCacioppo, J. T., & Petty, R. E. (1982). The need for cognition. Journal of Personality and Social Psychology, 42, 116\u2013131.Petty, R. E, Bri\u00f1ol, P., Loersch, C., & McCaslin, M. J. (2009). The need for cognition. In M. R. Leary & R. H. Hoyle (Eds.), Handbook of individual differences in social behavior (pp. 318\u2013329). New York, NY: Guilford Press. \u00a0","contextuality":{"text":"Reliability and validity are key concepts in psychological measurement. Reliability refers to the ___________ of a measure, encompassing test-retest reliability, internal ___________, and inter-rater reliability. Test-retest reliability ________ the consistency of scores over time, crucial for constructs _______ stable, like intelligence. Internal consistency _________ the correlation of responses across items within a measure, often using Cronbach\u2019s \u03b1. Inter-rater reliability ________ consistency across different observers\u2019 judgments. Validity ________ whether a measure accurately represents the intended variable, with types including face ________, content validity, and criterion validity. Criterion validity examines ____________ with expected variables, while discriminant validity ensures scores are not __________ with distinct constructs. These concepts collectively ensure measures are both consistent and meaningful.","gaps":[["consistency",98,11],["consistency",171,11],["assesses",237,8],["assumed",306,7],["evaluates",362,9],["involves",482,8],["concerns",551,8],["validity",649,8],["correlations",729,12],["correlated",818,10]]},"contextuality_plus":{"text":"Reliability and validity are key concepts in psychological measurement. Reliability refers to the ___________ of a measure, encompassing test-retest ___________, internal consistency, and inter-rater ___________. Test-retest reliability assesses the consistency of scores over time, crucial for constructs _______ stable, like intelligence. Internal consistency _________ the correlation of responses across items within a measure, often using Cronbach\u2019s \u03b1. Inter-rater reliability involves consistency across different observers\u2019 _________. Validity concerns whether a measure accurately __________ the intended variable, with types including face validity, content ________, and criterion validity. Criterion validity examines ____________ with expected variables, while discriminant validity ensures scores are not __________ with distinct constructs. These concepts collectively ensure measures are both consistent and meaningful.","gaps":[["consistency",98,11],["reliability",149,11],["reliability",200,11],["assumed",306,7],["evaluates",362,9],["judgments",531,9],["represents",589,10],["validity",667,8],["correlations",729,12],["correlated",818,10]]},"keyword":{"text":"Reliability and validity are key concepts in psychological measurement. Reliability refers to the ___________ of a measure, encompassing ___________ reliability, internal consistency, and ___________ reliability. Test-retest reliability assesses the ___________ of scores over time, crucial for constructs assumed stable, like intelligence. Internal ___________ evaluates the correlation of responses across items within a _______, often using Cronbach\u2019s \u03b1. Inter-rater reliability involves ___________ across different observers\u2019 judgments. Validity concerns whether a _______ accurately represents the intended variable, with types including face ________, content validity, and criterion ________. Criterion validity examines correlations with expected variables, while discriminant ________ ensures scores are not correlated with distinct constructs. These concepts collectively ensure _______s are both consistent and meaningful.","gaps":[["inter-rater",188,11],["test-retest",137,11],["measure",423,7],["measure",570,7],["measure",890,7],["validity",649,8],["validity",691,8],["validity",786,8],["consistency",98,11],["consistency",250,11],["consistency",350,11],["consistency",491,11]]}}
